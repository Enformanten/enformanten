{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "# anomaly detection\n",
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "# settings\n",
    "pd.options.plotting.backend = \"plotly\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import snowflake.connector\n",
    "import pandas as pd\n",
    "from snowflake.connector.pandas_tools import write_pandas\n",
    "\n",
    "prediction_table = \"4_FEATURIZ_DRIFTOPTIMERING_PREDICT_TEST\"\n",
    "training_table = \"4_FEATURIZ_DRIFTOPTIMERING_TRAINING_TEST\"\n",
    "output_table = \"1_RAW_DRIFTOPTIMERINGSMODEL_TEST\"\n",
    "\n",
    "conn = snowflake.connector.connect(\n",
    "        user='GOVTECH_ROBOT_USER',\n",
    "        password='Y^?:mZ\"s|dD%c&8G3cun',\n",
    "        account='iu89556.west-europe.azure',    \n",
    "        role=\"GOVTECH_DB_RW\",\n",
    "        warehouse='GOVTECH_ANALYST_WH',\n",
    "        database='GOVTECH_DB',\n",
    "        schema='RAW'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_snowflake_data(conn, table):\n",
    "\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    cursor.execute(f'Select * from \"RAW\".\"{table}\";')\n",
    "\n",
    "    # convert the tuple of tuples to a Pandas DataFrame\n",
    "    df = pd.DataFrame(cursor.fetchall(), columns=['ID', 'Kommune', 'Skole', 'Date', 'Time', 'Dayname',\n",
    "                      'Tidspunkt_Type', 'Type', 'Navn', 'CO2', 'Temp', 'Motion', 'IAQ', 'Booket', 'Skemalagt'])\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataf = get_snowflake_data(conn, \"4_FEATURIZ_DRIFTOPTIMERING_TRAINING_TEST\")\n",
    "# capitalize column names\n",
    "dataf.columns = dataf.columns.str.upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'ID': '03.S.06',\n",
       "  'KOMMUNE': 'Aarhus',\n",
       "  'SKOLE': 'Strandskolen',\n",
       "  'DATE': datetime.date(2022, 12, 28),\n",
       "  'TIME': datetime.time(5, 30),\n",
       "  'DAYNAME': 'Onsdag',\n",
       "  'TIDSPUNKT_TYPE': 'Skole',\n",
       "  'TYPE': 'Skole-ferie',\n",
       "  'NAVN': 'Juleferie',\n",
       "  'CO2': 414.0,\n",
       "  'TEMP': 14.0,\n",
       "  'MOTION': nan,\n",
       "  'IAQ': 0.02,\n",
       "  'BOOKET': nan,\n",
       "  'SKEMALAGT': nan}]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataf.sample().to_dict(orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Normal Dag', 'Skole-ferie', 'Helligdag', 'Mærkedage'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "types = ['Normal Dag', 'Skole-ferie', 'Helligdag', 'Mærkedage']\n",
    "# map the types to numbers\n",
    "dataf.TYPE.map({types[i]: i for i in range(len(types))})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMPORT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader():\n",
    "\n",
    "    SENSOR_COLUMNS = [\"CO2\", \"TEMP\", \"MOTION\", \"IAQ\", \"SKEMALAGT\", \"BOOKET\"]\n",
    "\n",
    "    @classmethod\n",
    "    def diagnose(cls, df, col, dfunc=\"unique\", **kwargs):\n",
    "        print(getattr(df[col], dfunc)(**kwargs))\n",
    "        return df\n",
    "\n",
    "    @classmethod\n",
    "    def fill_na(cls, df, cols, values, types):\n",
    "        return df.assign(\n",
    "            **{col: df[col].fillna(value).astype(_type) for col, value, _type in zip(cols, values, types)}\n",
    "            # .fillna(value).astype(_type\n",
    "            # fillna(method=\"ffill\", limit=2)\n",
    "        )\n",
    "\n",
    "    @classmethod\n",
    "    def display_missing_values(cls, df, print_nans):\n",
    "        if print_nans:\n",
    "            for i, munc in df.groupby('KOMMUNE'):\n",
    "                print(f\"Missing values for {i}\")\n",
    "                display(\n",
    "                    \n",
    "                    munc\n",
    "                    # filter between the first and last timeslot of activity\n",
    "                    .sort_values(\"DATETIME\", ascending=True)\n",
    "                    [lambda d: d[\"DATETIME\"].between(\n",
    "                            *(\n",
    "                                d.\n",
    "                                dropna(\n",
    "                                    subset=cls.SENSOR_COLUMNS,\n",
    "                                    how=\"all\"\n",
    "                                )\n",
    "                                [\"DATETIME\"]\n",
    "                                .iloc[[0, -1]]\n",
    "                            ),\n",
    "                            inclusive=\"both\"\n",
    "                        )\n",
    "                    ]\n",
    "                    \n",
    "                    .groupby('ID')\n",
    "                    [cls.SENSOR_COLUMNS]\n",
    "                    .apply(lambda x: x.isnull().sum()/len(x))\n",
    "                    .style.format(precision=2)\n",
    "                    .background_gradient(cmap='Reds', axis=0, vmin=0, vmax=1)\n",
    "                )\n",
    "        return df\n",
    "\n",
    "    @classmethod \n",
    "    def merge_dt(cls, df, date, time, name, sep=\" \"):\n",
    "        return df.assign(**{name: lambda d: pd.to_datetime(d[date].astype(str) + sep + d[time].astype(str))})\n",
    "\n",
    "    @classmethod\n",
    "    def drop_cols(cls, df, cols):\n",
    "        return df.drop(columns=cols)\n",
    "    \n",
    "    @classmethod\n",
    "    def full_process(cls, df, print_nans, **kwargs):\n",
    "        \n",
    "        return (\n",
    "            df\n",
    "            # .pipe(cls.drop_cols, cols=[\"KOMMUNE_DATO_LOKALE_TIME\"])\n",
    "            .pipe(cls.diagnose, col=\"KOMMUNE\", dfunc=\"unique\")\n",
    "            .pipe(cls.merge_dt, date=\"DATE\", time=\"TIME\", name=\"DATETIME\")\n",
    "            .pipe(cls.display_missing_values, print_nans)\n",
    "            .pipe(cls.fill_na, \n",
    "                cols=cls.SENSOR_COLUMNS[:-1],\n",
    "                values=[487, 20.0, 0.0, .03, 0.0],\n",
    "                types=[float, float, float, float, float]\n",
    "            )\n",
    "        )\n",
    "\n",
    "    @classmethod\n",
    "    def _load(cls, path):\n",
    "        return pd.read_csv(path)\n",
    "    \n",
    "    @classmethod\n",
    "    def load(cls, path = \"data/Skemaer.csv\", steps : dict = {}, print_nans = True, df = None, **kwargs):\n",
    "        if isinstance(df, pd.DataFrame):\n",
    "            dataf = df\n",
    "        elif path:\n",
    "            dataf = cls._load(path)\n",
    "        else:\n",
    "            raise ValueError(\"No data source specified\")\n",
    "        \n",
    "\n",
    "        if \"all\" in steps:\n",
    "            return cls.full_process(dataf, print_nans, **kwargs)\n",
    "\n",
    "        for func, func_kwargs in steps.items():\n",
    "            dataf = getattr(cls, func)(dataf, **func_kwargs)\n",
    "            \n",
    "        return dataf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "class RoomStdMean(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, high_value_threshold=2000):\n",
    "        self.high_value_threshold = high_value_threshold\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        filtered_X = X[(X['CO2'] < self.high_value_threshold) & (X['CO2'].notna())]\n",
    "        self.room_stats = filtered_X.groupby('ID')['CO2'].agg(['mean', 'std']).to_dict()\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        X['CO2_Z_SCORE'] = X.apply(\n",
    "            lambda row: (row['CO2'] - self.room_stats['mean'].get(row['ID'], np.nan)) / self.room_stats['std'].get(row['ID'], np.nan)\n",
    "            if pd.notna(row['CO2']) else np.nan, axis=1\n",
    "        )\n",
    "        return X\n",
    "    \n",
    "\n",
    "class StatelessFeatures(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        X['CO2_ROLLING_MEAN'] = X['CO2'].rolling(window=4).mean()\n",
    "        X['TEMP_ROLLING_MEAN'] = X['TEMP'].rolling(window=4).mean()\n",
    "        X['CO2/TEMP'] = X['CO2'] / X['TEMP']\n",
    "        X['MINUTES'] = X['DATETIME'].dt.hour * 60 + X['DATETIME'].dt.minute\n",
    "        X['DOW'] = X['DATETIME'].dt.dayofweek\n",
    "        X['BOOKET'] = X['BOOKET'].fillna(0.0)\n",
    "        X['Motion_Available'] = ~X['MOTION'].isna()\n",
    "        X['Motion_or_CO2'] = X['MOTION'].fillna(X['CO2'])\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('stateless_features', StatelessFeatures()),\n",
    "    ('room_std_mean', RoomStdMean(high_value_threshold=2000))\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PREPROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_values(df, col, values):\n",
    "    return df[lambda d: d[col].isin(values)]\n",
    "\n",
    "\n",
    "def drop_inactive_ranges(df, range_length=5):\n",
    "    # Identify intervals of 5+ rows of identical values\n",
    "    to_drop = (\n",
    "        df.groupby(\"ID\")\n",
    "        [\"CO2\"].transform(\n",
    "            lambda x: x.rolling(range_length).apply(\n",
    "                lambda d: d.nunique() == 1\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    cdataf = df.drop(index=to_drop[lambda d: d.eq(1.0)].index)\n",
    "    print(f\"Turns {df.shape[0]} rows into {cdataf.shape[0]} rows - Dropping {(df.shape[0] - cdataf.shape[0])/1000}K rows\")\n",
    "    return cdataf\n",
    "\n",
    "def add_date_range_group(grp):\n",
    "    grp['DATE_RANGE_GROUP'] = grp['DATETIME'].transform(lambda x: (x.diff().dt.total_seconds()/ 60).ne(15).cumsum())\n",
    "    return grp\n",
    "\n",
    "def acceleration_features(df):\n",
    "\n",
    "    return (\n",
    "        df.sort_values(\"DATETIME\").groupby(\"ID\").apply(add_date_range_group)\n",
    "        .reset_index(drop=True)\n",
    "        .assign(\n",
    "            \n",
    "            CO2_ACC=lambda d: d.groupby([\"ID\", \"DATE_RANGE_GROUP\"])[\"CO2\"].ffill().pct_change().fillna(0).replace(np.inf, 0),\n",
    "            TEMP_ACC=lambda d: d.groupby([\"ID\", \"DATE_RANGE_GROUP\"])[\"TEMP\"].ffill().pct_change().fillna(0).replace(np.inf, 0),\n",
    "            MOTION_ACC=lambda d: d.groupby([\"ID\", \"DATE_RANGE_GROUP\"])[\"MOTION\"].ffill().pct_change().fillna(0).replace(np.inf, 0),\n",
    "            IAQ_ACC=lambda d: d.groupby([\"ID\", \"DATE_RANGE_GROUP\"])[\"IAQ\"].ffill().pct_change().fillna(0).replace(np.inf, 0),\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "def preprocess_for_modelling(df):\n",
    "    return (\n",
    "        df\n",
    "        .drop(columns=[\n",
    "            \"DATE\",\n",
    "            \"TIDSPUNKT_TYPE\",\n",
    "            \"TYPE\",\n",
    "            \"DATE_RANGE_GROUP\",\n",
    "            \"DAYNAME\",\n",
    "            \"TIME\",\n",
    "            \"SKOLE\",\n",
    "            \"KOMMUNE\",\n",
    "            \"NAVN\"\n",
    "            ]\n",
    "        )\n",
    "    )\n",
    "\n",
    "def remove_outliers(df, columns):\n",
    "    if 'CO2' in columns:\n",
    "        df = df[(df['CO2'] >= 100) & (df['CO2'] <= 6000)]\n",
    "        \n",
    "    if 'TEMP' in columns:\n",
    "        df = df[(df['TEMP'] >= 1) & (df['TEMP'] <= 50)]\n",
    "        \n",
    "    return df\n",
    "\n",
    "def engineer_features(df):\n",
    "    df['CO2_ROLLING_MEAN'] = df['CO2'].rolling(window=4).mean()\n",
    "    df['TEMP_ROLLING_MEAN'] = df['TEMP'].rolling(window=4).mean()\n",
    "    df['CO2/TEMP'] = df['CO2'] / df['TEMP']\n",
    "    return df\n",
    "\n",
    "def encode_time_features(df):\n",
    "    # Example: Convert TIME to total minutes past midnight\n",
    "\n",
    "    types = ['Normal Dag', 'Skole-ferie', 'Helligdag', 'Mærkedage']\n",
    "\n",
    "    df = (\n",
    "        df.assign(\n",
    "            MINUTES=lambda d: d[\"DATETIME\"].dt.hour * 60 + d[\"DATETIME\"].dt.minute,\n",
    "            TYPE=lambda d: d[\"TYPE\"].map({types[i]: i for i in range(len(types))}),\n",
    "            SCHOOL_TIME = lambda d: d[\"TIDSPUNKT_TYPE\"].map({'Fritid': False, 'Skole': True}),\n",
    "            DOW=lambda d: d[\"DATETIME\"].dt.dayofweek,\n",
    "            BOOKET=lambda d: d[\"BOOKET\"].fillna(0.0),\n",
    "\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Rolling standard deviation\n",
    "    df['CO2_ROLLING_STD'] = df.groupby('ID')['CO2'].transform(lambda x: x.rolling(window=4).std())\n",
    "\n",
    "    # Z-score for each room\n",
    "    mean_std_df = df.groupby('ID')['CO2'].agg(['mean', 'std']).reset_index()\n",
    "    mean_std_df.columns = ['ID', 'CO2_MEAN', 'CO2_STD']\n",
    "    df = pd.merge(df, mean_std_df, on='ID', how='left')\n",
    "    df['CO2_Z_SCORE'] = (df['CO2'] - df['CO2_MEAN']) / df['CO2_STD']\n",
    "    return df\n",
    "\n",
    "\n",
    "def utilize_motion_sensors(df):\n",
    "    df['Motion_Available'] = ~df['MOTION'].isna()\n",
    "    df['Motion_or_CO2'] = df['MOTION'].fillna(df['CO2'])\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimate usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_usage(\n",
    "        data : pd.DataFrame,\n",
    "        usage_coeff : float = 2.1,\n",
    "        usage_limit : float = .2\n",
    "    ) -> float:\n",
    "\n",
    "    \"\"\"Estimate usage score for a given room based\n",
    "    on the number of timeslots with assumed activity,\n",
    "    e.g., a booked timeslot or a timeslot in school \n",
    "    schema \"\"\"\n",
    "\n",
    "    used_slots = max(\n",
    "        data[lambda d:\n",
    "            d[\"SKEMALAGT\"].astype(bool) | \n",
    "            d[\"BOOKET\"].fillna(0).astype(bool)\n",
    "        ].shape[0],  # number of slots with assumed activity\n",
    "        0.1\n",
    "    )\n",
    "    \n",
    "\n",
    "    # Evaluate against heuristic usage limit\n",
    "    est_usage = min(\n",
    "        usage_coeff * used_slots / data.shape[0], \n",
    "        usage_limit\n",
    "    )\n",
    "\n",
    "    # print(\n",
    "    #     f\"{data['ID'].iloc[0]} | \"\n",
    "    #     + f\"Est. usage score {est_usage:.2f} | \"\n",
    "    #     + f\"usage coeff: {usage_coeff:.2f}\"\n",
    "    # )\n",
    "    return est_usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MODELLING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_predict(df, **kwargs) -> tuple:\n",
    "    model_IF = IsolationForest(**kwargs)\n",
    "    model_IF.fit(df)\n",
    "\n",
    "    scores = model_IF.decision_function(df)\n",
    "    predictions = model_IF.predict(df)\n",
    "    return scores, predictions\n",
    "\n",
    "\n",
    "def format_predictions(preds : list) -> list:\n",
    "    return [1 if pred == -1 else 0 for pred in preds]\n",
    "\n",
    "\n",
    "def format_scores(scores : list) -> list:\n",
    "    \"\"\" Normalize scores in range [-1, 1] to\n",
    "    [0, 1] where 1 is most anomalous\n",
    "\n",
    "    Args:\n",
    "        scores (list): List of scores\n",
    "    \"\"\"\n",
    "    return np.interp(scores, (min(scores), max(scores)), (0, 1))\n",
    "\n",
    "\n",
    "def run_model(\n",
    "        room : pd.DataFrame,\n",
    "        features : list,\n",
    "        usage_coeff : float,\n",
    "        usage_limit : float,\n",
    "        **kwargs\n",
    "    ) -> pd.DataFrame:\n",
    "\n",
    "    \"\"\" Run fit_predict for every room in every building \"\"\"\n",
    "\n",
    "    _est_usage = estimate_usage(room, usage_coeff, usage_limit)\n",
    "    \n",
    "    scores, predictions = fit_predict(\n",
    "        room[features], \n",
    "        contamination=_est_usage,\n",
    "        **kwargs\n",
    "    )\n",
    "    return room.assign(\n",
    "        usage_score=1 - format_scores(scores),\n",
    "        in_use=format_predictions(predictions)\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Heuristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def add_heuristics(data, apply_rules):\n",
    "    \n",
    "    return data if not apply_rules else (\n",
    "\n",
    "        data\n",
    "        .assign(\n",
    "            in_use=lambda d: np.where(\n",
    "                d[\"CO2\"].lt(400),\n",
    "                0,\n",
    "                d[\"in_use\"],\n",
    "            ),\n",
    "        )\n",
    "        .assign( # Remove anomalies when CO2 accelerates or CO2 is high\n",
    "            in_use=lambda d: np.where(\n",
    "                (d[\"CO2_ACC\"].gt(0) & d[\"CO2\"].gt(600)) | (d[\"CO2\"].gt(1000)),\n",
    "                1, \n",
    "                d[\"in_use\"]\n",
    "            )\n",
    "        )\n",
    "\n",
    "        .assign(\n",
    "            # If the prior rules change the in_use value, then\n",
    "            # update the usage score to reflect this change\n",
    "            usage_score=lambda d: np.where(\n",
    "                (d[\"in_use\"].eq(1) & d[\"usage_score\"].lt(.5))\n",
    "                |\n",
    "                (d[\"in_use\"].eq(0) & d[\"usage_score\"].gt(.5)),\n",
    "                1 - d[\"usage_score\"],\n",
    "                d[\"usage_score\"]\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # .assign( \n",
    "        #     # Combine single usages - 1 IF n-1 = 1, n = 0, n+1 = 1, n+2 = 1\n",
    "        #     # Use for cases with a premature prediction of use\n",
    "        #     # NOTE: The check for 487 is to check for the fillna value.\n",
    "        #     # Otherwise, this does not work for Favrskov, since every\n",
    "        #     # other room in this school is a fillna value.\n",
    "        #     in_use=lambda d: np.where(\n",
    "        #         (d[\"in_use\"].shift(-1).eq(1) & d[\"CO2\"] != 487)\n",
    "        #         & \n",
    "        #         d[\"in_use\"].eq(0) \n",
    "        #         & \n",
    "        #         (d[\"in_use\"].shift(1).eq(1) & d[\"CO2\"].shift(1) != 487)\n",
    "        #         & \n",
    "        #         (d[\"in_use\"].shift(2).eq(1) & d[\"CO2\"].shift(2) != 487),\n",
    "        #         1,\n",
    "        #         d[\"in_use\"]\n",
    "        #     )\n",
    "        # )\n",
    "\n",
    "        # .assign( \n",
    "        #     # Remove single in_use - 0 IF n-1 = 0, n = 1, n+1 = 0\n",
    "        #     # NOTE: The check for 487 is to check for the fillna value.\n",
    "        #     # Otherwise, this does not work for Favrskov, since every\n",
    "        #     # other room in this school is a fillna value.\n",
    "        #     in_use=lambda d: np.where(\n",
    "        #         (d[\"in_use\"].shift(-1).eq(0) & d[\"CO2\"].shift(-1) != 487)\n",
    "        #         & \n",
    "        #         d[\"in_use\"].eq(1) \n",
    "        #         & \n",
    "        #         (d[\"in_use\"].shift(1).eq(0) & d[\"CO2\"].shift(1) != 487),\n",
    "        #         0,\n",
    "        #         d[\"in_use\"]\n",
    "        #     )\n",
    "        # )\n",
    "\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dir(run_id, _kommune) -> None:\n",
    "    Path(f\"results/{run_id}/{_kommune}\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "def export_plots(data, kommune, run_id):\n",
    "\n",
    "    for i, room in data.groupby(\"ID\"):\n",
    "\n",
    "        _kommune = kommune.lower()\n",
    "        create_dir(run_id, _kommune)\n",
    "\n",
    "        fig = room.plot.bar(    \n",
    "            x='DATETIME',\n",
    "            y='CO2',\n",
    "            color='in_use',\n",
    "            title=f'Anvendelsesmodel - Lokale {i} - {kommune} Kommune',\n",
    "            width=3000,\n",
    "            hover_data=data[[\"CO2_ACC\"]],\n",
    "        )\n",
    "        fig.update_traces(dict(marker_line_width=0))\n",
    "        fig.write_html(f'results/{run_id}/{_kommune}/anomaly-{_kommune}-{i}.html')\n",
    "        \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exit report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exit_report(df, kommune, original_data):\n",
    "\n",
    "    original = original_data[lambda d: d[\"KOMMUNE\"] == kommune]\n",
    "    print(\n",
    "        f\"EXIT REPORT - {kommune} | Size: {df.shape[0]} | \"\n",
    "        + f\"Orig. size: {original.shape[0]}\\n\"\n",
    "        + f\"Mean usage rate: {df['in_use'].mean():.2f} | \"\n",
    "        + f\"Mean usage score: {df['usage_score'].mean():.2f}\\n\"\n",
    "        + \"\\n\\n\"\n",
    "    )\n",
    "    assert df['ID'].nunique() == original['ID'].nunique(), \"Flow dropped rooms!\"\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*** \n",
    "\n",
    "### Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set feature set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "room_features = [\n",
    "    # \"SKEMALAGT\",\n",
    "    \"CO2_ACC\",\n",
    "    # \"TEMP_ACC\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def heartbeat(df, kommune, msg=\"\"):\n",
    "    print(f\"🧪 | Size: {df.shape[0]} | {df.columns} | {msg}\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flow(\n",
    "        data : pd.DataFrame,\n",
    "        run_id : str,\n",
    "        usage_coeff : float,\n",
    "        usage_limit : float,\n",
    "        random_state : int,\n",
    "        apply_rules : bool,\n",
    "        features : list,\n",
    "    ) -> pd.DataFrame:\n",
    "\n",
    "    \"\"\" Run the full flow for all municipalities\"\"\"\n",
    "                \n",
    "    for kommune in data[\"KOMMUNE\"].unique(): # kommune means municipality in Danish\n",
    "        print(f\"Running flow for {kommune}\")\n",
    "\n",
    "        yield (\n",
    "            \n",
    "            # Processing\n",
    "            data\n",
    "\n",
    "            # filter out data from other municipalities\n",
    "            .pipe(filter_values, col=\"KOMMUNE\", values=[kommune])\n",
    "\n",
    "            # drop consecutive rows with identical values, because\n",
    "            # these are likely to be inactive periods or faulty sensors\n",
    "            .pipe(drop_inactive_ranges, range_length=5)\n",
    "            .pipe(remove_outliers, columns=['CO2', 'TEMP'])\n",
    "\n",
    "            # adds features for acceleration\n",
    "            # such as CO2_ACC, TEMP_ACC, MOTION_ACC, IAQ_ACC\n",
    "            .pipe(acceleration_features)\n",
    "            .pipe(engineer_features)\n",
    "            .pipe(encode_time_features)\n",
    "            .pipe(utilize_motion_sensors)\n",
    "            .pipe(heartbeat, kommune, msg=\"Test\")\n",
    "            \n",
    "\n",
    "            # Creates other features such as \n",
    "            # \n",
    "            # AKTIVITET=lambda d: pd.factorize(d[\"TIDSPUNKT_TYPE\"])[0],\n",
    "            # DOW=lambda d: d[\"DATETIME\"].dt.dayofweek,\n",
    "            # HOUR=lambda d: d[\"DATETIME\"].dt.hour,\n",
    "            # DAY_TYPE=lambda d: pd.factorize(d[\"TYPE\"])[0],\n",
    "            #BOOKET=lambda d: d[\"BOOKET\"].fillna(0.0),\n",
    "            .pipe(preprocess_for_modelling)\n",
    "\n",
    "            # Modelling\n",
    "            .groupby(\"ID\").apply(\n",
    "                run_model,\n",
    "                features=features,\n",
    "                usage_limit=usage_limit, # the heuristic limit for usage, i.e. estimated anomaly rate / contamination\n",
    "                usage_coeff=usage_coeff, # A coefficient to scale the estimated anomaly rate / contamination\n",
    "                # Note that the contamination is estimated based on the number of timeslots with assumed activity\n",
    "                # assumed activity is either a booked timeslot or a timeslot in an external activity schema\n",
    "                # we cannot always assume that either is correct.\n",
    "                random_state=random_state, # random state for reproducibility\n",
    "            )\n",
    "            .reset_index(drop=True)\n",
    "\n",
    "            # heuristics\n",
    "            # adds heuristic rules such as \n",
    "            # - if CO2 is below 400, then the room is not in use\n",
    "            # - if CO2 is above 600 and CO2 is accelerating, then the room is in use\n",
    "            # - if CO2 is above 1000, then the room is in use\n",
    "            # - if the room is in use, then the usage score should be above 0.5\n",
    "            # - if the room is not in use, then the usage score should be below 0.5\n",
    "            \n",
    "            .pipe(\n",
    "                add_heuristics,\n",
    "                apply_rules=apply_rules\n",
    "            )\n",
    "\n",
    "            # Export plots\n",
    "            .pipe(export_plots, kommune=kommune, run_id=run_id)\n",
    "\n",
    "            # Postprocess \n",
    "            [[\"DATETIME\", \"ID\", \"in_use\", \"usage_score\"]]\n",
    "            .assign(KOMMUNE=kommune)\n",
    "\n",
    "            # Exit report\n",
    "            .pipe(exit_report, kommune=kommune, original_data=data)\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_flow(**kwargs):\n",
    "\n",
    "    run_id = f\"RUN-{datetime.now().strftime('%Y-%m-%d-%H-%M')}\"\n",
    "    print(f\"RUNNING FLOW '{run_id}'\\n\")\n",
    "\n",
    "    return {\n",
    "        \"run_id\": run_id,\n",
    "        \"data\": pd.concat(flow(run_id=run_id, **kwargs)),\n",
    "        \"paramters\": {k: v for k, v in kwargs.items() if k != \"data\"}\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Favrskov' 'Syddjurs' 'Aarhus']\n",
      "RUNNING FLOW 'RUN-2023-09-28-00-09'\n",
      "\n",
      "Running flow for Favrskov\n",
      "Turns 10166 rows into 10166 rows - Dropping 0.0K rows\n",
      "EXIT REPORT - Favrskov | Size: 10166 | Orig. size: 10166\n",
      "Mean usage rate: 0.13 | Mean usage score: 0.19\n",
      "\n",
      "\n",
      "\n",
      "Running flow for Syddjurs\n",
      "Turns 29760 rows into 4984 rows - Dropping 24.776K rows\n",
      "EXIT REPORT - Syddjurs | Size: 4984 | Orig. size: 29760\n",
      "Mean usage rate: 0.19 | Mean usage score: 0.21\n",
      "\n",
      "\n",
      "\n",
      "Running flow for Aarhus\n",
      "Turns 7692 rows into 7692 rows - Dropping 0.0K rows\n",
      "EXIT REPORT - Aarhus | Size: 7692 | Orig. size: 7692\n",
      "Mean usage rate: 0.15 | Mean usage score: 0.21\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results = run_flow(\n",
    "    data=DataLoader.load(\n",
    "        df=dataf,\n",
    "        print_nans=False,\n",
    "        steps=[\"all\"]\n",
    "    ),\n",
    "    usage_coeff=2.1,\n",
    "    usage_limit=.2,\n",
    "    random_state=123,\n",
    "    apply_rules=True,\n",
    "    features=room_features,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine with original data and save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    DataLoader.load(\n",
    "        path=\"data/full.csv\",\n",
    "        steps={\n",
    "            \"merge_dt\": dict(date=\"DATE\", time=\"TIME\", name=\"DATETIME\")\n",
    "        }\n",
    "    )\n",
    "    .merge(\n",
    "        results[\"data\"],\n",
    "        on=[\"DATETIME\", \"ID\", \"KOMMUNE\"],\n",
    "        how=\"left\"\n",
    "    )\n",
    "    [['DATE', 'TIME', 'DATETIME', 'ID', 'KOMMUNE', 'in_use', 'usage_score']]\n",
    "    .to_csv(f\"results/{results['run_id']}/results.csv\", index=False)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save run parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter out dataframes\n",
    "# save parameters as json\n",
    "with open(f\"results/{results['run_id']}/run_params.json\", 'w') as fp:\n",
    "    json.dump(results[\"paramters\"], fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Diagnostics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results[\"data\"].usage_score.value_counts(bins=20).sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results[\"data\"].usage_score.hist(\n",
    "    bins=100, \n",
    "    title=\"Usage score distribution\",\n",
    "    histnorm='percent',\n",
    "\n",
    "\n",
    ").update_layout(\n",
    "    showlegend=False,\n",
    "    xaxis_title=\"Usage score\",\n",
    "    # add '0' suffix to yaxis\n",
    "    # yaxis_tickformat=\".1%/100\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = results[\"data\"].groupby([\"KOMMUNE\", \"ID\"]).agg(\n",
    "    {\n",
    "        \"usage_score\": [\"mean\"],\n",
    "        \"in_use\": [\"mean\", \"count\"],\n",
    "    }\n",
    ").reset_index(level=[0, 1])\n",
    "df.columns = [\"KOMMUNE\", \"ID\", \"MEAN_USAGE_SCORE\", \"MEAN_IN_USE_RATE\", \"COUNT\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.round(2).plot.bar(\n",
    "    x=\"ID\",\n",
    "    y=\"MEAN_IN_USE_RATE\",\n",
    "    title=\"Mean predicted usage rate per room\",\n",
    "    color=\"KOMMUNE\",\n",
    "    width=600,\n",
    "    hover_data=df[[\"MEAN_IN_USE_RATE\", \"COUNT\"]],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = results[\"data\"].assign(DAY=lambda d: d[\"DATETIME\"].dt.date).groupby([\"KOMMUNE\", \"DAY\"]).agg(\n",
    "    {\n",
    "        \"usage_score\": [\"mean\"],\n",
    "        \"in_use\": [\"mean\", \"count\"],\n",
    "    }\n",
    ").reset_index(level=[0, 1])\n",
    "df.columns = [\"KOMMUNE\", \"DAY\", \"MEAN_USAGE_SCORE\", \"MEAN_USAGE_RATE\", \"COUNT\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = df.plot.bar(    \n",
    "    x='DAY',\n",
    "    y='MEAN_USAGE_RATE',\n",
    "    color='KOMMUNE',\n",
    "    width=800,\n",
    ")\n",
    "fig.update_traces(dict(marker_line_width=0))\n",
    "fig\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
