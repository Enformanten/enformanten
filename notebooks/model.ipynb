{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from DATETIMEimport DATETIMEn",
    "from pathlib import Path\n",
    "\n",
    "# anomaly detection\n",
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "# settings\n",
    "pd.options.plotting.backend = \"plotly\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMPORT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader():\n",
    "\n",
    "    SENSOR_COLUMNS = [\"CO2\", \"TEMP\", \"MOTION\", \"IAQ\", \"Skemalagt\", \"Booket\"]\n",
    "\n",
    "    @classmethod\n",
    "    def diagnose(cls, df, col, dfunc=\"unique\", **kwargs):\n",
    "        print(getattr(df[col], dfunc)(**kwargs))\n",
    "        return df\n",
    "\n",
    "    @classmethod\n",
    "    def fill_na(cls, df, cols, values, types):\n",
    "        return df.assign(\n",
    "            **{col: df[col].fillna(value).astype(_type) for col, value, _type in zip(cols, values, types)}\n",
    "            # .fillna(value).astype(_type\n",
    "            # fillna(method=\"ffill\", limit=2)\n",
    "        )\n",
    "\n",
    "    @classmethod\n",
    "    def display_missing_values(cls, df, print_nans):\n",
    "        if print_nans:\n",
    "            for i, munc in df.groupby('KOMMUNE'):\n",
    "                print(f\"Missing values for {i}\")\n",
    "                display(\n",
    "                    \n",
    "                    munc\n",
    "                    # filter between the first and last timeslot of activity\n",
    "                    .sort_values(\"DATETIME", ascending=True)\n",
    "                    [lambda d: d[\"DATETIME"].between(\n",
    "                            *(\n",
    "                                d.\n",
    "                                dropna(\n",
    "                                    subset=cls.SENSOR_COLUMNS,\n",
    "                                    how=\"all\"\n",
    "                                )\n",
    "                                [\"DATETIME"]\n",
    "                                .iloc[[0, -1]]\n",
    "                            ),\n",
    "                            inclusive=\"both\"\n",
    "                        )\n",
    "                    ]\n",
    "                    \n",
    "                    .groupby('ID')\n",
    "                    [cls.SENSOR_COLUMNS]\n",
    "                    .apply(lambda x: x.isnull().sum()/len(x))\n",
    "                    .style.format(precision=2)\n",
    "                    .background_gradient(cmap='Reds', axis=0, vmin=0, vmax=1)\n",
    "                )\n",
    "        return df\n",
    "\n",
    "    @classmethod \n",
    "    def merge_dt(cls, df, date, TIME name, sep=\" \"):\n",
    "        return df.assign(\n",
    "            **{name: lambda d: pd.to_DATETIME\n",
    "                    d[date] + sep + d[TIME\n",
    "                )\n",
    "            }\n",
    "        )\n",
    "\n",
    "    @classmethod\n",
    "    def drop_cols(cls, df, cols):\n",
    "        return df.drop(columns=cols)\n",
    "    \n",
    "    @classmethod\n",
    "    def full_process(cls, df, print_nans, **kwargs):\n",
    "        \n",
    "        return (\n",
    "            df\n",
    "            .pipe(cls.drop_cols, cols=[\"KOMMUNE_DATO_LOKALE_TIME"])\n",
    "            .pipe(cls.diagnose, col=\"KOMMUNE\", dfunc=\"unique\")\n",
    "            .pipe(cls.merge_dt, date=\"Date\", TIME\"TIME", name=\"DATETIME")\n",
    "            .pipe(cls.display_missing_values, print_nans)\n",
    "            .pipe(cls.fill_na, \n",
    "                cols=cls.SENSOR_COLUMNS[:-1],\n",
    "                values=[487, 20.0, 0.0, .03, 0.0],\n",
    "                types=[float, float, float, float, float]\n",
    "            )\n",
    "        )\n",
    "\n",
    "    @classmethod\n",
    "    def _load(cls, path):\n",
    "        return pd.read_csv(path)\n",
    "    \n",
    "    @classmethod\n",
    "    def load(cls, path = \"data/Skemaer.csv\", steps : dict = {}, print_nans = True, **kwargs):\n",
    "        dataf = cls._load(path)\n",
    "\n",
    "        if \"all\" in steps:\n",
    "            return cls.full_process(dataf, print_nans, **kwargs)\n",
    "\n",
    "        for func, func_kwargs in steps.items():\n",
    "            dataf = getattr(cls, func)(dataf, **func_kwargs)\n",
    "            \n",
    "        return dataf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = DataLoader.load(path=\"data/full.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['ID', 'KOMMUNE', 'SKOLE', 'Date', 'TIME, 'Dayname', 'TIDSPUNKT_TYPE',\n",
       "       'TYPE', 'Navn', 'KOMMUNE_DATO_LOKALE_TIME, 'CO2', 'TEMP', 'MOTION',\n",
       "       'IAQ', 'Booket', 'Skemalagt'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Normal Dag',\n",
       " 'Sommerferie',\n",
       " 'Påskeferie',\n",
       " 'Vinterferie',\n",
       " 'Efteraarsferie',\n",
       " 'Juleferie',\n",
       " 'Anden juledag',\n",
       " 'Juledag',\n",
       " 'Nytårsdag',\n",
       " 'Arbejdernes kampdag',\n",
       " 'Store bededag',\n",
       " 'Kristi himmelfartsdag',\n",
       " 'Pinsedag',\n",
       " 'Anden pinsedag']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.Navn.value_counts().index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(Normal Dag, Sommerferie, Påskeferie, Vinterferie, Efteraarsferie, Juleferie, Anden juledag, Juledag, Nytårsdag, Arbejdernes kampdag, Store bededag, Kristi himmelfartsdag, Pinsedag, Anden pinsedag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PREPROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_values(df, col, values):\n",
    "    return df[lambda d: d[col].isin(values)]\n",
    "\n",
    "\n",
    "def drop_inactive_ranges(df, range_length=5):\n",
    "    # Identify intervals of 5+ rows of identical values\n",
    "    to_drop = (\n",
    "        df.groupby(\"ID\")\n",
    "        [\"CO2\"].transform(\n",
    "            lambda x: x.rolling(range_length).apply(\n",
    "                lambda d: d.nunique() == 1\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    cdataf = df.drop(index=to_drop[lambda d: d.eq(1.0)].index)\n",
    "    print(f\"Turns {df.shape[0]} rows into {cdataf.shape[0]} rows - Dropping {(df.shape[0] - cdataf.shape[0])/1000}K rows\")\n",
    "    return cdataf\n",
    "\n",
    "def add_date_range_group(grp):\n",
    "    grp['Date_RANGE_GROUP'] = grp['DATETIME].transform(lambda x: (x.diff().dt.total_seconds()/ 60).ne(15).cumsum())\n",
    "    return grp\n",
    "\n",
    "def acceleration_features(df):\n",
    "\n",
    "    return (\n",
    "        df.sort_values(\"DATETIME").groupby(\"ID\").apply(add_date_range_group)\n",
    "        .reset_index(drop=True)\n",
    "        .assign(\n",
    "            \n",
    "            CO2_ACC=lambda d: d.groupby([\"ID\", \"Date_RANGE_GROUP\"])[\"CO2\"].ffill().pct_change().fillna(0).replace(np.inf, 0),\n",
    "            TEMP_ACC=lambda d: d.groupby([\"ID\", \"Date_RANGE_GROUP\"])[\"TEMP\"].ffill().pct_change().fillna(0).replace(np.inf, 0),\n",
    "            MOTION_ACC=lambda d: d.groupby([\"ID\", \"Date_RANGE_GROUP\"])[\"MOTION\"].ffill().pct_change().fillna(0).replace(np.inf, 0),\n",
    "            IAQ_ACC=lambda d: d.groupby([\"ID\", \"Date_RANGE_GROUP\"])[\"IAQ\"].ffill().pct_change().fillna(0).replace(np.inf, 0),\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "def preprocess_for_modelling(df):\n",
    "    return (\n",
    "        df\n",
    "        .assign(\n",
    "            AKTIVITET=lambda d: pd.factorize(d[\"TIDSPUNKT_TYPE\"])[0],\n",
    "            DOW=lambda d: d[\"DATETIME"].dt.dayofweek,\n",
    "            HOUR=lambda d: d[\"DATETIME"].dt.hour,\n",
    "            DAY_TYPE=lambda d: pd.factorize(d[\"TYPE\"])[0],\n",
    "            Booket=lambda d: d[\"Booket\"].fillna(0.0),\n",
    "            \n",
    "        )\n",
    "        .drop(columns=[\n",
    "            \"Date\",\n",
    "            \"TIDSPUNKT_TYPE\",\n",
    "            \"TYPE\",\n",
    "            \"Date_RANGE_GROUP\",\n",
    "            \"Dayname\",\n",
    "            \"TIME",\n",
    "            \"SKOLE\",\n",
    "            \"KOMMUNE\",\n",
    "            \"Navn\"\n",
    "            ]\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimate usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_usage(\n",
    "        data : pd.DataFrame,\n",
    "        usage_coeff : float = 2.1,\n",
    "        usage_limit : float = .2\n",
    "    ) -> float:\n",
    "\n",
    "    \"\"\"Estimate usage score for a given room based\n",
    "    on the number of timeslots with assumed activity,\n",
    "    e.g., a booked timeslot or a timeslot in school \n",
    "    schema \"\"\"\n",
    "\n",
    "    used_slots = max(\n",
    "        data[lambda d:\n",
    "            d[\"Skemalagt\"].astype(bool) | \n",
    "            d[\"Booket\"].fillna(0).astype(bool)\n",
    "        ].shape[0],  # number of slots with assumed activity\n",
    "        0.1\n",
    "    )\n",
    "    \n",
    "\n",
    "    # Evaluate against heuristic usage limit\n",
    "    est_usage = min(\n",
    "        usage_coeff * used_slots / data.shape[0], \n",
    "        usage_limit\n",
    "    )\n",
    "\n",
    "    # print(\n",
    "    #     f\"{data['ID'].iloc[0]} | \"\n",
    "    #     + f\"Est. usage score {est_usage:.2f} | \"\n",
    "    #     + f\"usage coeff: {usage_coeff:.2f}\"\n",
    "    # )\n",
    "    return est_usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MODELLING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_predict(df, **kwargs) -> tuple:\n",
    "    model_IF = IsolationForest(**kwargs)\n",
    "    model_IF.fit(df)\n",
    "\n",
    "    scores = model_IF.decision_function(df)\n",
    "    predictions = model_IF.predict(df)\n",
    "    return scores, predictions\n",
    "\n",
    "\n",
    "def format_predictions(preds : list) -> list:\n",
    "    return [1 if pred == -1 else 0 for pred in preds]\n",
    "\n",
    "\n",
    "def format_scores(scores : list) -> list:\n",
    "    \"\"\" Normalize scores in range [-1, 1] to\n",
    "    [0, 1] where 1 is most anomalous\n",
    "\n",
    "    Args:\n",
    "        scores (list): List of scores\n",
    "    \"\"\"\n",
    "    return np.interp(scores, (min(scores), max(scores)), (0, 1))\n",
    "\n",
    "\n",
    "def run_model(\n",
    "        room : pd.DataFrame,\n",
    "        features : list,\n",
    "        usage_coeff : float,\n",
    "        usage_limit : float,\n",
    "        **kwargs\n",
    "    ) -> pd.DataFrame:\n",
    "\n",
    "    \"\"\" Run fit_predict for every room in every building \"\"\"\n",
    "\n",
    "    _est_usage = estimate_usage(room, usage_coeff, usage_limit)\n",
    "    \n",
    "    scores, predictions = fit_predict(\n",
    "        room[features], \n",
    "        contamination=_est_usage,\n",
    "        **kwargs\n",
    "    )\n",
    "    return room.assign(\n",
    "        usage_score=1 - format_scores(scores),\n",
    "        in_use=format_predictions(predictions)\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Heuristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def add_heuristics(data, apply_rules):\n",
    "    \n",
    "    return data if not apply_rules else (\n",
    "\n",
    "        data\n",
    "        .assign(\n",
    "            in_use=lambda d: np.where(\n",
    "                d[\"CO2\"].lt(400),\n",
    "                0,\n",
    "                d[\"in_use\"],\n",
    "            ),\n",
    "        )\n",
    "        .assign( # Remove anomalies when CO2 accelerates or CO2 is high\n",
    "            in_use=lambda d: np.where(\n",
    "                (d[\"CO2_ACC\"].gt(0) & d[\"CO2\"].gt(600)) | (d[\"CO2\"].gt(1000)),\n",
    "                1, \n",
    "                d[\"in_use\"]\n",
    "            )\n",
    "        )\n",
    "\n",
    "        .assign(\n",
    "            # If the prior rules change the in_use value, then\n",
    "            # update the usage score to reflect this change\n",
    "            usage_score=lambda d: np.where(\n",
    "                (d[\"in_use\"].eq(1) & d[\"usage_score\"].lt(.5))\n",
    "                |\n",
    "                (d[\"in_use\"].eq(0) & d[\"usage_score\"].gt(.5)),\n",
    "                1 - d[\"usage_score\"],\n",
    "                d[\"usage_score\"]\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # .assign( \n",
    "        #     # Combine single usages - 1 IF n-1 = 1, n = 0, n+1 = 1, n+2 = 1\n",
    "        #     # Use for cases with a premature prediction of use\n",
    "        #     # NOTE: The check for 487 is to check for the fillna value.\n",
    "        #     # Otherwise, this does not work for Favrskov, since every\n",
    "        #     # other room in this school is a fillna value.\n",
    "        #     in_use=lambda d: np.where(\n",
    "        #         (d[\"in_use\"].shift(-1).eq(1) & d[\"CO2\"] != 487)\n",
    "        #         & \n",
    "        #         d[\"in_use\"].eq(0) \n",
    "        #         & \n",
    "        #         (d[\"in_use\"].shift(1).eq(1) & d[\"CO2\"].shift(1) != 487)\n",
    "        #         & \n",
    "        #         (d[\"in_use\"].shift(2).eq(1) & d[\"CO2\"].shift(2) != 487),\n",
    "        #         1,\n",
    "        #         d[\"in_use\"]\n",
    "        #     )\n",
    "        # )\n",
    "\n",
    "        # .assign( \n",
    "        #     # Remove single in_use - 0 IF n-1 = 0, n = 1, n+1 = 0\n",
    "        #     # NOTE: The check for 487 is to check for the fillna value.\n",
    "        #     # Otherwise, this does not work for Favrskov, since every\n",
    "        #     # other room in this school is a fillna value.\n",
    "        #     in_use=lambda d: np.where(\n",
    "        #         (d[\"in_use\"].shift(-1).eq(0) & d[\"CO2\"].shift(-1) != 487)\n",
    "        #         & \n",
    "        #         d[\"in_use\"].eq(1) \n",
    "        #         & \n",
    "        #         (d[\"in_use\"].shift(1).eq(0) & d[\"CO2\"].shift(1) != 487),\n",
    "        #         0,\n",
    "        #         d[\"in_use\"]\n",
    "        #     )\n",
    "        # )\n",
    "\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dir(run_id, _KOMMUNE) -> None:\n",
    "    Path(f\"results/{run_id}/{_KOMMUNE}\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "def export_plots(data, KOMMUNE, run_id):\n",
    "\n",
    "    for i, room in data.groupby(\"ID\"):\n",
    "\n",
    "        _KOMMUNE = KOMMUNE.lower()\n",
    "        create_dir(run_id, _KOMMUNE)\n",
    "\n",
    "        fig = room.plot.bar(    \n",
    "            x='DATETIME,\n",
    "            y='CO2',\n",
    "            color='in_use',\n",
    "            title=f'Anvendelsesmodel - Lokale {i} - {KOMMUNE} KOMMUNE',\n",
    "            width=3000,\n",
    "            hover_data=data[[\"CO2_ACC\"]],\n",
    "        )\n",
    "        fig.update_traces(dict(marker_line_width=0))\n",
    "        fig.write_html(f'results/{run_id}/{_KOMMUNE}/anomaly-{_KOMMUNE}-{i}.html')\n",
    "        \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exit report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exit_report(df, KOMMUNE, original_data):\n",
    "\n",
    "    original = original_data[lambda d: d[\"KOMMUNE\"] == KOMMUNE]\n",
    "    print(\n",
    "        f\"EXIT REPORT - {KOMMUNE} | Size: {df.shape[0]} | \"\n",
    "        + f\"Orig. size: {original.shape[0]}\\n\"\n",
    "        + f\"Mean usage rate: {df['in_use'].mean():.2f} | \"\n",
    "        + f\"Mean usage score: {df['usage_score'].mean():.2f}\\n\"\n",
    "        + \"\\n\\n\"\n",
    "    )\n",
    "    assert df['ID'].nunique() == original['ID'].nunique(), \"Flow dropped rooms!\"\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*** \n",
    "\n",
    "### Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set feature set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "room_features = [\n",
    "    # \"Skemalagt\",\n",
    "    \"CO2_ACC\",\n",
    "    # \"TEMP_ACC\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def heartbeat(df, KOMMUNE, msg=\"\"):\n",
    "    print(f\"🧪 | Size: {df.shape[0]} | {msg}\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flow(\n",
    "        data : pd.DataFrame,\n",
    "        run_id : str,\n",
    "        usage_coeff : float,\n",
    "        usage_limit : float,\n",
    "        random_state : int,\n",
    "        apply_rules : bool,\n",
    "        features : list,\n",
    "    ) -> pd.DataFrame:\n",
    "\n",
    "    \"\"\" Run the full flow for all municipalities\"\"\"\n",
    "                \n",
    "    for KOMMUNE in data[\"KOMMUNE\"].unique():\n",
    "        print(f\"Running flow for {KOMMUNE}\")\n",
    "\n",
    "        yield (\n",
    "            \n",
    "            # Processing\n",
    "            data\n",
    "            .pipe(filter_values, col=\"KOMMUNE\", values=[KOMMUNE])\n",
    "            .pipe(drop_inactive_ranges, range_length=5)\n",
    "            .pipe(acceleration_features)\n",
    "            .pipe(preprocess_for_modelling)\n",
    "\n",
    "            # Modelling\n",
    "            .groupby(\"ID\").apply(\n",
    "                run_model,\n",
    "                features=features,\n",
    "                usage_limit=usage_limit,\n",
    "                usage_coeff=usage_coeff,\n",
    "                random_state=random_state,\n",
    "            )\n",
    "            .reset_index(drop=True)\n",
    "\n",
    "            # heuristics\n",
    "            .pipe(\n",
    "                add_heuristics,\n",
    "                apply_rules=apply_rules\n",
    "            )\n",
    "\n",
    "            # Export plots\n",
    "            .pipe(export_plots, KOMMUNE=KOMMUNE, run_id=run_id)\n",
    "\n",
    "            # Postprocess \n",
    "            [[\"DATETIME", \"ID\", \"in_use\", \"usage_score\"]]\n",
    "            .assign(KOMMUNE=KOMMUNE)\n",
    "\n",
    "            # Exit report\n",
    "            .pipe(exit_report, KOMMUNE=KOMMUNE, original_data=data)\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_flow(**kwargs):\n",
    "\n",
    "    run_id = f\"RUN-{DATETIMEnow().strfTIME'%Y-%m-%d-%H-%M')}\"\n",
    "    print(f\"RUNNING FLOW '{run_id}'\\n\")\n",
    "\n",
    "    return {\n",
    "        \"run_id\": run_id,\n",
    "        \"data\": pd.concat(flow(run_id=run_id, **kwargs)),\n",
    "        \"paramters\": {k: v for k, v in kwargs.items() if k != \"data\"}\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = run_flow(\n",
    "    data=DataLoader.load(\n",
    "        path=\"data/full.csv\",\n",
    "        print_nans=False,\n",
    "        steps=[\"all\"]\n",
    "    ),\n",
    "    usage_coeff=2.1,\n",
    "    usage_limit=.2,\n",
    "    random_state=123,\n",
    "    apply_rules=True,\n",
    "    features=room_features,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine with original data and save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    DataLoader.load(\n",
    "        path=\"data/full.csv\",\n",
    "        steps={\n",
    "            \"merge_dt\": dict(date=\"Date\", TIME\"TIME", name=\"DATETIME")\n",
    "        }\n",
    "    )\n",
    "    .merge(\n",
    "        results[\"data\"],\n",
    "        on=[\"DATETIME", \"ID\", \"KOMMUNE\"],\n",
    "        how=\"left\"\n",
    "    )\n",
    "    [['Date', 'TIME, 'DATETIME, 'ID', 'KOMMUNE', 'in_use', 'usage_score']]\n",
    "    .to_csv(f\"results/{results['run_id']}/results.csv\", index=False)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "Date\n",
    "TIMEn",
    "DATETIMEn",
    "ID\n",
    "KOMMUNE\n",
    "in_use\n",
    "usage_score\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save run parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter out dataframes\n",
    "# save parameters as json\n",
    "with open(f\"results/{results['run_id']}/run_params.json\", 'w') as fp:\n",
    "    json.dump(results[\"paramters\"], fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Diagnostics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results[\"data\"].usage_score.value_counts(bins=20).sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results[\"data\"].usage_score.hist(\n",
    "    bins=100, \n",
    "    title=\"Usage score distribution\",\n",
    "    histnorm='percent',\n",
    "\n",
    "\n",
    ").update_layout(\n",
    "    showlegend=False,\n",
    "    xaxis_title=\"Usage score\",\n",
    "    # add '0' suffix to yaxis\n",
    "    # yaxis_tickformat=\".1%/100\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = results[\"data\"].groupby([\"KOMMUNE\", \"ID\"]).agg(\n",
    "    {\n",
    "        \"usage_score\": [\"mean\"],\n",
    "        \"in_use\": [\"mean\", \"count\"],\n",
    "    }\n",
    ").reset_index(level=[0, 1])\n",
    "df.columns = [\"KOMMUNE\", \"ID\", \"MEAN_USAGE_SCORE\", \"MEAN_IN_USE_RATE\", \"COUNT\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.round(2).plot.bar(\n",
    "    x=\"ID\",\n",
    "    y=\"MEAN_IN_USE_RATE\",\n",
    "    title=\"Mean predicted usage rate per room\",\n",
    "    color=\"KOMMUNE\",\n",
    "    width=600,\n",
    "    hover_data=df[[\"MEAN_IN_USE_RATE\", \"COUNT\"]],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = results[\"data\"].assign(DAY=lambda d: d[\"DATETIME"].dt.date).groupby([\"KOMMUNE\", \"DAY\"]).agg(\n",
    "    {\n",
    "        \"usage_score\": [\"mean\"],\n",
    "        \"in_use\": [\"mean\", \"count\"],\n",
    "    }\n",
    ").reset_index(level=[0, 1])\n",
    "df.columns = [\"KOMMUNE\", \"DAY\", \"MEAN_USAGE_SCORE\", \"MEAN_USAGE_RATE\", \"COUNT\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = df.plot.bar(    \n",
    "    x='DAY',\n",
    "    y='MEAN_USAGE_RATE',\n",
    "    color='KOMMUNE',\n",
    "    width=800,\n",
    ")\n",
    "fig.update_traces(dict(marker_line_width=0))\n",
    "fig\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
