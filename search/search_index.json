{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Enformanten / GovTech Midtjylland - Smart M 2 Enformanten is an energy saving tool for getting insights into energy usage (Distrinct Heating & Electricity) as well as analyzing room/booking usage on public schools/buildings. The project has developed two main parts: Analytics & AI. Analytics The analytics part of the project has leveraged an existing platform built by NTT DATA to ingest, transform and visualize data in dashboards. This has resulted in an overall dashboard with pages for getting insights into several views on energy usage spanding from vacation closing of buildings to the usage of energy in passive hours. The main output is data pipelines, transformations and a Power Bi Report AI The AI Room Utilization Model (Formally, Driftsoptimeringsmodellen in Danish) is a machine learning anomaly detection solution that uses sensor data to detect whether a given room has been in use (i.e. have had human activity) in a given time 15-minute time interval. This functionality is wrapped in a FastAPI web application that exposes a REST API for querying the model. The model is deployed to Azure as a Docker container and is hosted on an Azure Web App - But the model can be deployed to any cloud provider that supports Docker containers. NOTE: The AI booking system (The second AI component in the project) is not included in this repository but in a separate repository under this organization.","title":"Enformanten / GovTech Midtjylland - Smart M<sup>2</sup>"},{"location":"#enformanten-govtech-midtjylland-smart-m2","text":"Enformanten is an energy saving tool for getting insights into energy usage (Distrinct Heating & Electricity) as well as analyzing room/booking usage on public schools/buildings. The project has developed two main parts: Analytics & AI.","title":"Enformanten / GovTech Midtjylland - Smart M2"},{"location":"#analytics","text":"The analytics part of the project has leveraged an existing platform built by NTT DATA to ingest, transform and visualize data in dashboards. This has resulted in an overall dashboard with pages for getting insights into several views on energy usage spanding from vacation closing of buildings to the usage of energy in passive hours. The main output is data pipelines, transformations and a Power Bi Report","title":"Analytics"},{"location":"#ai","text":"The AI Room Utilization Model (Formally, Driftsoptimeringsmodellen in Danish) is a machine learning anomaly detection solution that uses sensor data to detect whether a given room has been in use (i.e. have had human activity) in a given time 15-minute time interval. This functionality is wrapped in a FastAPI web application that exposes a REST API for querying the model. The model is deployed to Azure as a Docker container and is hosted on an Azure Web App - But the model can be deployed to any cloud provider that supports Docker containers. NOTE: The AI booking system (The second AI component in the project) is not included in this repository but in a separate repository under this organization.","title":"AI"},{"location":"ai/algorithm/","text":"Algorithm: Room Occupancy Detection Simply stated, the objective is to provide a measurement of whether a given room had been in use at a given time. This information is intended for downstream analytics. The setting includes n municipalities, each with m schools and k rooms. Each room has a time series of multivariate sensor data collected at 15-minute intervals. Notations N: Set of municipalities, with size |N| = n M i : Set of schools in municipality i, with size |M i | = m i K j : Set of rooms in school j, with size |K j | = k j T: Set of discrete time intervals, sampled every 15 minutes, T = {t 1 , t 2 , ..., t T } x r,t : Multivariate sensor data for room r at time t Objective The objective is to find a function f that takes as input the sensor data x r,t for a room r at time t and outputs a label y r,t indicating whether the room has been in use. f: x r,t \u21a6 y r,t Here, y r,t is a binary label: y r,t = 1 if the room r is in use at time t y r,t = 0 if the room r is not in use at time t Motivation for Anomaly Detection Traditional supervised learning methods require labeled data, which we do not have in our case. This makes anomaly detection, particularly using Isolation Forest, an appealing choice because it can function well in unsupervised settings. Isolation Forest is highly suited for this task because it doesn't require labeled data for training. Instead, it focuses on isolating anomalies, providing an efficient and robust mechanism to classify rooms based on their usage. Isolation Forest Algorithm The Isolation Forest algorithm builds an ensemble of Isolation Trees, denoted as \u2111, for the data sample. Each tree is constructed recursively as follows: Randomly select a feature d from the data. Randomly select a split value v between the minimum and maximum values of feature d. Partition the data into two subsets: D left = {z r,t \u2208 D | z r,t [d] < v} and D right = {z r,t \u2208 D | z r,t [d] \u2265 v}. Repeat the process for each subset until the tree reaches a certain height h max , or the subset contains fewer than a certain number of points. The anomaly score for each data point z r,t , denoted by s(z r,t ), is computed as the average path length from the root to the terminal node across all trees in the forest: s(z r,t ) = (1 / |\u2111|) \u2211 t \u2208 \u2111 h t (z r,t ) Here, h t (z r,t ) is the path length of data point z r,t in tree t. Anomalies are the points that have shorter path lengths, meaning they are easier to isolate. Therefore, a smaller score indicates that the data point is more likely an anomaly. Scoring Function f Given the Isolation Forest's anomaly score function s(z_r,t) , we can set a threshold \u03b8 to determine the binary label y_r,t as follows: f(x_r,t) = 1 if s(g(x_r,t)) < \u03b8, else 0 In this case, if the anomaly score is less than the threshold \u03b8 , the room is considered to be in use ( y_r,t = 1 ). Otherwise, it is considered not in use ( y_r,t = 0 ). Individualized Room Models Due to the significant variation in room characteristics such as size, number of windows, and presence of air conditioning, using a single Isolation Forest model for all rooms is insufficient. These factors affect the sensor readings, and thereby the derived kinematic quantities, in a way that can be highly room-specific. Therefore, it becomes crucial to train a separate model for each room.","title":"Algorithm"},{"location":"ai/algorithm/#algorithm-room-occupancy-detection","text":"Simply stated, the objective is to provide a measurement of whether a given room had been in use at a given time. This information is intended for downstream analytics. The setting includes n municipalities, each with m schools and k rooms. Each room has a time series of multivariate sensor data collected at 15-minute intervals.","title":"Algorithm: Room Occupancy Detection"},{"location":"ai/algorithm/#notations","text":"N: Set of municipalities, with size |N| = n M i : Set of schools in municipality i, with size |M i | = m i K j : Set of rooms in school j, with size |K j | = k j T: Set of discrete time intervals, sampled every 15 minutes, T = {t 1 , t 2 , ..., t T } x r,t : Multivariate sensor data for room r at time t","title":"Notations"},{"location":"ai/algorithm/#objective","text":"The objective is to find a function f that takes as input the sensor data x r,t for a room r at time t and outputs a label y r,t indicating whether the room has been in use. f: x r,t \u21a6 y r,t Here, y r,t is a binary label: y r,t = 1 if the room r is in use at time t y r,t = 0 if the room r is not in use at time t","title":"Objective"},{"location":"ai/algorithm/#motivation-for-anomaly-detection","text":"Traditional supervised learning methods require labeled data, which we do not have in our case. This makes anomaly detection, particularly using Isolation Forest, an appealing choice because it can function well in unsupervised settings. Isolation Forest is highly suited for this task because it doesn't require labeled data for training. Instead, it focuses on isolating anomalies, providing an efficient and robust mechanism to classify rooms based on their usage.","title":"Motivation for Anomaly Detection"},{"location":"ai/algorithm/#isolation-forest-algorithm","text":"The Isolation Forest algorithm builds an ensemble of Isolation Trees, denoted as \u2111, for the data sample. Each tree is constructed recursively as follows: Randomly select a feature d from the data. Randomly select a split value v between the minimum and maximum values of feature d. Partition the data into two subsets: D left = {z r,t \u2208 D | z r,t [d] < v} and D right = {z r,t \u2208 D | z r,t [d] \u2265 v}. Repeat the process for each subset until the tree reaches a certain height h max , or the subset contains fewer than a certain number of points. The anomaly score for each data point z r,t , denoted by s(z r,t ), is computed as the average path length from the root to the terminal node across all trees in the forest: s(z r,t ) = (1 / |\u2111|) \u2211 t \u2208 \u2111 h t (z r,t ) Here, h t (z r,t ) is the path length of data point z r,t in tree t. Anomalies are the points that have shorter path lengths, meaning they are easier to isolate. Therefore, a smaller score indicates that the data point is more likely an anomaly.","title":"Isolation Forest Algorithm"},{"location":"ai/algorithm/#scoring-function-f","text":"Given the Isolation Forest's anomaly score function s(z_r,t) , we can set a threshold \u03b8 to determine the binary label y_r,t as follows: f(x_r,t) = 1 if s(g(x_r,t)) < \u03b8, else 0 In this case, if the anomaly score is less than the threshold \u03b8 , the room is considered to be in use ( y_r,t = 1 ). Otherwise, it is considered not in use ( y_r,t = 0 ).","title":"Scoring Function f"},{"location":"ai/algorithm/#individualized-room-models","text":"Due to the significant variation in room characteristics such as size, number of windows, and presence of air conditioning, using a single Isolation Forest model for all rooms is insufficient. These factors affect the sensor readings, and thereby the derived kinematic quantities, in a way that can be highly room-specific. Therefore, it becomes crucial to train a separate model for each room.","title":"Individualized Room Models"},{"location":"ai/deployment/","text":"Deployment The AI solution is designed to be exposed as a RESTful FastAPI (V0.103.2) web application. The /ai folder contains a Dockerfile that can be used to build a Docker image of the application. The image can then be deployed to a cloud-based container service - In our case, Azure Container Registry. The application is designed to be deployed as a single container, but it can be scaled horizontally to multiple instances if needed. As evident from the figure above, the solution has multiple modules even though the name suggests that it is only a single AI model. The technical specifications for each these modules can be found in /references . Authentication In its current state, the solution uses the OAuth2 protocol for authentication. As such, the solution is closed to traffic from unauthorized users. A list of pre-defined users are configured in an environment variabel file and will be created in the in-server sqlite database on initial deployment (Please see overview for details on adding an .env file for local development, and the interactive API documentation for details on the authentication process at <app-url>/docs ). GitHub Actions The solution is set up with GitHub Actions for continuous integration and deployment. The workflow is triggered on push to the main branch. Please see the .github/workflows folder for details on the workflow. Integration The solution integrates with Azure Data Factory data flow and the associated (Snowflake) database but is otherwise closed to traffic. The specifications for the connecting pipeline in Azure Data Factory is shown below. Note that the pipeline first authenticates using the pre-defined credentials associated with its user and then calls the train endpoint. The endpoint is responsible for training the AI model and updating the model registry. The model registry is then used by the predict endpoint to return the predicted utilization rate for a given room. Extracted 30.10.2023 { \"name\" : \"PL_DRIFTOPTIMERINGSMODEL\" , \"properties\" : { \"description\" : \"This pipelines is responsible for training the AI-model \\\"Driftoptimeringsmodel\\\" once a month. \" , \"activities\" : [ { \"name\" : \"Get token\" , \"type\" : \"WebActivity\" , \"dependsOn\" : [ { \"activity\" : \"Get Config\" , \"dependencyConditions\" : [ \"Succeeded\" ] } ], \"policy\" : { \"timeout\" : \"0.12:00:00\" , \"retry\" : 0 , \"retryIntervalInSeconds\" : 30 , \"secureOutput\" : false , \"secureInput\" : false }, \"userProperties\" : [], \"typeProperties\" : { \"url\" : \"https://app-govtech.azurewebsites.net/auth/jwt/login\" , \"method\" : \"POST\" , \"headers\" : { \"accept\" : \"application/json\" , \"Content-Type\" : \"application/x-www-form-urlencoded\" }, \"body\" : { \"value\" : \"@activity('Get Config').output.value\" , \"type\" : \"Expression\" } } }, { \"name\" : \"Train Model\" , \"type\" : \"Lookup\" , \"dependsOn\" : [ { \"activity\" : \"Get token\" , \"dependencyConditions\" : [ \"Succeeded\" ] }, { \"activity\" : \"UPDATE_DRIFTOPTIMERING_TRAINING\" , \"dependencyConditions\" : [ \"Succeeded\" ] } ], \"policy\" : { \"timeout\" : \"0.12:00:00\" , \"retry\" : 0 , \"retryIntervalInSeconds\" : 30 , \"secureOutput\" : false , \"secureInput\" : false }, \"userProperties\" : [], \"typeProperties\" : { \"source\" : { \"type\" : \"JsonSource\" , \"storeSettings\" : { \"type\" : \"HttpReadSettings\" , \"requestMethod\" : \"POST\" , \"additionalHeaders\" : { \"value\" : \"@{concat('Authorization: Bearer ' , activity('Get token').output.access_token)}\" , \"type\" : \"Expression\" }, \"requestTimeout\" : \"10:00:00\" }, \"formatSettings\" : { \"type\" : \"JsonReadSettings\" } }, \"dataset\" : { \"referenceName\" : \"DS_DRIFTOPTIMERINGSMODEL_TRAIN\" , \"type\" : \"DatasetReference\" } } }, { \"name\" : \"UPDATE_DRIFTOPTIMERING_TRAINING\" , \"type\" : \"Lookup\" , \"dependsOn\" : [], \"policy\" : { \"timeout\" : \"0.12:00:00\" , \"retry\" : 0 , \"retryIntervalInSeconds\" : 30 , \"secureOutput\" : false , \"secureInput\" : false }, \"userProperties\" : [], \"typeProperties\" : { \"source\" : { \"type\" : \"SnowflakeSource\" , \"query\" : \"CALL \\\"RAW\\\".UPDATEFEATURIZDISTINCT('GOVTECH_DB', 'RAW', '4_FEATURIZ_DRIFTOPTIMERING_TRAINING', '3_CLEANSED_DRIFTOPTIMERING_TRAINING')\" , \"exportSettings\" : { \"type\" : \"SnowflakeExportCopyCommand\" } }, \"dataset\" : { \"referenceName\" : \"DS_SNOWFLAKE\" , \"type\" : \"DatasetReference\" , \"parameters\" : { \"database\" : \"GOVTECH_DB\" , \"table\" : \"4_FEATURIZ_DRIFTOPTIMERING_TRAINING\" , \"schema\" : \"RAW\" } } } }, { \"name\" : \"Get Config\" , \"type\" : \"WebActivity\" , \"dependsOn\" : [], \"policy\" : { \"timeout\" : \"0.12:00:00\" , \"retry\" : 0 , \"retryIntervalInSeconds\" : 30 , \"secureOutput\" : false , \"secureInput\" : false }, \"userProperties\" : [], \"typeProperties\" : { \"url\" : \"<HIDDEN>\" , \"method\" : \"GET\" , \"authentication\" : { \"type\" : \"MSI\" , \"resource\" : \"https://vault.azure.net\" } } } ], \"folder\" : { \"name\" : \"Driftoptimeringsmodel\" }, \"annotations\" : [], \"lastPublishTime\" : \"2023-10-13T12:27:21Z\" }, \"type\" : \"Microsoft.DataFactory/factories/pipelines\" } Remote Resource Specification Extracted 30.10.2023 { \"id\" : \"/subscriptions/<HIDDEN/resourceGroups/rg-govtech/providers/Microsoft.Web/sites/app-govtech\" , \"name\" : \"app-govtech\" , \"type\" : \"Microsoft.Web/sites\" , \"kind\" : \"app,linux,container\" , \"location\" : \"West Europe\" , \"tags\" : {}, \"properties\" : { \"name\" : \"app-govtech\" , \"state\" : \"Running\" , \"hostNames\" : [ \"app-govtech.azurewebsites.net\" ], \"webSpace\" : \"rg-govtech-WestEuropewebspace-Linux\" , \"selfLink\" : \"<HIDDEN>\" , \"repositorySiteName\" : \"app-govtech\" , \"owner\" : null , \"usageState\" : 0 , \"enabled\" : true , \"adminEnabled\" : true , \"afdEnabled\" : false , \"enabledHostNames\" : [ \"app-govtech.azurewebsites.net\" , \"app-govtech.scm.azurewebsites.net\" ], \"siteProperties\" : { \"metadata\" : null , \"properties\" : [ { \"name\" : \"LinuxFxVersion\" , \"value\" : \"DOCKER|acrgovtech.azurecr.io/acrgovtech/production:<HIDDEN>\" }, { \"name\" : \"WindowsFxVersion\" , \"value\" : null } ], \"appSettings\" : null }, \"availabilityState\" : 0 , \"sslCertificates\" : null , \"csrs\" : [], \"cers\" : null , \"siteMode\" : null , \"hostNameSslStates\" : [ { \"name\" : \"app-govtech.azurewebsites.net\" , \"sslState\" : 0 , \"ipBasedSslResult\" : null , \"virtualIP\" : null , \"virtualIPv6\" : null , \"thumbprint\" : null , \"certificateResourceId\" : null , \"toUpdate\" : null , \"toUpdateIpBasedSsl\" : null , \"ipBasedSslState\" : 0 , \"hostType\" : 0 }, { \"name\" : \"app-govtech.scm.azurewebsites.net\" , \"sslState\" : 0 , \"ipBasedSslResult\" : null , \"virtualIP\" : null , \"virtualIPv6\" : null , \"thumbprint\" : null , \"certificateResourceId\" : null , \"toUpdate\" : null , \"toUpdateIpBasedSsl\" : null , \"ipBasedSslState\" : 0 , \"hostType\" : 1 } ], \"computeMode\" : null , \"serverFarm\" : null , \"serverFarmId\" : \"/subscriptions/<HIDDEN>/resourceGroups/rg-govtech/providers/Microsoft.Web/serverfarms/ASP-govtech\" , \"reserved\" : true , \"isXenon\" : false , \"hyperV\" : false , \"lastModifiedTimeUtc\" : \"2023-10-30T14:43:21.26\" , \"storageRecoveryDefaultState\" : \"Running\" , \"contentAvailabilityState\" : 0 , \"runtimeAvailabilityState\" : 0 , \"dnsConfiguration\" : {}, \"vnetRouteAllEnabled\" : false , \"containerAllocationSubnet\" : null , \"useContainerLocalhostBindings\" : null , \"vnetImagePullEnabled\" : false , \"vnetContentShareEnabled\" : false , \"siteConfig\" : { \"numberOfWorkers\" : 1 , \"defaultDocuments\" : null , \"netFrameworkVersion\" : null , \"phpVersion\" : null , \"pythonVersion\" : null , \"nodeVersion\" : null , \"powerShellVersion\" : null , \"linuxFxVersion\" : \"DOCKER|acrgovtech.azurecr.io/acrgovtech/production:<HIDDEN>\" , \"windowsFxVersion\" : null , \"windowsConfiguredStacks\" : null , \"requestTracingEnabled\" : null , \"remoteDebuggingEnabled\" : null , \"remoteDebuggingVersion\" : null , \"httpLoggingEnabled\" : null , \"azureMonitorLogCategories\" : null , \"acrUseManagedIdentityCreds\" : false , \"acrUserManagedIdentityID\" : null , \"logsDirectorySizeLimit\" : null , \"detailedErrorLoggingEnabled\" : null , \"publishingUsername\" : null , \"publishingPassword\" : null , \"appSettings\" : null , \"metadata\" : null , \"connectionStrings\" : null , \"machineKey\" : null , \"handlerMappings\" : null , \"documentRoot\" : null , \"scmType\" : null , \"use32BitWorkerProcess\" : null , \"webSocketsEnabled\" : null , \"alwaysOn\" : true , \"javaVersion\" : null , \"javaContainer\" : null , \"javaContainerVersion\" : null , \"appCommandLine\" : null , \"managedPipelineMode\" : null , \"virtualApplications\" : null , \"winAuthAdminState\" : null , \"winAuthTenantState\" : null , \"customAppPoolIdentityAdminState\" : null , \"customAppPoolIdentityTenantState\" : null , \"runtimeADUser\" : null , \"runtimeADUserPassword\" : null , \"loadBalancing\" : null , \"routingRules\" : null , \"experiments\" : null , \"limits\" : null , \"autoHealEnabled\" : null , \"autoHealRules\" : null , \"tracingOptions\" : null , \"vnetName\" : null , \"vnetRouteAllEnabled\" : null , \"vnetPrivatePortsCount\" : null , \"publicNetworkAccess\" : null , \"cors\" : null , \"push\" : null , \"apiDefinition\" : null , \"apiManagementConfig\" : null , \"autoSwapSlotName\" : null , \"localMySqlEnabled\" : null , \"managedServiceIdentityId\" : null , \"xManagedServiceIdentityId\" : null , \"keyVaultReferenceIdentity\" : null , \"ipSecurityRestrictions\" : null , \"ipSecurityRestrictionsDefaultAction\" : null , \"scmIpSecurityRestrictions\" : null , \"scmIpSecurityRestrictionsDefaultAction\" : null , \"scmIpSecurityRestrictionsUseMain\" : null , \"http20Enabled\" : false , \"minTlsVersion\" : null , \"minTlsCipherSuite\" : null , \"supportedTlsCipherSuites\" : null , \"scmMinTlsVersion\" : null , \"ftpsState\" : null , \"preWarmedInstanceCount\" : null , \"functionAppScaleLimit\" : 0 , \"elasticWebAppScaleLimit\" : null , \"healthCheckPath\" : null , \"fileChangeAuditEnabled\" : null , \"functionsRuntimeScaleMonitoringEnabled\" : null , \"websiteTimeZone\" : null , \"minimumElasticInstanceCount\" : 0 , \"azureStorageAccounts\" : null , \"http20ProxyFlag\" : null , \"sitePort\" : null , \"antivirusScanEnabled\" : null , \"storageType\" : null , \"sitePrivateLinkHostEnabled\" : null }, \"daprConfig\" : null , \"deploymentId\" : \"app-govtech\" , \"slotName\" : null , \"trafficManagerHostNames\" : null , \"sku\" : \"PremiumV3\" , \"scmSiteAlsoStopped\" : false , \"targetSwapSlot\" : null , \"hostingEnvironment\" : null , \"hostingEnvironmentProfile\" : null , \"clientAffinityEnabled\" : false , \"clientCertEnabled\" : false , \"clientCertMode\" : 0 , \"clientCertExclusionPaths\" : null , \"hostNamesDisabled\" : false , \"ipMode\" : \"IPv4\" , \"vnetBackupRestoreEnabled\" : false , \"domainVerificationIdentifiers\" : null , \"customDomainVerificationId\" : \"<HIDDEN>\" , \"kind\" : \"app,linux,container\" , \"managedEnvironmentId\" : null , \"inboundIpAddress\" : \"<HIDDEN>\" , \"possibleInboundIpAddresses\" : \"<HIDDEN>\" , \"ftpUsername\" : \"app-govtech\\\\$app-govtech\" , \"ftpsHostName\" : \"<HIDDEN>\" , \"outboundIpAddresses\" : \"<HIDDEN>\" , \"possibleOutboundIpAddresses\" : \"<HIDDEN>\" , \"containerSize\" : 0 , \"dailyMemoryTimeQuota\" : 0 , \"suspendedTill\" : null , \"siteDisabledReason\" : 0 , \"functionExecutionUnitsCache\" : null , \"maxNumberOfWorkers\" : null , \"homeStamp\" : \"waws-prod-am2-649\" , \"cloningInfo\" : null , \"hostingEnvironmentId\" : null , \"tags\" : {}, \"resourceGroup\" : \"rg-govtech\" , \"defaultHostName\" : \"app-govtech.azurewebsites.net\" , \"slotSwapStatus\" : null , \"httpsOnly\" : true , \"endToEndEncryptionEnabled\" : false , \"functionsRuntimeAdminIsolationEnabled\" : false , \"redundancyMode\" : 0 , \"inProgressOperationId\" : null , \"geoDistributions\" : null , \"privateEndpointConnections\" : [], \"publicNetworkAccess\" : \"Enabled\" , \"buildVersion\" : null , \"targetBuildVersion\" : null , \"migrationState\" : null , \"eligibleLogCategories\" : \"AppServiceAppLogs,AppServiceAuditLogs,AppServiceConsoleLogs,AppServiceHTTPLogs,AppServiceIPSecAuditLogs,AppServicePlatformLogs,ScanLogs,AppServiceFileAuditLogs,AppServiceAntivirusScanAuditLogs\" , \"inFlightFeatures\" : [], \"storageAccountRequired\" : false , \"virtualNetworkSubnetId\" : null , \"keyVaultReferenceIdentity\" : \"SystemAssigned\" , \"defaultHostNameScope\" : 0 , \"privateLinkIdentifiers\" : null , \"sshEnabled\" : null } } API Documentation The API documentation is generated by FastAPI and can be accessed at <app-url>/docs . The documentation is interactive and allows for testing of the API endpoints. The documentation is also available in JSON format at <app-url>/openapi.json . For transparency, the JSON specification is also given below { \"openapi\" : \"3.1.0\" , \"info\" :{ \"title\" : \"Tilly API\" , \"description\" : \"Unsupervised anomaly detection for room usage\" , \"version\" : \"54c0cc62-bekkeremil-Oct 30, 15:41\" }, \"paths\" :{ \"/\" :{ \"get\" :{ \"tags\" :[ \"dashboard\" ], \"summary\" : \"Read Root\" , \"description\" : \"Serve the Tilly Dashboard.\\n\\nThis route returns an HTML response that serves the Tilly dashboard.\\n\\nArgs:\\n request (Request): The FastAPI request object.\\n\\nReturns:\\n HTMLResponse: The HTML response containing the rendered Tilly dashboard.\\n\\nExamples:\\n ```bash\\n curl http://localhost:8000/\\n ```\\n\\n This will return the HTML content of the Tilly dashboard.\" , \"operationId\" : \"read_root__get\" , \"responses\" :{ \"200\" :{ \"description\" : \"Successful Response\" , \"content\" :{ \"text/html\" :{ \"schema\" :{ \"type\" : \"string\" } } } } } } }, \"/plots_structure\" :{ \"get\" :{ \"tags\" :[ \"dashboard\" ], \"summary\" : \"Get Plots Structure\" , \"description\" : \"Get Plot Directory Structure.\\n\\nThis route returns the directory structure of the plots as a JSON object.\\nIf the directory structure is invalid, a 404 HTTP error is raised.\\n\\nReturns:\\n Optional[Dict]: The dictionary representing the directory structure.\\n\\nExamples:\\n ```bash\\n curl http://localhost:8000/plots_structure\\n ```\\n\\n This will return a JSON object representing the directory structure of\\n the plots.\" , \"operationId\" : \"get_plots_structure_plots_structure_get\" , \"responses\" :{ \"200\" :{ \"description\" : \"Successful Response\" , \"content\" :{ \"application/json\" :{ \"schema\" :{ \"anyOf\" :[ { \"type\" : \"object\" }, { \"type\" : \"null\" } ], \"title\" : \"Response Get Plots Structure Plots Structure Get\" } } } } } } }, \"/auth/jwt/login\" :{ \"post\" :{ \"tags\" :[ \"auth\" ], \"summary\" : \"Auth:Jwt.Login\" , \"operationId\" : \"auth_jwt_login_auth_jwt_login_post\" , \"requestBody\" :{ \"content\" :{ \"application/x-www-form-urlencoded\" :{ \"schema\" :{ \"$ref\" : \"#/components/schemas/Body_auth_jwt_login_auth_jwt_login_post\" } } }, \"required\" : true }, \"responses\" :{ \"200\" :{ \"description\" : \"Successful Response\" , \"content\" :{ \"application/json\" :{ \"schema\" :{ \"$ref\" : \"#/components/schemas/BearerResponse\" }, \"example\" :{ \"access_token\" : \"eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJ1c2VyX2lkIjoiOTIyMWZmYzktNjQwZi00MzcyLTg2ZDMtY2U2NDJjYmE1NjAzIiwiYXVkIjoiZmFzdGFwaS11c2VyczphdXRoIiwiZXhwIjoxNTcxNTA0MTkzfQ.M10bjOe45I5Ncu_uXvOmVV8QxnL-nZfcH96U90JaocI\" , \"token_type\" : \"bearer\" } } } }, \"400\" :{ \"description\" : \"Bad Request\" , \"content\" :{ \"application/json\" :{ \"schema\" :{ \"$ref\" : \"#/components/schemas/ErrorModel\" }, \"examples\" :{ \"LOGIN_BAD_CREDENTIALS\" :{ \"summary\" : \"Bad credentials or the user is inactive.\" , \"value\" :{ \"detail\" : \"LOGIN_BAD_CREDENTIALS\" } }, \"LOGIN_USER_NOT_VERIFIED\" :{ \"summary\" : \"The user is not verified.\" , \"value\" :{ \"detail\" : \"LOGIN_USER_NOT_VERIFIED\" } } } } } }, \"422\" :{ \"description\" : \"Validation Error\" , \"content\" :{ \"application/json\" :{ \"schema\" :{ \"$ref\" : \"#/components/schemas/HTTPValidationError\" } } } } } } }, \"/auth/jwt/logout\" :{ \"post\" :{ \"tags\" :[ \"auth\" ], \"summary\" : \"Auth:Jwt.Logout\" , \"operationId\" : \"auth_jwt_logout_auth_jwt_logout_post\" , \"responses\" :{ \"200\" :{ \"description\" : \"Successful Response\" , \"content\" :{ \"application/json\" :{ \"schema\" :{ } } } }, \"401\" :{ \"description\" : \"Missing token or inactive user.\" } }, \"security\" :[ { \"OAuth2PasswordBearer\" :[] } ] } }, \"/auth/register\" :{ \"post\" :{ \"tags\" :[ \"auth\" ], \"summary\" : \"Register:Register\" , \"operationId\" : \"register_register_auth_register_post\" , \"requestBody\" :{ \"content\" :{ \"application/json\" :{ \"schema\" :{ \"$ref\" : \"#/components/schemas/UserCreate\" } } }, \"required\" : true }, \"responses\" :{ \"201\" :{ \"description\" : \"Successful Response\" , \"content\" :{ \"application/json\" :{ \"schema\" :{ \"$ref\" : \"#/components/schemas/UserRead\" } } } }, \"400\" :{ \"description\" : \"Bad Request\" , \"content\" :{ \"application/json\" :{ \"schema\" :{ \"$ref\" : \"#/components/schemas/ErrorModel\" }, \"examples\" :{ \"REGISTER_USER_ALREADY_EXISTS\" :{ \"summary\" : \"A user with this email already exists.\" , \"value\" :{ \"detail\" : \"REGISTER_USER_ALREADY_EXISTS\" } }, \"REGISTER_INVALID_PASSWORD\" :{ \"summary\" : \"Password validation failed.\" , \"value\" :{ \"detail\" :{ \"code\" : \"REGISTER_INVALID_PASSWORD\" , \"reason\" : \"Password should beat least 3 characters\" } } } } } } }, \"422\" :{ \"description\" : \"Validation Error\" , \"content\" :{ \"application/json\" :{ \"schema\" :{ \"$ref\" : \"#/components/schemas/HTTPValidationError\" } } } } } } }, \"/users/me\" :{ \"get\" :{ \"tags\" :[ \"users\" ], \"summary\" : \"Users:Current User\" , \"operationId\" : \"users_current_user_users_me_get\" , \"responses\" :{ \"200\" :{ \"description\" : \"Successful Response\" , \"content\" :{ \"application/json\" :{ \"schema\" :{ \"$ref\" : \"#/components/schemas/UserRead\" } } } }, \"401\" :{ \"description\" : \"Missing token or inactive user.\" } }, \"security\" :[ { \"OAuth2PasswordBearer\" :[] } ] }, \"patch\" :{ \"tags\" :[ \"users\" ], \"summary\" : \"Users:Patch Current User\" , \"operationId\" : \"users_patch_current_user_users_me_patch\" , \"requestBody\" :{ \"content\" :{ \"application/json\" :{ \"schema\" :{ \"$ref\" : \"#/components/schemas/UserUpdate\" } } }, \"required\" : true }, \"responses\" :{ \"200\" :{ \"description\" : \"Successful Response\" , \"content\" :{ \"application/json\" :{ \"schema\" :{ \"$ref\" : \"#/components/schemas/UserRead\" } } } }, \"401\" :{ \"description\" : \"Missing token or inactive user.\" }, \"400\" :{ \"description\" : \"Bad Request\" , \"content\" :{ \"application/json\" :{ \"schema\" :{ \"$ref\" : \"#/components/schemas/ErrorModel\" }, \"examples\" :{ \"UPDATE_USER_EMAIL_ALREADY_EXISTS\" :{ \"summary\" : \"A user with this email already exists.\" , \"value\" :{ \"detail\" : \"UPDATE_USER_EMAIL_ALREADY_EXISTS\" } }, \"UPDATE_USER_INVALID_PASSWORD\" :{ \"summary\" : \"Password validation failed.\" , \"value\" :{ \"detail\" :{ \"code\" : \"UPDATE_USER_INVALID_PASSWORD\" , \"reason\" : \"Password should beat least 3 characters\" } } } } } } }, \"422\" :{ \"description\" : \"Validation Error\" , \"content\" :{ \"application/json\" :{ \"schema\" :{ \"$ref\" : \"#/components/schemas/HTTPValidationError\" } } } } }, \"security\" :[ { \"OAuth2PasswordBearer\" :[] } ] } }, \"/users/{id}\" :{ \"get\" :{ \"tags\" :[ \"users\" ], \"summary\" : \"Users:User\" , \"operationId\" : \"users_user_users__id__get\" , \"security\" :[ { \"OAuth2PasswordBearer\" :[] } ], \"parameters\" :[ { \"name\" : \"id\" , \"in\" : \"path\" , \"required\" : true , \"schema\" :{ \"type\" : \"string\" , \"title\" : \"Id\" } } ], \"responses\" :{ \"200\" :{ \"description\" : \"Successful Response\" , \"content\" :{ \"application/json\" :{ \"schema\" :{ \"$ref\" : \"#/components/schemas/UserRead\" } } } }, \"401\" :{ \"description\" : \"Missing token or inactive user.\" }, \"403\" :{ \"description\" : \"Not a superuser.\" }, \"404\" :{ \"description\" : \"The user does not exist.\" }, \"422\" :{ \"description\" : \"Validation Error\" , \"content\" :{ \"application/json\" :{ \"schema\" :{ \"$ref\" : \"#/components/schemas/HTTPValidationError\" } } } } } }, \"patch\" :{ \"tags\" :[ \"users\" ], \"summary\" : \"Users:Patch User\" , \"operationId\" : \"users_patch_user_users__id__patch\" , \"security\" :[ { \"OAuth2PasswordBearer\" :[] } ], \"parameters\" :[ { \"name\" : \"id\" , \"in\" : \"path\" , \"required\" : true , \"schema\" :{ \"type\" : \"string\" , \"title\" : \"Id\" } } ], \"requestBody\" :{ \"required\" : true , \"content\" :{ \"application/json\" :{ \"schema\" :{ \"$ref\" : \"#/components/schemas/UserUpdate\" } } } }, \"responses\" :{ \"200\" :{ \"description\" : \"Successful Response\" , \"content\" :{ \"application/json\" :{ \"schema\" :{ \"$ref\" : \"#/components/schemas/UserRead\" } } } }, \"401\" :{ \"description\" : \"Missing token or inactive user.\" }, \"403\" :{ \"description\" : \"Not a superuser.\" }, \"404\" :{ \"description\" : \"The user does not exist.\" }, \"400\" :{ \"content\" :{ \"application/json\" :{ \"examples\" :{ \"UPDATE_USER_EMAIL_ALREADY_EXISTS\" :{ \"summary\" : \"A user with this email already exists.\" , \"value\" :{ \"detail\" : \"UPDATE_USER_EMAIL_ALREADY_EXISTS\" } }, \"UPDATE_USER_INVALID_PASSWORD\" :{ \"summary\" : \"Password validation failed.\" , \"value\" :{ \"detail\" :{ \"code\" : \"UPDATE_USER_INVALID_PASSWORD\" , \"reason\" : \"Password should beat least 3 characters\" } } } }, \"schema\" :{ \"$ref\" : \"#/components/schemas/ErrorModel\" } } }, \"description\" : \"Bad Request\" }, \"422\" :{ \"description\" : \"Validation Error\" , \"content\" :{ \"application/json\" :{ \"schema\" :{ \"$ref\" : \"#/components/schemas/HTTPValidationError\" } } } } } }, \"delete\" :{ \"tags\" :[ \"users\" ], \"summary\" : \"Users:Delete User\" , \"operationId\" : \"users_delete_user_users__id__delete\" , \"security\" :[ { \"OAuth2PasswordBearer\" :[] } ], \"parameters\" :[ { \"name\" : \"id\" , \"in\" : \"path\" , \"required\" : true , \"schema\" :{ \"type\" : \"string\" , \"title\" : \"Id\" } } ], \"responses\" :{ \"204\" :{ \"description\" : \"Successful Response\" }, \"401\" :{ \"description\" : \"Missing token or inactive user.\" }, \"403\" :{ \"description\" : \"Not a superuser.\" }, \"404\" :{ \"description\" : \"The user does not exist.\" }, \"422\" :{ \"description\" : \"Validation Error\" , \"content\" :{ \"application/json\" :{ \"schema\" :{ \"$ref\" : \"#/components/schemas/HTTPValidationError\" } } } } } } }, \"/train\" :{ \"post\" :{ \"tags\" :[ \"train\" ], \"summary\" : \"Train\" , \"description\" : \"Initiate training of room-specific ML models for all rooms in data source.\\n\\nCalls the `training_flow` function as a async background task.\\n\\n**NOTE**: Authentication is required for this endpoint.\\n\\nArgs:\\n\\n request: Request: FastAPI Request object. Not used, but kept for FastAPI\\n dependency injection.\\n\\n background_tasks: FastAPI BackgroundTasks for running functions in the\\n background.\\n\\n session: SQLAlchemy Session object for database interactions\\n (injected via FastAPI's dependency system).\\n\\nReturns:\\n\\n dict: A dictionary containing a message indicating that the training\\n sequence has been initialized.\\n\\nExamples:\\n ```bash\\n curl -X POST http://localhost:8000/train\\n ```\\n\\n This will initiate the training sequence and return:\\n\\n ```json\\n {\\n\\\"message\\\": \\\"Training sequence initialized\\\"\\n }\\n ```\" , \"operationId\" : \"train_train_post\" , \"responses\" :{ \"200\" :{ \"description\" : \"Successful Response\" , \"content\" :{ \"application/json\" :{ \"schema\" :{} } } } }, \"security\" :[ { \"OAuth2PasswordBearer\" :[] } ] } }, \"/predict/\" :{ \"post\" :{ \"tags\" :[ \"predict\" ], \"summary\" : \"Predict\" , \"description\" : \"Initiatie prediction of room-specific ML models for all rooms in data source.\\nThe endpoint triggers a process to retrieve the unscored rooms, scores them\\nusing the designated room-specific machine learning model, and stores the\\nscored data back into the database.\\n\\nCalls the `prediction_flow` function as a async background task.\\n\\n**NOTE**: Authentication is required for this endpoint.\\n\\nArgs:\\n\\n request: The FastAPI request object. This argument is currently not used.\\n\\n session (Session, optional): SQLAlchemy session. Defaults to a new\\n session from `get_session`.\\n\\n model_registry (ModelRegistry, optional): ML model registry. Defaults to\\n the current model from `get_current_registry`.\\n\\nReturns:\\n\\n dict: A message indicating the completion status of the scoring sequence.\\n\\n\\nExamples:\\n ```bash\\n curl -X POST http://localhost:8000/predict/\\n ```\\n\\nOutput:\\n ```json\\n {\\n \\\"message\\\": \\\"Scoring sequence completed.\\\"\\n }\\n ```\" , \"operationId\" : \"predict_predict__post\" , \"responses\" :{ \"200\" :{ \"description\" : \"Successful Response\" , \"content\" :{ \"application/json\" :{ \"schema\" :{ \"additionalProperties\" :{ \"type\" : \"string\" }, \"type\" : \"object\" , \"title\" : \"Response Predict Predict Post\" } } } } }, \"security\" :[ { \"OAuth2PasswordBearer\" :[] } ] } }, \"/heartbeat/\" :{ \"get\" :{ \"tags\" :[ \"heartbeat\" ], \"summary\" : \"Heartbeat\" , \"description\" : \"Heartbeat Endpoint for Health Check and Versioning.\\n\\nThis route returns a JSON object containing the current version of the service.\\nThe version information is retrieved from an environment variable, `GIT_METADATA`,\\nwhich is set during a GitHub Actions job.\\n\\nArgs:\\n\\n request (Request): The FastAPI request object. This argument is ignored but\\n included for potential future use.\\n\\nReturns:\\n\\n dict: A dictionary containing the version information, with key \\\"version\\\" and\\n value as the short version of the git hash of the last commit.\\n\\n\\nExamples:\\n\\n ```bash\\n curl http://localhost:8000/heartbeat/\\n ```\\n\\nOutput:\\n\\n ```json\\n {\\n \\\"version\\\": \\\"abc123\\\" # The git hash short version\\n }\\n ```\\n\\nNote:\\n\\n The `GIT_METADATA` environment variable must be set, usually during a\\n GitHub Actions job, for this endpoint to return accurate version information.\\n If run locally, the version will be set to \\\"local\\\".\" , \"operationId\" : \"heartbeat_heartbeat__get\" , \"responses\" :{ \"200\" :{ \"description\" : \"Successful Response\" , \"content\" :{ \"application/json\" :{ \"schema\" :{ \"type\" : \"object\" , \"title\" : \"Response Heartbeat Heartbeat Get\" } } } } } } } }, \"components\" :{ \"schemas\" :{ \"BearerResponse\" :{ \"properties\" :{ \"access_token\" :{ \"type\" : \"string\" , \"title\" : \"Access Token\" }, \"token_type\" :{ \"type\" : \"string\" , \"title\" : \"Token Type\" } }, \"type\" : \"object\" , \"required\" :[ \"access_token\" , \"token_type\" ], \"title\" : \"BearerResponse\" }, \"Body_auth_jwt_login_auth_jwt_login_post\" :{ \"properties\" :{ \"grant_type\" :{ \"anyOf\" :[ { \"type\" : \"string\" , \"pattern\" : \"password\" }, { \"type\" : \"null\" } ], \"title\" : \"Grant Type\" }, \"username\" :{ \"type\" : \"string\" , \"title\" : \"Username\" }, \"password\" :{ \"type\" : \"string\" , \"title\" : \"Password\" }, \"scope\" :{ \"type\" : \"string\" , \"title\" : \"Scope\" , \"default\" : \"\" }, \"client_id\" :{ \"anyOf\" :[ { \"type\" : \"string\" }, { \"type\" : \"null\" } ], \"title\" : \"Client Id\" }, \"client_secret\" :{ \"anyOf\" :[ { \"type\" : \"string\" }, { \"type\" : \"null\" } ], \"title\" : \"Client Secret\" } }, \"type\" : \"object\" , \"required\" :[ \"username\" , \"password\" ], \"title\" : \"Body_auth_jwt_login_auth_jwt_login_post\" }, \"ErrorModel\" :{ \"properties\" :{ \"detail\" :{ \"anyOf\" :[ { \"type\" : \"string\" }, { \"additionalProperties\" :{ \"type\" : \"string\" }, \"type\" : \"object\" } ], \"title\" : \"Detail\" } }, \"type\" : \"object\" , \"required\" :[ \"detail\" ], \"title\" : \"ErrorModel\" }, \"HTTPValidationError\" :{ \"properties\" :{ \"detail\" :{ \"items\" :{ \"$ref\" : \"#/components/schemas/ValidationError\" }, \"type\" : \"array\" , \"title\" : \"Detail\" } }, \"type\" : \"object\" , \"title\" : \"HTTPValidationError\" }, \"UserCreate\" :{ \"properties\" :{ \"email\" :{ \"type\" : \"string\" , \"format\" : \"email\" , \"title\" : \"Email\" }, \"password\" :{ \"type\" : \"string\" , \"title\" : \"Password\" }, \"is_active\" :{ \"anyOf\" :[ { \"type\" : \"boolean\" }, { \"type\" : \"null\" } ], \"title\" : \"Is Active\" , \"default\" : true }, \"is_superuser\" :{ \"anyOf\" :[ { \"type\" : \"boolean\" }, { \"type\" : \"null\" } ], \"title\" : \"Is Superuser\" , \"default\" : false }, \"is_verified\" :{ \"anyOf\" :[ { \"type\" : \"boolean\" }, { \"type\" : \"null\" } ], \"title\" : \"Is Verified\" , \"default\" : false } }, \"type\" : \"object\" , \"required\" :[ \"email\" , \"password\" ], \"title\" : \"UserCreate\" , \"description\" : \"Schema for creating a new user. Extends the BaseUserCreate schema provided\\nby FastAPI Users.\\n\\nAttributes:\\n Inherits all attributes from schemas.BaseUserCreate.\\n\\nExample:\\n >>> create_user = UserCreate(email=\\\"example@example.com\\\",\\n >>> password=\\\"example_password\\\")\" }, \"UserRead\" :{ \"properties\" :{ \"id\" :{ \"title\" : \"Id\" }, \"email\" :{ \"type\" : \"string\" , \"format\" : \"email\" , \"title\" : \"Email\" }, \"is_active\" :{ \"type\" : \"boolean\" , \"title\" : \"Is Active\" , \"default\" : true }, \"is_superuser\" :{ \"type\" : \"boolean\" , \"title\" : \"Is Superuser\" , \"default\" : false }, \"is_verified\" :{ \"type\" : \"boolean\" , \"title\" : \"Is Verified\" , \"default\" : false } }, \"type\" : \"object\" , \"required\" :[ \"id\" , \"email\" ], \"title\" : \"UserRead\" , \"description\" : \"Schema for reading user details. Extends the BaseUser schema provided\\nby FastAPI Users.\\n\\nAttributes:\\n Inherits all attributes from schemas.BaseUser.\\n\\nExample:\\n >>> read_user = UserRead(email=\\\"example@example.com\\\",\\n >>> id=uuid.UUID(\\\"some-uuid\\\"))\" }, \"UserUpdate\" :{ \"properties\" :{ \"password\" :{ \"anyOf\" :[ { \"type\" : \"string\" }, { \"type\" : \"null\" } ], \"title\" : \"Password\" }, \"email\" :{ \"anyOf\" :[ { \"type\" : \"string\" , \"format\" : \"email\" }, { \"type\" : \"null\" } ], \"title\" : \"Email\" }, \"is_active\" :{ \"anyOf\" :[ { \"type\" : \"boolean\" }, { \"type\" : \"null\" } ], \"title\" : \"Is Active\" }, \"is_superuser\" :{ \"anyOf\" :[ { \"type\" : \"boolean\" }, { \"type\" : \"null\" } ], \"title\" : \"Is Superuser\" }, \"is_verified\" :{ \"anyOf\" :[ { \"type\" : \"boolean\" }, { \"type\" : \"null\" } ], \"title\" : \"Is Verified\" } }, \"type\" : \"object\" , \"required\" :[ \"password\" , \"email\" , \"is_active\" , \"is_superuser\" , \"is_verified\" ], \"title\" : \"UserUpdate\" , \"description\" : \"Schema for updating an existing user's details. Extends the\\nBaseUserUpdate schema provided by FastAPI Users.\\n\\nAttributes:\\n Inherits all attributes from schemas.BaseUserUpdate.\\n\\nExample:\\n >>> update_user = UserUpdate(email=\\\"new_example@example.com\\\")\" }, \"ValidationError\" :{ \"properties\" :{ \"loc\" :{ \"items\" :{ \"anyOf\" :[ { \"type\" : \"string\" }, { \"type\" : \"integer\" } ] }, \"type\" : \"array\" , \"title\" : \"Location\" }, \"msg\" :{ \"type\" : \"string\" , \"title\" : \"Message\" }, \"type\" :{ \"type\" : \"string\" , \"title\" : \"Error Type\" } }, \"type\" : \"object\" , \"required\" :[ \"loc\" , \"msg\" , \"type\" ], \"title\" : \"ValidationError\" } }, \"securitySchemes\" :{ \"OAuth2PasswordBearer\" :{ \"type\" : \"oauth2\" , \"flows\" :{ \"password\" :{ \"scopes\" :{}, \"tokenUrl\" : \"auth/jwt/login\" } } } } } }","title":"Deployment"},{"location":"ai/deployment/#deployment","text":"The AI solution is designed to be exposed as a RESTful FastAPI (V0.103.2) web application. The /ai folder contains a Dockerfile that can be used to build a Docker image of the application. The image can then be deployed to a cloud-based container service - In our case, Azure Container Registry. The application is designed to be deployed as a single container, but it can be scaled horizontally to multiple instances if needed. As evident from the figure above, the solution has multiple modules even though the name suggests that it is only a single AI model. The technical specifications for each these modules can be found in /references .","title":"Deployment"},{"location":"ai/deployment/#authentication","text":"In its current state, the solution uses the OAuth2 protocol for authentication. As such, the solution is closed to traffic from unauthorized users. A list of pre-defined users are configured in an environment variabel file and will be created in the in-server sqlite database on initial deployment (Please see overview for details on adding an .env file for local development, and the interactive API documentation for details on the authentication process at <app-url>/docs ).","title":"Authentication"},{"location":"ai/deployment/#github-actions","text":"The solution is set up with GitHub Actions for continuous integration and deployment. The workflow is triggered on push to the main branch. Please see the .github/workflows folder for details on the workflow.","title":"GitHub Actions"},{"location":"ai/deployment/#integration","text":"The solution integrates with Azure Data Factory data flow and the associated (Snowflake) database but is otherwise closed to traffic. The specifications for the connecting pipeline in Azure Data Factory is shown below. Note that the pipeline first authenticates using the pre-defined credentials associated with its user and then calls the train endpoint. The endpoint is responsible for training the AI model and updating the model registry. The model registry is then used by the predict endpoint to return the predicted utilization rate for a given room. Extracted 30.10.2023 { \"name\" : \"PL_DRIFTOPTIMERINGSMODEL\" , \"properties\" : { \"description\" : \"This pipelines is responsible for training the AI-model \\\"Driftoptimeringsmodel\\\" once a month. \" , \"activities\" : [ { \"name\" : \"Get token\" , \"type\" : \"WebActivity\" , \"dependsOn\" : [ { \"activity\" : \"Get Config\" , \"dependencyConditions\" : [ \"Succeeded\" ] } ], \"policy\" : { \"timeout\" : \"0.12:00:00\" , \"retry\" : 0 , \"retryIntervalInSeconds\" : 30 , \"secureOutput\" : false , \"secureInput\" : false }, \"userProperties\" : [], \"typeProperties\" : { \"url\" : \"https://app-govtech.azurewebsites.net/auth/jwt/login\" , \"method\" : \"POST\" , \"headers\" : { \"accept\" : \"application/json\" , \"Content-Type\" : \"application/x-www-form-urlencoded\" }, \"body\" : { \"value\" : \"@activity('Get Config').output.value\" , \"type\" : \"Expression\" } } }, { \"name\" : \"Train Model\" , \"type\" : \"Lookup\" , \"dependsOn\" : [ { \"activity\" : \"Get token\" , \"dependencyConditions\" : [ \"Succeeded\" ] }, { \"activity\" : \"UPDATE_DRIFTOPTIMERING_TRAINING\" , \"dependencyConditions\" : [ \"Succeeded\" ] } ], \"policy\" : { \"timeout\" : \"0.12:00:00\" , \"retry\" : 0 , \"retryIntervalInSeconds\" : 30 , \"secureOutput\" : false , \"secureInput\" : false }, \"userProperties\" : [], \"typeProperties\" : { \"source\" : { \"type\" : \"JsonSource\" , \"storeSettings\" : { \"type\" : \"HttpReadSettings\" , \"requestMethod\" : \"POST\" , \"additionalHeaders\" : { \"value\" : \"@{concat('Authorization: Bearer ' , activity('Get token').output.access_token)}\" , \"type\" : \"Expression\" }, \"requestTimeout\" : \"10:00:00\" }, \"formatSettings\" : { \"type\" : \"JsonReadSettings\" } }, \"dataset\" : { \"referenceName\" : \"DS_DRIFTOPTIMERINGSMODEL_TRAIN\" , \"type\" : \"DatasetReference\" } } }, { \"name\" : \"UPDATE_DRIFTOPTIMERING_TRAINING\" , \"type\" : \"Lookup\" , \"dependsOn\" : [], \"policy\" : { \"timeout\" : \"0.12:00:00\" , \"retry\" : 0 , \"retryIntervalInSeconds\" : 30 , \"secureOutput\" : false , \"secureInput\" : false }, \"userProperties\" : [], \"typeProperties\" : { \"source\" : { \"type\" : \"SnowflakeSource\" , \"query\" : \"CALL \\\"RAW\\\".UPDATEFEATURIZDISTINCT('GOVTECH_DB', 'RAW', '4_FEATURIZ_DRIFTOPTIMERING_TRAINING', '3_CLEANSED_DRIFTOPTIMERING_TRAINING')\" , \"exportSettings\" : { \"type\" : \"SnowflakeExportCopyCommand\" } }, \"dataset\" : { \"referenceName\" : \"DS_SNOWFLAKE\" , \"type\" : \"DatasetReference\" , \"parameters\" : { \"database\" : \"GOVTECH_DB\" , \"table\" : \"4_FEATURIZ_DRIFTOPTIMERING_TRAINING\" , \"schema\" : \"RAW\" } } } }, { \"name\" : \"Get Config\" , \"type\" : \"WebActivity\" , \"dependsOn\" : [], \"policy\" : { \"timeout\" : \"0.12:00:00\" , \"retry\" : 0 , \"retryIntervalInSeconds\" : 30 , \"secureOutput\" : false , \"secureInput\" : false }, \"userProperties\" : [], \"typeProperties\" : { \"url\" : \"<HIDDEN>\" , \"method\" : \"GET\" , \"authentication\" : { \"type\" : \"MSI\" , \"resource\" : \"https://vault.azure.net\" } } } ], \"folder\" : { \"name\" : \"Driftoptimeringsmodel\" }, \"annotations\" : [], \"lastPublishTime\" : \"2023-10-13T12:27:21Z\" }, \"type\" : \"Microsoft.DataFactory/factories/pipelines\" }","title":"Integration"},{"location":"ai/deployment/#remote-resource-specification","text":"Extracted 30.10.2023 { \"id\" : \"/subscriptions/<HIDDEN/resourceGroups/rg-govtech/providers/Microsoft.Web/sites/app-govtech\" , \"name\" : \"app-govtech\" , \"type\" : \"Microsoft.Web/sites\" , \"kind\" : \"app,linux,container\" , \"location\" : \"West Europe\" , \"tags\" : {}, \"properties\" : { \"name\" : \"app-govtech\" , \"state\" : \"Running\" , \"hostNames\" : [ \"app-govtech.azurewebsites.net\" ], \"webSpace\" : \"rg-govtech-WestEuropewebspace-Linux\" , \"selfLink\" : \"<HIDDEN>\" , \"repositorySiteName\" : \"app-govtech\" , \"owner\" : null , \"usageState\" : 0 , \"enabled\" : true , \"adminEnabled\" : true , \"afdEnabled\" : false , \"enabledHostNames\" : [ \"app-govtech.azurewebsites.net\" , \"app-govtech.scm.azurewebsites.net\" ], \"siteProperties\" : { \"metadata\" : null , \"properties\" : [ { \"name\" : \"LinuxFxVersion\" , \"value\" : \"DOCKER|acrgovtech.azurecr.io/acrgovtech/production:<HIDDEN>\" }, { \"name\" : \"WindowsFxVersion\" , \"value\" : null } ], \"appSettings\" : null }, \"availabilityState\" : 0 , \"sslCertificates\" : null , \"csrs\" : [], \"cers\" : null , \"siteMode\" : null , \"hostNameSslStates\" : [ { \"name\" : \"app-govtech.azurewebsites.net\" , \"sslState\" : 0 , \"ipBasedSslResult\" : null , \"virtualIP\" : null , \"virtualIPv6\" : null , \"thumbprint\" : null , \"certificateResourceId\" : null , \"toUpdate\" : null , \"toUpdateIpBasedSsl\" : null , \"ipBasedSslState\" : 0 , \"hostType\" : 0 }, { \"name\" : \"app-govtech.scm.azurewebsites.net\" , \"sslState\" : 0 , \"ipBasedSslResult\" : null , \"virtualIP\" : null , \"virtualIPv6\" : null , \"thumbprint\" : null , \"certificateResourceId\" : null , \"toUpdate\" : null , \"toUpdateIpBasedSsl\" : null , \"ipBasedSslState\" : 0 , \"hostType\" : 1 } ], \"computeMode\" : null , \"serverFarm\" : null , \"serverFarmId\" : \"/subscriptions/<HIDDEN>/resourceGroups/rg-govtech/providers/Microsoft.Web/serverfarms/ASP-govtech\" , \"reserved\" : true , \"isXenon\" : false , \"hyperV\" : false , \"lastModifiedTimeUtc\" : \"2023-10-30T14:43:21.26\" , \"storageRecoveryDefaultState\" : \"Running\" , \"contentAvailabilityState\" : 0 , \"runtimeAvailabilityState\" : 0 , \"dnsConfiguration\" : {}, \"vnetRouteAllEnabled\" : false , \"containerAllocationSubnet\" : null , \"useContainerLocalhostBindings\" : null , \"vnetImagePullEnabled\" : false , \"vnetContentShareEnabled\" : false , \"siteConfig\" : { \"numberOfWorkers\" : 1 , \"defaultDocuments\" : null , \"netFrameworkVersion\" : null , \"phpVersion\" : null , \"pythonVersion\" : null , \"nodeVersion\" : null , \"powerShellVersion\" : null , \"linuxFxVersion\" : \"DOCKER|acrgovtech.azurecr.io/acrgovtech/production:<HIDDEN>\" , \"windowsFxVersion\" : null , \"windowsConfiguredStacks\" : null , \"requestTracingEnabled\" : null , \"remoteDebuggingEnabled\" : null , \"remoteDebuggingVersion\" : null , \"httpLoggingEnabled\" : null , \"azureMonitorLogCategories\" : null , \"acrUseManagedIdentityCreds\" : false , \"acrUserManagedIdentityID\" : null , \"logsDirectorySizeLimit\" : null , \"detailedErrorLoggingEnabled\" : null , \"publishingUsername\" : null , \"publishingPassword\" : null , \"appSettings\" : null , \"metadata\" : null , \"connectionStrings\" : null , \"machineKey\" : null , \"handlerMappings\" : null , \"documentRoot\" : null , \"scmType\" : null , \"use32BitWorkerProcess\" : null , \"webSocketsEnabled\" : null , \"alwaysOn\" : true , \"javaVersion\" : null , \"javaContainer\" : null , \"javaContainerVersion\" : null , \"appCommandLine\" : null , \"managedPipelineMode\" : null , \"virtualApplications\" : null , \"winAuthAdminState\" : null , \"winAuthTenantState\" : null , \"customAppPoolIdentityAdminState\" : null , \"customAppPoolIdentityTenantState\" : null , \"runtimeADUser\" : null , \"runtimeADUserPassword\" : null , \"loadBalancing\" : null , \"routingRules\" : null , \"experiments\" : null , \"limits\" : null , \"autoHealEnabled\" : null , \"autoHealRules\" : null , \"tracingOptions\" : null , \"vnetName\" : null , \"vnetRouteAllEnabled\" : null , \"vnetPrivatePortsCount\" : null , \"publicNetworkAccess\" : null , \"cors\" : null , \"push\" : null , \"apiDefinition\" : null , \"apiManagementConfig\" : null , \"autoSwapSlotName\" : null , \"localMySqlEnabled\" : null , \"managedServiceIdentityId\" : null , \"xManagedServiceIdentityId\" : null , \"keyVaultReferenceIdentity\" : null , \"ipSecurityRestrictions\" : null , \"ipSecurityRestrictionsDefaultAction\" : null , \"scmIpSecurityRestrictions\" : null , \"scmIpSecurityRestrictionsDefaultAction\" : null , \"scmIpSecurityRestrictionsUseMain\" : null , \"http20Enabled\" : false , \"minTlsVersion\" : null , \"minTlsCipherSuite\" : null , \"supportedTlsCipherSuites\" : null , \"scmMinTlsVersion\" : null , \"ftpsState\" : null , \"preWarmedInstanceCount\" : null , \"functionAppScaleLimit\" : 0 , \"elasticWebAppScaleLimit\" : null , \"healthCheckPath\" : null , \"fileChangeAuditEnabled\" : null , \"functionsRuntimeScaleMonitoringEnabled\" : null , \"websiteTimeZone\" : null , \"minimumElasticInstanceCount\" : 0 , \"azureStorageAccounts\" : null , \"http20ProxyFlag\" : null , \"sitePort\" : null , \"antivirusScanEnabled\" : null , \"storageType\" : null , \"sitePrivateLinkHostEnabled\" : null }, \"daprConfig\" : null , \"deploymentId\" : \"app-govtech\" , \"slotName\" : null , \"trafficManagerHostNames\" : null , \"sku\" : \"PremiumV3\" , \"scmSiteAlsoStopped\" : false , \"targetSwapSlot\" : null , \"hostingEnvironment\" : null , \"hostingEnvironmentProfile\" : null , \"clientAffinityEnabled\" : false , \"clientCertEnabled\" : false , \"clientCertMode\" : 0 , \"clientCertExclusionPaths\" : null , \"hostNamesDisabled\" : false , \"ipMode\" : \"IPv4\" , \"vnetBackupRestoreEnabled\" : false , \"domainVerificationIdentifiers\" : null , \"customDomainVerificationId\" : \"<HIDDEN>\" , \"kind\" : \"app,linux,container\" , \"managedEnvironmentId\" : null , \"inboundIpAddress\" : \"<HIDDEN>\" , \"possibleInboundIpAddresses\" : \"<HIDDEN>\" , \"ftpUsername\" : \"app-govtech\\\\$app-govtech\" , \"ftpsHostName\" : \"<HIDDEN>\" , \"outboundIpAddresses\" : \"<HIDDEN>\" , \"possibleOutboundIpAddresses\" : \"<HIDDEN>\" , \"containerSize\" : 0 , \"dailyMemoryTimeQuota\" : 0 , \"suspendedTill\" : null , \"siteDisabledReason\" : 0 , \"functionExecutionUnitsCache\" : null , \"maxNumberOfWorkers\" : null , \"homeStamp\" : \"waws-prod-am2-649\" , \"cloningInfo\" : null , \"hostingEnvironmentId\" : null , \"tags\" : {}, \"resourceGroup\" : \"rg-govtech\" , \"defaultHostName\" : \"app-govtech.azurewebsites.net\" , \"slotSwapStatus\" : null , \"httpsOnly\" : true , \"endToEndEncryptionEnabled\" : false , \"functionsRuntimeAdminIsolationEnabled\" : false , \"redundancyMode\" : 0 , \"inProgressOperationId\" : null , \"geoDistributions\" : null , \"privateEndpointConnections\" : [], \"publicNetworkAccess\" : \"Enabled\" , \"buildVersion\" : null , \"targetBuildVersion\" : null , \"migrationState\" : null , \"eligibleLogCategories\" : \"AppServiceAppLogs,AppServiceAuditLogs,AppServiceConsoleLogs,AppServiceHTTPLogs,AppServiceIPSecAuditLogs,AppServicePlatformLogs,ScanLogs,AppServiceFileAuditLogs,AppServiceAntivirusScanAuditLogs\" , \"inFlightFeatures\" : [], \"storageAccountRequired\" : false , \"virtualNetworkSubnetId\" : null , \"keyVaultReferenceIdentity\" : \"SystemAssigned\" , \"defaultHostNameScope\" : 0 , \"privateLinkIdentifiers\" : null , \"sshEnabled\" : null } }","title":"Remote Resource Specification"},{"location":"ai/deployment/#api-documentation","text":"The API documentation is generated by FastAPI and can be accessed at <app-url>/docs . The documentation is interactive and allows for testing of the API endpoints. The documentation is also available in JSON format at <app-url>/openapi.json . For transparency, the JSON specification is also given below { \"openapi\" : \"3.1.0\" , \"info\" :{ \"title\" : \"Tilly API\" , \"description\" : \"Unsupervised anomaly detection for room usage\" , \"version\" : \"54c0cc62-bekkeremil-Oct 30, 15:41\" }, \"paths\" :{ \"/\" :{ \"get\" :{ \"tags\" :[ \"dashboard\" ], \"summary\" : \"Read Root\" , \"description\" : \"Serve the Tilly Dashboard.\\n\\nThis route returns an HTML response that serves the Tilly dashboard.\\n\\nArgs:\\n request (Request): The FastAPI request object.\\n\\nReturns:\\n HTMLResponse: The HTML response containing the rendered Tilly dashboard.\\n\\nExamples:\\n ```bash\\n curl http://localhost:8000/\\n ```\\n\\n This will return the HTML content of the Tilly dashboard.\" , \"operationId\" : \"read_root__get\" , \"responses\" :{ \"200\" :{ \"description\" : \"Successful Response\" , \"content\" :{ \"text/html\" :{ \"schema\" :{ \"type\" : \"string\" } } } } } } }, \"/plots_structure\" :{ \"get\" :{ \"tags\" :[ \"dashboard\" ], \"summary\" : \"Get Plots Structure\" , \"description\" : \"Get Plot Directory Structure.\\n\\nThis route returns the directory structure of the plots as a JSON object.\\nIf the directory structure is invalid, a 404 HTTP error is raised.\\n\\nReturns:\\n Optional[Dict]: The dictionary representing the directory structure.\\n\\nExamples:\\n ```bash\\n curl http://localhost:8000/plots_structure\\n ```\\n\\n This will return a JSON object representing the directory structure of\\n the plots.\" , \"operationId\" : \"get_plots_structure_plots_structure_get\" , \"responses\" :{ \"200\" :{ \"description\" : \"Successful Response\" , \"content\" :{ \"application/json\" :{ \"schema\" :{ \"anyOf\" :[ { \"type\" : \"object\" }, { \"type\" : \"null\" } ], \"title\" : \"Response Get Plots Structure Plots Structure Get\" } } } } } } }, \"/auth/jwt/login\" :{ \"post\" :{ \"tags\" :[ \"auth\" ], \"summary\" : \"Auth:Jwt.Login\" , \"operationId\" : \"auth_jwt_login_auth_jwt_login_post\" , \"requestBody\" :{ \"content\" :{ \"application/x-www-form-urlencoded\" :{ \"schema\" :{ \"$ref\" : \"#/components/schemas/Body_auth_jwt_login_auth_jwt_login_post\" } } }, \"required\" : true }, \"responses\" :{ \"200\" :{ \"description\" : \"Successful Response\" , \"content\" :{ \"application/json\" :{ \"schema\" :{ \"$ref\" : \"#/components/schemas/BearerResponse\" }, \"example\" :{ \"access_token\" : \"eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJ1c2VyX2lkIjoiOTIyMWZmYzktNjQwZi00MzcyLTg2ZDMtY2U2NDJjYmE1NjAzIiwiYXVkIjoiZmFzdGFwaS11c2VyczphdXRoIiwiZXhwIjoxNTcxNTA0MTkzfQ.M10bjOe45I5Ncu_uXvOmVV8QxnL-nZfcH96U90JaocI\" , \"token_type\" : \"bearer\" } } } }, \"400\" :{ \"description\" : \"Bad Request\" , \"content\" :{ \"application/json\" :{ \"schema\" :{ \"$ref\" : \"#/components/schemas/ErrorModel\" }, \"examples\" :{ \"LOGIN_BAD_CREDENTIALS\" :{ \"summary\" : \"Bad credentials or the user is inactive.\" , \"value\" :{ \"detail\" : \"LOGIN_BAD_CREDENTIALS\" } }, \"LOGIN_USER_NOT_VERIFIED\" :{ \"summary\" : \"The user is not verified.\" , \"value\" :{ \"detail\" : \"LOGIN_USER_NOT_VERIFIED\" } } } } } }, \"422\" :{ \"description\" : \"Validation Error\" , \"content\" :{ \"application/json\" :{ \"schema\" :{ \"$ref\" : \"#/components/schemas/HTTPValidationError\" } } } } } } }, \"/auth/jwt/logout\" :{ \"post\" :{ \"tags\" :[ \"auth\" ], \"summary\" : \"Auth:Jwt.Logout\" , \"operationId\" : \"auth_jwt_logout_auth_jwt_logout_post\" , \"responses\" :{ \"200\" :{ \"description\" : \"Successful Response\" , \"content\" :{ \"application/json\" :{ \"schema\" :{ } } } }, \"401\" :{ \"description\" : \"Missing token or inactive user.\" } }, \"security\" :[ { \"OAuth2PasswordBearer\" :[] } ] } }, \"/auth/register\" :{ \"post\" :{ \"tags\" :[ \"auth\" ], \"summary\" : \"Register:Register\" , \"operationId\" : \"register_register_auth_register_post\" , \"requestBody\" :{ \"content\" :{ \"application/json\" :{ \"schema\" :{ \"$ref\" : \"#/components/schemas/UserCreate\" } } }, \"required\" : true }, \"responses\" :{ \"201\" :{ \"description\" : \"Successful Response\" , \"content\" :{ \"application/json\" :{ \"schema\" :{ \"$ref\" : \"#/components/schemas/UserRead\" } } } }, \"400\" :{ \"description\" : \"Bad Request\" , \"content\" :{ \"application/json\" :{ \"schema\" :{ \"$ref\" : \"#/components/schemas/ErrorModel\" }, \"examples\" :{ \"REGISTER_USER_ALREADY_EXISTS\" :{ \"summary\" : \"A user with this email already exists.\" , \"value\" :{ \"detail\" : \"REGISTER_USER_ALREADY_EXISTS\" } }, \"REGISTER_INVALID_PASSWORD\" :{ \"summary\" : \"Password validation failed.\" , \"value\" :{ \"detail\" :{ \"code\" : \"REGISTER_INVALID_PASSWORD\" , \"reason\" : \"Password should beat least 3 characters\" } } } } } } }, \"422\" :{ \"description\" : \"Validation Error\" , \"content\" :{ \"application/json\" :{ \"schema\" :{ \"$ref\" : \"#/components/schemas/HTTPValidationError\" } } } } } } }, \"/users/me\" :{ \"get\" :{ \"tags\" :[ \"users\" ], \"summary\" : \"Users:Current User\" , \"operationId\" : \"users_current_user_users_me_get\" , \"responses\" :{ \"200\" :{ \"description\" : \"Successful Response\" , \"content\" :{ \"application/json\" :{ \"schema\" :{ \"$ref\" : \"#/components/schemas/UserRead\" } } } }, \"401\" :{ \"description\" : \"Missing token or inactive user.\" } }, \"security\" :[ { \"OAuth2PasswordBearer\" :[] } ] }, \"patch\" :{ \"tags\" :[ \"users\" ], \"summary\" : \"Users:Patch Current User\" , \"operationId\" : \"users_patch_current_user_users_me_patch\" , \"requestBody\" :{ \"content\" :{ \"application/json\" :{ \"schema\" :{ \"$ref\" : \"#/components/schemas/UserUpdate\" } } }, \"required\" : true }, \"responses\" :{ \"200\" :{ \"description\" : \"Successful Response\" , \"content\" :{ \"application/json\" :{ \"schema\" :{ \"$ref\" : \"#/components/schemas/UserRead\" } } } }, \"401\" :{ \"description\" : \"Missing token or inactive user.\" }, \"400\" :{ \"description\" : \"Bad Request\" , \"content\" :{ \"application/json\" :{ \"schema\" :{ \"$ref\" : \"#/components/schemas/ErrorModel\" }, \"examples\" :{ \"UPDATE_USER_EMAIL_ALREADY_EXISTS\" :{ \"summary\" : \"A user with this email already exists.\" , \"value\" :{ \"detail\" : \"UPDATE_USER_EMAIL_ALREADY_EXISTS\" } }, \"UPDATE_USER_INVALID_PASSWORD\" :{ \"summary\" : \"Password validation failed.\" , \"value\" :{ \"detail\" :{ \"code\" : \"UPDATE_USER_INVALID_PASSWORD\" , \"reason\" : \"Password should beat least 3 characters\" } } } } } } }, \"422\" :{ \"description\" : \"Validation Error\" , \"content\" :{ \"application/json\" :{ \"schema\" :{ \"$ref\" : \"#/components/schemas/HTTPValidationError\" } } } } }, \"security\" :[ { \"OAuth2PasswordBearer\" :[] } ] } }, \"/users/{id}\" :{ \"get\" :{ \"tags\" :[ \"users\" ], \"summary\" : \"Users:User\" , \"operationId\" : \"users_user_users__id__get\" , \"security\" :[ { \"OAuth2PasswordBearer\" :[] } ], \"parameters\" :[ { \"name\" : \"id\" , \"in\" : \"path\" , \"required\" : true , \"schema\" :{ \"type\" : \"string\" , \"title\" : \"Id\" } } ], \"responses\" :{ \"200\" :{ \"description\" : \"Successful Response\" , \"content\" :{ \"application/json\" :{ \"schema\" :{ \"$ref\" : \"#/components/schemas/UserRead\" } } } }, \"401\" :{ \"description\" : \"Missing token or inactive user.\" }, \"403\" :{ \"description\" : \"Not a superuser.\" }, \"404\" :{ \"description\" : \"The user does not exist.\" }, \"422\" :{ \"description\" : \"Validation Error\" , \"content\" :{ \"application/json\" :{ \"schema\" :{ \"$ref\" : \"#/components/schemas/HTTPValidationError\" } } } } } }, \"patch\" :{ \"tags\" :[ \"users\" ], \"summary\" : \"Users:Patch User\" , \"operationId\" : \"users_patch_user_users__id__patch\" , \"security\" :[ { \"OAuth2PasswordBearer\" :[] } ], \"parameters\" :[ { \"name\" : \"id\" , \"in\" : \"path\" , \"required\" : true , \"schema\" :{ \"type\" : \"string\" , \"title\" : \"Id\" } } ], \"requestBody\" :{ \"required\" : true , \"content\" :{ \"application/json\" :{ \"schema\" :{ \"$ref\" : \"#/components/schemas/UserUpdate\" } } } }, \"responses\" :{ \"200\" :{ \"description\" : \"Successful Response\" , \"content\" :{ \"application/json\" :{ \"schema\" :{ \"$ref\" : \"#/components/schemas/UserRead\" } } } }, \"401\" :{ \"description\" : \"Missing token or inactive user.\" }, \"403\" :{ \"description\" : \"Not a superuser.\" }, \"404\" :{ \"description\" : \"The user does not exist.\" }, \"400\" :{ \"content\" :{ \"application/json\" :{ \"examples\" :{ \"UPDATE_USER_EMAIL_ALREADY_EXISTS\" :{ \"summary\" : \"A user with this email already exists.\" , \"value\" :{ \"detail\" : \"UPDATE_USER_EMAIL_ALREADY_EXISTS\" } }, \"UPDATE_USER_INVALID_PASSWORD\" :{ \"summary\" : \"Password validation failed.\" , \"value\" :{ \"detail\" :{ \"code\" : \"UPDATE_USER_INVALID_PASSWORD\" , \"reason\" : \"Password should beat least 3 characters\" } } } }, \"schema\" :{ \"$ref\" : \"#/components/schemas/ErrorModel\" } } }, \"description\" : \"Bad Request\" }, \"422\" :{ \"description\" : \"Validation Error\" , \"content\" :{ \"application/json\" :{ \"schema\" :{ \"$ref\" : \"#/components/schemas/HTTPValidationError\" } } } } } }, \"delete\" :{ \"tags\" :[ \"users\" ], \"summary\" : \"Users:Delete User\" , \"operationId\" : \"users_delete_user_users__id__delete\" , \"security\" :[ { \"OAuth2PasswordBearer\" :[] } ], \"parameters\" :[ { \"name\" : \"id\" , \"in\" : \"path\" , \"required\" : true , \"schema\" :{ \"type\" : \"string\" , \"title\" : \"Id\" } } ], \"responses\" :{ \"204\" :{ \"description\" : \"Successful Response\" }, \"401\" :{ \"description\" : \"Missing token or inactive user.\" }, \"403\" :{ \"description\" : \"Not a superuser.\" }, \"404\" :{ \"description\" : \"The user does not exist.\" }, \"422\" :{ \"description\" : \"Validation Error\" , \"content\" :{ \"application/json\" :{ \"schema\" :{ \"$ref\" : \"#/components/schemas/HTTPValidationError\" } } } } } } }, \"/train\" :{ \"post\" :{ \"tags\" :[ \"train\" ], \"summary\" : \"Train\" , \"description\" : \"Initiate training of room-specific ML models for all rooms in data source.\\n\\nCalls the `training_flow` function as a async background task.\\n\\n**NOTE**: Authentication is required for this endpoint.\\n\\nArgs:\\n\\n request: Request: FastAPI Request object. Not used, but kept for FastAPI\\n dependency injection.\\n\\n background_tasks: FastAPI BackgroundTasks for running functions in the\\n background.\\n\\n session: SQLAlchemy Session object for database interactions\\n (injected via FastAPI's dependency system).\\n\\nReturns:\\n\\n dict: A dictionary containing a message indicating that the training\\n sequence has been initialized.\\n\\nExamples:\\n ```bash\\n curl -X POST http://localhost:8000/train\\n ```\\n\\n This will initiate the training sequence and return:\\n\\n ```json\\n {\\n\\\"message\\\": \\\"Training sequence initialized\\\"\\n }\\n ```\" , \"operationId\" : \"train_train_post\" , \"responses\" :{ \"200\" :{ \"description\" : \"Successful Response\" , \"content\" :{ \"application/json\" :{ \"schema\" :{} } } } }, \"security\" :[ { \"OAuth2PasswordBearer\" :[] } ] } }, \"/predict/\" :{ \"post\" :{ \"tags\" :[ \"predict\" ], \"summary\" : \"Predict\" , \"description\" : \"Initiatie prediction of room-specific ML models for all rooms in data source.\\nThe endpoint triggers a process to retrieve the unscored rooms, scores them\\nusing the designated room-specific machine learning model, and stores the\\nscored data back into the database.\\n\\nCalls the `prediction_flow` function as a async background task.\\n\\n**NOTE**: Authentication is required for this endpoint.\\n\\nArgs:\\n\\n request: The FastAPI request object. This argument is currently not used.\\n\\n session (Session, optional): SQLAlchemy session. Defaults to a new\\n session from `get_session`.\\n\\n model_registry (ModelRegistry, optional): ML model registry. Defaults to\\n the current model from `get_current_registry`.\\n\\nReturns:\\n\\n dict: A message indicating the completion status of the scoring sequence.\\n\\n\\nExamples:\\n ```bash\\n curl -X POST http://localhost:8000/predict/\\n ```\\n\\nOutput:\\n ```json\\n {\\n \\\"message\\\": \\\"Scoring sequence completed.\\\"\\n }\\n ```\" , \"operationId\" : \"predict_predict__post\" , \"responses\" :{ \"200\" :{ \"description\" : \"Successful Response\" , \"content\" :{ \"application/json\" :{ \"schema\" :{ \"additionalProperties\" :{ \"type\" : \"string\" }, \"type\" : \"object\" , \"title\" : \"Response Predict Predict Post\" } } } } }, \"security\" :[ { \"OAuth2PasswordBearer\" :[] } ] } }, \"/heartbeat/\" :{ \"get\" :{ \"tags\" :[ \"heartbeat\" ], \"summary\" : \"Heartbeat\" , \"description\" : \"Heartbeat Endpoint for Health Check and Versioning.\\n\\nThis route returns a JSON object containing the current version of the service.\\nThe version information is retrieved from an environment variable, `GIT_METADATA`,\\nwhich is set during a GitHub Actions job.\\n\\nArgs:\\n\\n request (Request): The FastAPI request object. This argument is ignored but\\n included for potential future use.\\n\\nReturns:\\n\\n dict: A dictionary containing the version information, with key \\\"version\\\" and\\n value as the short version of the git hash of the last commit.\\n\\n\\nExamples:\\n\\n ```bash\\n curl http://localhost:8000/heartbeat/\\n ```\\n\\nOutput:\\n\\n ```json\\n {\\n \\\"version\\\": \\\"abc123\\\" # The git hash short version\\n }\\n ```\\n\\nNote:\\n\\n The `GIT_METADATA` environment variable must be set, usually during a\\n GitHub Actions job, for this endpoint to return accurate version information.\\n If run locally, the version will be set to \\\"local\\\".\" , \"operationId\" : \"heartbeat_heartbeat__get\" , \"responses\" :{ \"200\" :{ \"description\" : \"Successful Response\" , \"content\" :{ \"application/json\" :{ \"schema\" :{ \"type\" : \"object\" , \"title\" : \"Response Heartbeat Heartbeat Get\" } } } } } } } }, \"components\" :{ \"schemas\" :{ \"BearerResponse\" :{ \"properties\" :{ \"access_token\" :{ \"type\" : \"string\" , \"title\" : \"Access Token\" }, \"token_type\" :{ \"type\" : \"string\" , \"title\" : \"Token Type\" } }, \"type\" : \"object\" , \"required\" :[ \"access_token\" , \"token_type\" ], \"title\" : \"BearerResponse\" }, \"Body_auth_jwt_login_auth_jwt_login_post\" :{ \"properties\" :{ \"grant_type\" :{ \"anyOf\" :[ { \"type\" : \"string\" , \"pattern\" : \"password\" }, { \"type\" : \"null\" } ], \"title\" : \"Grant Type\" }, \"username\" :{ \"type\" : \"string\" , \"title\" : \"Username\" }, \"password\" :{ \"type\" : \"string\" , \"title\" : \"Password\" }, \"scope\" :{ \"type\" : \"string\" , \"title\" : \"Scope\" , \"default\" : \"\" }, \"client_id\" :{ \"anyOf\" :[ { \"type\" : \"string\" }, { \"type\" : \"null\" } ], \"title\" : \"Client Id\" }, \"client_secret\" :{ \"anyOf\" :[ { \"type\" : \"string\" }, { \"type\" : \"null\" } ], \"title\" : \"Client Secret\" } }, \"type\" : \"object\" , \"required\" :[ \"username\" , \"password\" ], \"title\" : \"Body_auth_jwt_login_auth_jwt_login_post\" }, \"ErrorModel\" :{ \"properties\" :{ \"detail\" :{ \"anyOf\" :[ { \"type\" : \"string\" }, { \"additionalProperties\" :{ \"type\" : \"string\" }, \"type\" : \"object\" } ], \"title\" : \"Detail\" } }, \"type\" : \"object\" , \"required\" :[ \"detail\" ], \"title\" : \"ErrorModel\" }, \"HTTPValidationError\" :{ \"properties\" :{ \"detail\" :{ \"items\" :{ \"$ref\" : \"#/components/schemas/ValidationError\" }, \"type\" : \"array\" , \"title\" : \"Detail\" } }, \"type\" : \"object\" , \"title\" : \"HTTPValidationError\" }, \"UserCreate\" :{ \"properties\" :{ \"email\" :{ \"type\" : \"string\" , \"format\" : \"email\" , \"title\" : \"Email\" }, \"password\" :{ \"type\" : \"string\" , \"title\" : \"Password\" }, \"is_active\" :{ \"anyOf\" :[ { \"type\" : \"boolean\" }, { \"type\" : \"null\" } ], \"title\" : \"Is Active\" , \"default\" : true }, \"is_superuser\" :{ \"anyOf\" :[ { \"type\" : \"boolean\" }, { \"type\" : \"null\" } ], \"title\" : \"Is Superuser\" , \"default\" : false }, \"is_verified\" :{ \"anyOf\" :[ { \"type\" : \"boolean\" }, { \"type\" : \"null\" } ], \"title\" : \"Is Verified\" , \"default\" : false } }, \"type\" : \"object\" , \"required\" :[ \"email\" , \"password\" ], \"title\" : \"UserCreate\" , \"description\" : \"Schema for creating a new user. Extends the BaseUserCreate schema provided\\nby FastAPI Users.\\n\\nAttributes:\\n Inherits all attributes from schemas.BaseUserCreate.\\n\\nExample:\\n >>> create_user = UserCreate(email=\\\"example@example.com\\\",\\n >>> password=\\\"example_password\\\")\" }, \"UserRead\" :{ \"properties\" :{ \"id\" :{ \"title\" : \"Id\" }, \"email\" :{ \"type\" : \"string\" , \"format\" : \"email\" , \"title\" : \"Email\" }, \"is_active\" :{ \"type\" : \"boolean\" , \"title\" : \"Is Active\" , \"default\" : true }, \"is_superuser\" :{ \"type\" : \"boolean\" , \"title\" : \"Is Superuser\" , \"default\" : false }, \"is_verified\" :{ \"type\" : \"boolean\" , \"title\" : \"Is Verified\" , \"default\" : false } }, \"type\" : \"object\" , \"required\" :[ \"id\" , \"email\" ], \"title\" : \"UserRead\" , \"description\" : \"Schema for reading user details. Extends the BaseUser schema provided\\nby FastAPI Users.\\n\\nAttributes:\\n Inherits all attributes from schemas.BaseUser.\\n\\nExample:\\n >>> read_user = UserRead(email=\\\"example@example.com\\\",\\n >>> id=uuid.UUID(\\\"some-uuid\\\"))\" }, \"UserUpdate\" :{ \"properties\" :{ \"password\" :{ \"anyOf\" :[ { \"type\" : \"string\" }, { \"type\" : \"null\" } ], \"title\" : \"Password\" }, \"email\" :{ \"anyOf\" :[ { \"type\" : \"string\" , \"format\" : \"email\" }, { \"type\" : \"null\" } ], \"title\" : \"Email\" }, \"is_active\" :{ \"anyOf\" :[ { \"type\" : \"boolean\" }, { \"type\" : \"null\" } ], \"title\" : \"Is Active\" }, \"is_superuser\" :{ \"anyOf\" :[ { \"type\" : \"boolean\" }, { \"type\" : \"null\" } ], \"title\" : \"Is Superuser\" }, \"is_verified\" :{ \"anyOf\" :[ { \"type\" : \"boolean\" }, { \"type\" : \"null\" } ], \"title\" : \"Is Verified\" } }, \"type\" : \"object\" , \"required\" :[ \"password\" , \"email\" , \"is_active\" , \"is_superuser\" , \"is_verified\" ], \"title\" : \"UserUpdate\" , \"description\" : \"Schema for updating an existing user's details. Extends the\\nBaseUserUpdate schema provided by FastAPI Users.\\n\\nAttributes:\\n Inherits all attributes from schemas.BaseUserUpdate.\\n\\nExample:\\n >>> update_user = UserUpdate(email=\\\"new_example@example.com\\\")\" }, \"ValidationError\" :{ \"properties\" :{ \"loc\" :{ \"items\" :{ \"anyOf\" :[ { \"type\" : \"string\" }, { \"type\" : \"integer\" } ] }, \"type\" : \"array\" , \"title\" : \"Location\" }, \"msg\" :{ \"type\" : \"string\" , \"title\" : \"Message\" }, \"type\" :{ \"type\" : \"string\" , \"title\" : \"Error Type\" } }, \"type\" : \"object\" , \"required\" :[ \"loc\" , \"msg\" , \"type\" ], \"title\" : \"ValidationError\" } }, \"securitySchemes\" :{ \"OAuth2PasswordBearer\" :{ \"type\" : \"oauth2\" , \"flows\" :{ \"password\" :{ \"scopes\" :{}, \"tokenUrl\" : \"auth/jwt/login\" } } } } } }","title":"API Documentation"},{"location":"ai/overview/","text":"AI Room Utilization Model The AI Room Utilization Model (Tilly) (Formally, Driftsoptimeringsmodellen in Danish) is a machine learning anomaly detection solution that uses sensor data to detect whether a given room has been in use (i.e. have had human activity) in a given time 15-minute time interval. The following sections provide an overview of the solution, including its architecture, components, and development environment. Please see algorithm for a detailed description of the AI model and its implementation. deployment for a detailed description of the deployment process and the associated infrastructure. evaluation for a detailed description of the evaluation process and the associated dashboard. reference for code references and other resources. Background The GTM Smart 2 AI signature project revolves around using AI and advanced analytics to extract actionable insights on energy use for schools across municipalities. Early on, the project group identified a bottleneck in the practical derivation of these insights: The available data did not provide a reliable measurement of whether a given room had been in use at a given time. To address this, the project group NTT DATA developed Tilly, a room utilization model, which is the subject of this documentation. As evident from the figure above, the solution has multiple components even though the name suggests it's solely an AI model. These components are covered in the following section. Tilly integrates with Enformanten's data flow and the associated (Snowflake) database but is otherwise closed to traffic. Data Foundation From Enformanten's general data flow, Tilly receives a series of variables for each time unit (i.e., 15-minute intervals) for each room. The variable definitions are shown below. Variable Name Description Data Type ID Local ID Integer KOMMUNE Associated Municipality String SKOLE Associated School String DATE Date for the time interval String TIME Time for the time interval String DAYNAME e.g., Monday, Tuesday String TIDSPUNKT_TYPE e.g., School time or free time String SKEMALAGT Whether the time interval appears in the school schedule Boolean TYPE Type of day, e.g., Public holiday, special day, school vacation String NAVN Specific calendar day, e.g., Christmas holiday, summer vacation or Ascension Day String CO2 CO2 level measured in the given time interval Float TEMP Temperature measured in the given time interval Float MOTION Number of movements detected in the given time interval Float IAQ Indoor Air Quality value measured in the given time interval Float BOOKET Whether a booking is registered in the time interval Boolean By end of the development, only CO2 is included in the actual training of the model among the sensor variables (CO2, temperature, movement, and IAQ). This is partly due to inconsistent data quality for the remaining variables and partly due to iterative experimentation, where the aggregate performance of AI models has been evaluated by training on different combinations of the given sensor variables. This is further elaborated in the technical documentation of the solution. However, the architecture is designed for combinability, meaning that future development will be able to incorporate the remaining variables without major changes. For the same reason, Enformant's data flow is set up to deliver all existing sensor data to the Operational Optimization Model. Components As mentioned earlier, the Operational Optimization Model contains multiple components. With the exception of the dashboard used for evaluation, these components are essential for the chosen architecture. Model Registry Because the utilization rate is desired to be detected for each room, across schools, a separate AI model is trained per room. This results in 100+ AI models that are collectively trained on over 9 million variables. This number of models makes it necessary to create a model registry to systematically access and apply each model in the correct context. Here, a non-relational in-memory data structure is used in which each room, for each school, is indexed under a unique ID, derived from the school and the room's name. Preprocessing To train the models on the received data, extensive preprocessing is required. Preprocessing can be divided into two categories: Cleaning and Augmentation. Each subprocess in these categories is detailed below, where 1 \u2013 5 are data cleaning and 6 \u2013 7 are data augmentation. 1. Heuristic Estimation of Utilization Rate To prepare the data for the AI models, the first step is to estimate the utilization rate for each room based on booked and planned time periods. The exact process for this is described in the technical documentation of the solution. This process provides prior knowledge for the AI models, which in advance adjust their expected level of utilization for a given room over time. Therefore, we use a heuristic method to create an informed basis for the model's predictions. Heuristic Estimation of Utilization Rate for Room Utilization In the business context of room scheduling and occupancy optimization, one of the most crucial preprocessing steps for training AI models is estimating the utilization rate of each room. This is the first step in a two-category preprocessing pipeline, which also includes Data Cleaning and Data Augmentation. Parameters and Heuristics Utilization Coefficient: A scaling factor used to fine-tune the estimated usage rate. Its default value is set at 2.1. Minimum Utilization Bound: A heuristic lower limit set to ensure that the estimated usage of a room doesn't fall below this value, typically set at 0.1. Maximum Utilization Bound: A heuristic upper limit set to ensure that the estimated usage of a room doesn't exceed this value, typically set at 0.4. Methodology The algorithm calculates the number of slots that are in use, which include times marked as either \"Booked\" or \"Scheduled\" according to the organizational schema. This count of used slots is then adjusted to ensure it doesn't fall below the predefined minimum utilization bound. The net estimated utilization Exception Cases If the algorithm encounters a situation where there are zero total slots, it returns a predefined fallback value, denoted as \"auto,\" to indicate that the utilization could not be computed. Business Implications This heuristic estimation serves as an informative prior for AI models aiming to predict room utilization. The model adjusts its expectations based on this heuristic, improving the overall accuracy and reliability of future predictions. This is especially valuable for optimizing room bookings and schedules, thereby leading to more efficient use of organizational resources. Local development","title":"AI Room Utilization Model"},{"location":"ai/overview/#ai-room-utilization-model","text":"The AI Room Utilization Model (Tilly) (Formally, Driftsoptimeringsmodellen in Danish) is a machine learning anomaly detection solution that uses sensor data to detect whether a given room has been in use (i.e. have had human activity) in a given time 15-minute time interval. The following sections provide an overview of the solution, including its architecture, components, and development environment. Please see algorithm for a detailed description of the AI model and its implementation. deployment for a detailed description of the deployment process and the associated infrastructure. evaluation for a detailed description of the evaluation process and the associated dashboard. reference for code references and other resources.","title":"AI Room Utilization Model"},{"location":"ai/overview/#background","text":"The GTM Smart 2 AI signature project revolves around using AI and advanced analytics to extract actionable insights on energy use for schools across municipalities. Early on, the project group identified a bottleneck in the practical derivation of these insights: The available data did not provide a reliable measurement of whether a given room had been in use at a given time. To address this, the project group NTT DATA developed Tilly, a room utilization model, which is the subject of this documentation. As evident from the figure above, the solution has multiple components even though the name suggests it's solely an AI model. These components are covered in the following section. Tilly integrates with Enformanten's data flow and the associated (Snowflake) database but is otherwise closed to traffic.","title":"Background"},{"location":"ai/overview/#data-foundation","text":"From Enformanten's general data flow, Tilly receives a series of variables for each time unit (i.e., 15-minute intervals) for each room. The variable definitions are shown below. Variable Name Description Data Type ID Local ID Integer KOMMUNE Associated Municipality String SKOLE Associated School String DATE Date for the time interval String TIME Time for the time interval String DAYNAME e.g., Monday, Tuesday String TIDSPUNKT_TYPE e.g., School time or free time String SKEMALAGT Whether the time interval appears in the school schedule Boolean TYPE Type of day, e.g., Public holiday, special day, school vacation String NAVN Specific calendar day, e.g., Christmas holiday, summer vacation or Ascension Day String CO2 CO2 level measured in the given time interval Float TEMP Temperature measured in the given time interval Float MOTION Number of movements detected in the given time interval Float IAQ Indoor Air Quality value measured in the given time interval Float BOOKET Whether a booking is registered in the time interval Boolean By end of the development, only CO2 is included in the actual training of the model among the sensor variables (CO2, temperature, movement, and IAQ). This is partly due to inconsistent data quality for the remaining variables and partly due to iterative experimentation, where the aggregate performance of AI models has been evaluated by training on different combinations of the given sensor variables. This is further elaborated in the technical documentation of the solution. However, the architecture is designed for combinability, meaning that future development will be able to incorporate the remaining variables without major changes. For the same reason, Enformant's data flow is set up to deliver all existing sensor data to the Operational Optimization Model.","title":"Data Foundation"},{"location":"ai/overview/#components","text":"As mentioned earlier, the Operational Optimization Model contains multiple components. With the exception of the dashboard used for evaluation, these components are essential for the chosen architecture.","title":"Components"},{"location":"ai/overview/#model-registry","text":"Because the utilization rate is desired to be detected for each room, across schools, a separate AI model is trained per room. This results in 100+ AI models that are collectively trained on over 9 million variables. This number of models makes it necessary to create a model registry to systematically access and apply each model in the correct context. Here, a non-relational in-memory data structure is used in which each room, for each school, is indexed under a unique ID, derived from the school and the room's name.","title":"Model Registry"},{"location":"ai/overview/#preprocessing","text":"To train the models on the received data, extensive preprocessing is required. Preprocessing can be divided into two categories: Cleaning and Augmentation. Each subprocess in these categories is detailed below, where 1 \u2013 5 are data cleaning and 6 \u2013 7 are data augmentation.","title":"Preprocessing"},{"location":"ai/overview/#1-heuristic-estimation-of-utilization-rate","text":"To prepare the data for the AI models, the first step is to estimate the utilization rate for each room based on booked and planned time periods. The exact process for this is described in the technical documentation of the solution. This process provides prior knowledge for the AI models, which in advance adjust their expected level of utilization for a given room over time. Therefore, we use a heuristic method to create an informed basis for the model's predictions. Heuristic Estimation of Utilization Rate for Room Utilization In the business context of room scheduling and occupancy optimization, one of the most crucial preprocessing steps for training AI models is estimating the utilization rate of each room. This is the first step in a two-category preprocessing pipeline, which also includes Data Cleaning and Data Augmentation. Parameters and Heuristics Utilization Coefficient: A scaling factor used to fine-tune the estimated usage rate. Its default value is set at 2.1. Minimum Utilization Bound: A heuristic lower limit set to ensure that the estimated usage of a room doesn't fall below this value, typically set at 0.1. Maximum Utilization Bound: A heuristic upper limit set to ensure that the estimated usage of a room doesn't exceed this value, typically set at 0.4. Methodology The algorithm calculates the number of slots that are in use, which include times marked as either \"Booked\" or \"Scheduled\" according to the organizational schema. This count of used slots is then adjusted to ensure it doesn't fall below the predefined minimum utilization bound. The net estimated utilization Exception Cases If the algorithm encounters a situation where there are zero total slots, it returns a predefined fallback value, denoted as \"auto,\" to indicate that the utilization could not be computed. Business Implications This heuristic estimation serves as an informative prior for AI models aiming to predict room utilization. The model adjusts its expectations based on this heuristic, improving the overall accuracy and reliability of future predictions. This is especially valuable for optimizing room bookings and schedules, thereby leading to more efficient use of organizational resources.","title":"1. Heuristic Estimation of Utilization Rate"},{"location":"ai/overview/#local-development","text":"","title":"Local development"},{"location":"ai/reference/dashboard/","text":"Dashboard ai.tilly.dashboard The Dashboard module is responsible for the Performance Overview Dashboard, accessible on the root endpoints of the web app. This dashboard provides a visualization of CO2 over time, where each timeslot is colored by the predicted IN_USE state of each timeslot. The dashboard module is organized as follows: /dashboard/dashboard.html: The main HTML file that serves as the dashboard interface. /dashboard/plots: A subdirectory that holds room-specific data, organized by municipality, school, and room. Note that these .html documents are generated during model training. /dashboard/styles.css: The CSS file responsible for styling the dashboard. Dashboard Features Navigation The dashboard provides navigation buttons to move between municipalities, schools, and rooms, allowing users to explore different data points easily. Note that when navigating between municipalities, the dashboard will display the first school in the municipality, and when navigating between schools, the dashboard will display the first room in the school. Displaying Plots The dashboard dynamically loads and displays plots for selected rooms within the iframe container. The plots are organized in the /plots directory. Note that the dashboard relies on the endpoints in routes/dashboard.py to fetch plot data from the server. Initialization On startup, the dashboard fetches plot data from the server if any plots are available in /dashboard/plots. It then displays the plots for the first room in the first school in the first municipality. Customization You can customize the dashboard's appearance and behavior by modifying the CSS in styles.css and the HTML and JavaScript in dashboard.html. Additionally, you can extend its functionality to suit your specific requirements. Why do we need this? The Performance Overview Dashboard is a means of evaluation of our ML models. This is useful, since the data available for this task is unlabelled, which means that we cannot easily use quantitative metrics. As such, the web app will function without this module, but it is a useful tool for evaluating the performance of our models.","title":"Dashboard"},{"location":"ai/reference/dashboard/#dashboard","text":"","title":"Dashboard"},{"location":"ai/reference/dashboard/#ai.tilly.dashboard","text":"The Dashboard module is responsible for the Performance Overview Dashboard, accessible on the root endpoints of the web app. This dashboard provides a visualization of CO2 over time, where each timeslot is colored by the predicted IN_USE state of each timeslot. The dashboard module is organized as follows: /dashboard/dashboard.html: The main HTML file that serves as the dashboard interface. /dashboard/plots: A subdirectory that holds room-specific data, organized by municipality, school, and room. Note that these .html documents are generated during model training. /dashboard/styles.css: The CSS file responsible for styling the dashboard.","title":"dashboard"},{"location":"ai/reference/dashboard/#ai.tilly.dashboard--dashboard-features","text":"","title":"Dashboard Features"},{"location":"ai/reference/dashboard/#ai.tilly.dashboard--navigation","text":"The dashboard provides navigation buttons to move between municipalities, schools, and rooms, allowing users to explore different data points easily. Note that when navigating between municipalities, the dashboard will display the first school in the municipality, and when navigating between schools, the dashboard will display the first room in the school.","title":"Navigation"},{"location":"ai/reference/dashboard/#ai.tilly.dashboard--displaying-plots","text":"The dashboard dynamically loads and displays plots for selected rooms within the iframe container. The plots are organized in the /plots directory. Note that the dashboard relies on the endpoints in routes/dashboard.py to fetch plot data from the server.","title":"Displaying Plots"},{"location":"ai/reference/dashboard/#ai.tilly.dashboard--initialization","text":"On startup, the dashboard fetches plot data from the server if any plots are available in /dashboard/plots. It then displays the plots for the first room in the first school in the first municipality.","title":"Initialization"},{"location":"ai/reference/dashboard/#ai.tilly.dashboard--customization","text":"You can customize the dashboard's appearance and behavior by modifying the CSS in styles.css and the HTML and JavaScript in dashboard.html. Additionally, you can extend its functionality to suit your specific requirements.","title":"Customization"},{"location":"ai/reference/dashboard/#ai.tilly.dashboard--why-do-we-need-this","text":"The Performance Overview Dashboard is a means of evaluation of our ML models. This is useful, since the data available for this task is unlabelled, which means that we cannot easily use quantitative metrics. As such, the web app will function without this module, but it is a useful tool for evaluating the performance of our models.","title":"Why do we need this?"},{"location":"ai/reference/database/","text":"Database ai.tilly.database The database module is responsible for all communication with the databases. In this solution, we integrate with two databases: - One that holds tables of data related to Enformaten, and - One that holds a table of users. Note : In the current implementation, the users database is an in-memory SQLite database. Further work could be done to integrate with a more robust database solution. The specifications for each submodule are given in their respective docstrings. data The database.data module is responsible for all communication with the dataplatform used in Enformanten. The module is organized as follows: /database/data/db.py : Contains the database initialization and connections. /database/data/crud.py : Contains the functions that are used to interact with the database. /database/data/models.py : Contains the SQLAlchemy schemas that are used to interact with the database. crud Module for data-related CRUD operations This module contains functions that handle CRUD (Create, Read, Update, Delete) operations related to the data pipelines of Enformanten. push_data ( rooms : pd . DataFrame , table_name : str , session : Session ) -> None Push Data to a Snowflake Table. This function pushes a DataFrame to a specified Snowflake table. If the operation fails due to a ProgrammingError, the function will attempt to retry the operation with a new session. Parameters: Name Type Description Default rooms DataFrame The DataFrame containing the data to push. required table_name str The name of the Snowflake table to which data should be pushed. required session Session The SQLAlchemy session used for the operation. required Returns: Type Description None None Examples: import pandas as pd from sqlalchemy.orm import Session # Initialize SQLAlchemy session session = Session () # Example DataFrame # (See OUTPUT_COLUMNS constant in tilly.config for the # required column names) rooms = pd . DataFrame ({ 'Column1' : [ 1 , 2 , 3 ], 'Column2' : [ 'a' , 'b' , 'c' ], }) # Example table name table_name = \"YourSnowflakeTable\" # Push data to Snowflake table push_data ( rooms , table_name , session ) Source code in ai/tilly/database/data/crud.py 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 @retry ( stop = stop_after_attempt ( 3 ), retry = retry_if_exception_type ( ProgrammingError ), retry_error_callback = refresh_session , ) def push_data ( rooms : pd . DataFrame , table_name : str , session : Session ) -> None : \"\"\" Push Data to a Snowflake Table. This function pushes a DataFrame to a specified Snowflake table. If the operation fails due to a ProgrammingError, the function will attempt to retry the operation with a new session. Args: rooms (pd.DataFrame): The DataFrame containing the data to push. table_name (str): The name of the Snowflake table to which data should be pushed. session (Session): The SQLAlchemy session used for the operation. Returns: None Examples: ```python import pandas as pd from sqlalchemy.orm import Session # Initialize SQLAlchemy session session = Session() # Example DataFrame # (See OUTPUT_COLUMNS constant in tilly.config for the # required column names) rooms = pd.DataFrame({ 'Column1': [1, 2, 3], 'Column2': ['a', 'b', 'c'], }) # Example table name table_name = \"YourSnowflakeTable\" # Push data to Snowflake table push_data(rooms, table_name, session) ``` \"\"\" try : logger . debug ( f \"Sending { rooms . shape [ 0 ] } rows to { table_name } ..\" ) session . write_pandas ( rooms [ OUTPUT_COLUMNS ], f '\" { table_name } \"' , overwrite = False , quote_identifiers = False , ) except ProgrammingError as e : logger . debug ( f \" { e } - Retrying with new session ..\" ) session . close () # Explicitly close the session then raise error for retry raise e retrieve_data ( session : Session , table_name : str ) -> dict [ str , pd . DataFrame ] Retrieve Data from a Table and Group by School and Room IDs. This function retrieves data from a specified table and groups it by a unique identifier generated using the 'SKOLE' and 'ID' fields from the table. Each group of data is stored in a DataFrame, and these DataFrames are then stored in a dictionary. Args: session (Session): The SQLAlchemy session used to interact with the database. table_name (str): The name of the table from which to retrieve data. Returns: dict[str, pd.DataFrame]: A dictionary where each key is a unique identifier for a room, and the associated value is a DataFrame containing the data for that room. Examples: ```python from sqlalchemy.orm import Session import pandas as pd # Initialize SQLAlchemy session session = Session() # Example table name table_name = \"YourTableName\" # Retrieve and group data data = retrieve_data(session, table_name) # Access a specific room's data using its unique identifier room_id = \"School123_Room456\" room_data = data.get(room_id) if room_data is not None: logger.debug(f\"Data for room {room_id}: {room_data.head()}\") ``` Source code in ai/tilly/database/data/crud.py 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 def retrieve_data ( session : Session , table_name : str ) -> dict [ str , pd . DataFrame ]: \"\"\" Retrieve Data from a Table and Group by School and Room IDs. This function retrieves data from a specified table and groups it by a unique identifier generated using the 'SKOLE' and 'ID' fields from the table. Each group of data is stored in a DataFrame, and these DataFrames are then stored in a dictionary. Args: session (Session): The SQLAlchemy session used to interact with the database. table_name (str): The name of the table from which to retrieve data. Returns: dict[str, pd.DataFrame]: A dictionary where each key is a unique identifier for a room, and the associated value is a DataFrame containing the data for that room. Examples: ```python from sqlalchemy.orm import Session import pandas as pd # Initialize SQLAlchemy session session = Session() # Example table name table_name = \"YourTableName\" # Retrieve and group data data = retrieve_data(session, table_name) # Access a specific room's data using its unique identifier room_id = \"School123_Room456\" room_data = data.get(room_id) if room_data is not None: logger.debug(f\"Data for room {room_id}:\\n{room_data.head()}\") ``` \"\"\" logger . debug ( f \"Retrieving data from { table_name } \" ) data = { school_room : df for school_room , df in ( session . table ( f '\" { table_name } \"' ) . to_pandas () . assign ( SKOLE_ID = lambda d : d . SKOLE + \"_\" + d . ID ) . pipe ( log_size ) . rename ( str , axis = \"columns\" ) . groupby ( \"SKOLE_ID\" ) ) } return data db Snowflake Database Connection Script This module contains functions for establishing and managing connections to a Snowflake database. It relies on the snowflake.snowpark package to handle the actual database operations. get_session () -> Generator [ Session , None , None ] Get a Snowflake Session for Database Interactions. This function yields a Snowflake session for interacting with the Snowflake database. The session is yielded as a generator and should be used within a 'with' context to ensure that resources are managed appropriately. Yields: Type Description Session Generator[Session, None, None]: A generator yielding a Snowflake session object. The session is automatically closed when exiting the 'with' context. Examples: from your_module import MyTable # Replace with actual table class with get_session () as session : # Perform database operations using the 'session' object # Replace 'MyTable' and 'column_name' with actual table and column result = session . query ( MyTable ) . filter_by ( column_name = 'value' ) . all () # The session is automatically closed when the 'with' block is exited. Source code in ai/tilly/database/data/db.py 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 def get_session () -> Generator [ Session , None , None ]: \"\"\" Get a Snowflake Session for Database Interactions. This function yields a Snowflake session for interacting with the Snowflake database. The session is yielded as a generator and should be used within a 'with' context to ensure that resources are managed appropriately. Yields: Generator[Session, None, None]: A generator yielding a Snowflake session object. The session is automatically closed when exiting the 'with' context. Examples: ```python from your_module import MyTable # Replace with actual table class with get_session() as session: # Perform database operations using the 'session' object # Replace 'MyTable' and 'column_name' with actual table and column result = session.query(MyTable).filter_by(column_name='value').all() # The session is automatically closed when the 'with' block is exited. ``` \"\"\" with Session . builder . configs ( SNOWFLAKE_CREDENTIALS ) . create () as session : yield session refresh_session ( retry_state ) Create a new Snowflake session if an error occurs. using tenacity's before_retry hook Source code in ai/tilly/database/data/db.py 44 45 46 47 48 49 50 51 52 def refresh_session ( retry_state ): \"\"\"Create a new Snowflake session if an error occurs. using tenacity's before_retry hook\"\"\" old_session = retry_state . kwargs . get ( \"session\" ) if old_session : old_session . close () # Explicitly close the old session with get_session () as new_session : retry_state . kwargs [ \"session\" ] = new_session models Database Tables Definition This script defines SQLAlchemy database tables for training, unscored, and scored timeslots data. ScoredTimeslots Bases: DeclarativeBase SQLAlchemy table definition for scored timeslots data. Parameters: Name Type Description Default DeclarativeBase Base class for declarative SQLAlchemy table definitions. required Source code in ai/tilly/database/data/models.py 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 class ScoredTimeslots ( DeclarativeBase ): \"\"\" SQLAlchemy table definition for scored timeslots data. Args: DeclarativeBase: Base class for declarative SQLAlchemy table definitions. \"\"\" __tablename__ = SCORED_TABLE_NAME id = Column ( Integer , primary_key = True ) ID = Column ( \"ID\" , String , nullable = False ) KOMMUNE = Column ( \"KOMMUNE\" , String , nullable = False ) DATE = Column ( \"DATE\" , Date , nullable = False ) TIME = Column ( \"TIME\" , Time , nullable = False ) ANOMALY_SCORE = Column ( \"ANOMALY_SCORE\" , Float , nullable = True ) IN_USE = Column ( \"IN_USE\" , Boolean , nullable = True ) def as_dict ( self ) -> dict : \"\"\" Convert the table row to a dictionary. Returns: dict: A dictionary representing the table row. \"\"\" return { c . key : getattr ( self , c . key ) for c in self . __table__ . columns } as_dict () -> dict Convert the table row to a dictionary. Returns: Name Type Description dict dict A dictionary representing the table row. Source code in ai/tilly/database/data/models.py 155 156 157 158 159 160 161 162 def as_dict ( self ) -> dict : \"\"\" Convert the table row to a dictionary. Returns: dict: A dictionary representing the table row. \"\"\" return { c . key : getattr ( self , c . key ) for c in self . __table__ . columns } TrainingTimeslots Bases: DeclarativeBase SQLAlchemy table definition for training timeslots data. Parameters: Name Type Description Default DeclarativeBase Base class for declarative SQLAlchemy table definitions. required Example: # Create a new TrainingTimeslots object new_training_entry = TrainingTimeslots ( room_id = 123 , municipality = \"Example Municipality\" , school = \"Example School\" , date = \"2023-10-17\" , time = \"08:00 AM\" , dayname = \"Monday\" , time_type = \"Regular\" , scheduled = True , type = \"Classroom\" , name = \"Room 101\" , co2 = 400.0 , temp = 22.5 , motion = 1.0 , iaq = 0.95 , booked = True ) Source code in ai/tilly/database/data/models.py 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 class TrainingTimeslots ( DeclarativeBase ): \"\"\" SQLAlchemy table definition for training timeslots data. Args: DeclarativeBase: Base class for declarative SQLAlchemy table definitions. Example: ```python # Create a new TrainingTimeslots object new_training_entry = TrainingTimeslots( room_id=123, municipality=\"Example Municipality\", school=\"Example School\", date=\"2023-10-17\", time=\"08:00 AM\", dayname=\"Monday\", time_type=\"Regular\", scheduled=True, type=\"Classroom\", name=\"Room 101\", co2=400.0, temp=22.5, motion=1.0, iaq=0.95, booked=True ) ``` \"\"\" __tablename__ = TRAINING_TABLE_NAME id = Column ( Integer , primary_key = True ) room_id = Column ( \"ID\" , Integer , nullable = False ) municipality = Column ( \"KOMMUNE\" , String , nullable = False ) school = Column ( \"SKOLE\" , String , nullable = False ) date = Column ( \"DATE\" , String , nullable = False ) time = Column ( \"TIME\" , String , nullable = False ) dayname = Column ( \"DAYNAME\" , String , nullable = False ) time_type = Column ( \"TIDSPUNKT_TYPE\" , String , nullable = False ) scheduled = Column ( \"SKEMALAGT\" , Boolean , nullable = False ) type = Column ( \"TYPE\" , String , nullable = False ) name = Column ( \"NAVN\" , String , nullable = False ) co2 = Column ( \"CO2\" , Float , nullable = True ) temp = Column ( \"TEMP\" , Float , nullable = True ) motion = Column ( \"MOTION\" , Float , nullable = True ) iaq = Column ( \"IAQ\" , Float , nullable = True ) booked = Column ( \"BOOKET\" , Boolean , nullable = True ) def as_dict ( self ) -> dict : \"\"\" Convert the table row to a dictionary. Returns: dict: A dictionary representing the table row. \"\"\" return { c . key : getattr ( self , c . key ) for c in self . __table__ . columns } as_dict () -> dict Convert the table row to a dictionary. Returns: Name Type Description dict dict A dictionary representing the table row. Source code in ai/tilly/database/data/models.py 66 67 68 69 70 71 72 73 def as_dict ( self ) -> dict : \"\"\" Convert the table row to a dictionary. Returns: dict: A dictionary representing the table row. \"\"\" return { c . key : getattr ( self , c . key ) for c in self . __table__ . columns } UnscoredTimeslots Bases: DeclarativeBase SQLAlchemy table definition for unscored timeslots data. Parameters: Name Type Description Default DeclarativeBase Base class for declarative SQLAlchemy table definitions. required Example: # Create a new UnscoredTimeslots object new_unscored_entry = UnscoredTimeslots ( room_id = \"Room101\" , municipality = \"Example Municipality\" , school = \"Example School\" , date = \"2023-10-17\" , time = \"08:00 AM\" , dayname = \"Monday\" , time_type = \"Regular\" , scheduled = True , type = \"Classroom\" , name = \"Room 101\" , co2 = 400.0 , temp = 22.5 , motion = 1.0 , iaq = 0.95 , booked = True ) Source code in ai/tilly/database/data/models.py 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 class UnscoredTimeslots ( DeclarativeBase ): \"\"\" SQLAlchemy table definition for unscored timeslots data. Args: DeclarativeBase: Base class for declarative SQLAlchemy table definitions. Example: ```python # Create a new UnscoredTimeslots object new_unscored_entry = UnscoredTimeslots( room_id=\"Room101\", municipality=\"Example Municipality\", school=\"Example School\", date=\"2023-10-17\", time=\"08:00 AM\", dayname=\"Monday\", time_type=\"Regular\", scheduled=True, type=\"Classroom\", name=\"Room 101\", co2=400.0, temp=22.5, motion=1.0, iaq=0.95, booked=True ) ``` \"\"\" __tablename__ = UNSCORED_TABLE_NAME id = Column ( Integer , primary_key = True ) room_id = Column ( \"ID\" , String , nullable = False ) municipality = Column ( \"KOMMUNE\" , String , nullable = False ) school = Column ( \"SKOLE\" , String , nullable = False ) date = Column ( \"DATE\" , String , nullable = False ) time = Column ( \"TIME\" , String , nullable = False ) dayname = Column ( \"DAYNAME\" , String , nullable = False ) time_type = Column ( \"TIDSPUNKT_TYPE\" , String , nullable = False ) scheduled = Column ( \"SKEMALAGT\" , Boolean , nullable = False ) type = Column ( \"TYPE\" , String , nullable = False ) name = Column ( \"NAVN\" , String , nullable = False ) co2 = Column ( \"CO2\" , Float , nullable = True ) temp = Column ( \"TEMP\" , Float , nullable = True ) motion = Column ( \"MOTION\" , Float , nullable = True ) iaq = Column ( \"IAQ\" , Float , nullable = True ) booked = Column ( \"BOOKET\" , Boolean , nullable = True ) def as_dict ( self ) -> dict : \"\"\" Convert the table row to a dictionary. Returns: dict: A dictionary representing the table row. \"\"\" return { c . key : getattr ( self , c . key ) for c in self . __table__ . columns } as_dict () -> dict Convert the table row to a dictionary. Returns: Name Type Description dict dict A dictionary representing the table row. Source code in ai/tilly/database/data/models.py 126 127 128 129 130 131 132 133 def as_dict ( self ) -> dict : \"\"\" Convert the table row to a dictionary. Returns: dict: A dictionary representing the table row. \"\"\" return { c . key : getattr ( self , c . key ) for c in self . __table__ . columns } users The database.users module is responsible for all communication with the in-memory sqllite database that stores user credentials. The module is organized as follows: /database/users/crud.py : Contains the functions that are used to interact with the database. /database/users/models.py : Contains the SQLAlchemy schemas that are used to interact with the database. NOTE : This project uses fastapi-users to automate parts of the user management. The fastapi-users configuration can be found in /users/ . crud Asynchronous Database Connection and Session for user CRUD Operations. Defines an asynchronous database connection and session for user data. Also defines a function to create the database and tables on startup. create_db_and_tables () async Create the database and tables. Example: # Create the database and tables asynchronously await create_db_and_tables () Source code in ai/tilly/database/users/crud.py 24 25 26 27 28 29 30 31 32 33 34 35 async def create_db_and_tables (): \"\"\" Create the database and tables. Example: ```python # Create the database and tables asynchronously await create_db_and_tables() ``` \"\"\" async with engine . begin () as conn : await conn . run_sync ( Base . metadata . create_all ) get_async_session () -> AsyncGenerator [ AsyncSession , None ] async Get an asynchronous database session. Returns: Name Type Description AsyncSession AsyncGenerator [ AsyncSession , None] An asynchronous database session. Example: async with get_async_session () as session : # Use the database session for asynchronous operations user = await session . get ( User , user_id ) Source code in ai/tilly/database/users/crud.py 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 async def get_async_session () -> AsyncGenerator [ AsyncSession , None ]: \"\"\" Get an asynchronous database session. Returns: AsyncSession: An asynchronous database session. Example: ```python async with get_async_session() as session: # Use the database session for asynchronous operations user = await session.get(User, user_id) ``` \"\"\" async with Session () as session : yield session models User Database Models Script This script defines database models for user data using fastapi-users and SQLAlchemy. Base Bases: DeclarativeBase Base class for database models. Parameters: Name Type Description Default DeclarativeBase Base class for declarative SQLAlchemy required Source code in ai/tilly/database/users/models.py 15 16 17 18 19 20 21 22 23 24 25 class Base ( DeclarativeBase ): \"\"\" Base class for database models. Args: DeclarativeBase: Base class for declarative SQLAlchemy table definitions. \"\"\" __abstract__ = True pass User Bases: SQLAlchemyBaseUserTableUUID , Base Database model for user data. Parameters: Name Type Description Default SQLAlchemyBaseUserTableUUID Base class for fastapi-users user table definitions. required Base Base class for database models. required Example: # Create a new User object new_user = User ( email = \"example@example.com\" , hashed_password = \"hashed_password\" , is_active = True ) Source code in ai/tilly/database/users/models.py 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 class User ( SQLAlchemyBaseUserTableUUID , Base ): \"\"\" Database model for user data. Args: SQLAlchemyBaseUserTableUUID: Base class for fastapi-users user table definitions. Base: Base class for database models. Example: ```python # Create a new User object new_user = User( email=\"example@example.com\", hashed_password=\"hashed_password\", is_active=True ) ``` \"\"\" __tablename__ = \"users\" pass","title":"Database"},{"location":"ai/reference/database/#database","text":"","title":"Database"},{"location":"ai/reference/database/#ai.tilly.database","text":"The database module is responsible for all communication with the databases. In this solution, we integrate with two databases: - One that holds tables of data related to Enformaten, and - One that holds a table of users. Note : In the current implementation, the users database is an in-memory SQLite database. Further work could be done to integrate with a more robust database solution. The specifications for each submodule are given in their respective docstrings.","title":"database"},{"location":"ai/reference/database/#ai.tilly.database.data","text":"The database.data module is responsible for all communication with the dataplatform used in Enformanten. The module is organized as follows: /database/data/db.py : Contains the database initialization and connections. /database/data/crud.py : Contains the functions that are used to interact with the database. /database/data/models.py : Contains the SQLAlchemy schemas that are used to interact with the database.","title":"data"},{"location":"ai/reference/database/#ai.tilly.database.data.crud","text":"Module for data-related CRUD operations This module contains functions that handle CRUD (Create, Read, Update, Delete) operations related to the data pipelines of Enformanten.","title":"crud"},{"location":"ai/reference/database/#ai.tilly.database.data.crud.push_data","text":"Push Data to a Snowflake Table. This function pushes a DataFrame to a specified Snowflake table. If the operation fails due to a ProgrammingError, the function will attempt to retry the operation with a new session. Parameters: Name Type Description Default rooms DataFrame The DataFrame containing the data to push. required table_name str The name of the Snowflake table to which data should be pushed. required session Session The SQLAlchemy session used for the operation. required Returns: Type Description None None Examples: import pandas as pd from sqlalchemy.orm import Session # Initialize SQLAlchemy session session = Session () # Example DataFrame # (See OUTPUT_COLUMNS constant in tilly.config for the # required column names) rooms = pd . DataFrame ({ 'Column1' : [ 1 , 2 , 3 ], 'Column2' : [ 'a' , 'b' , 'c' ], }) # Example table name table_name = \"YourSnowflakeTable\" # Push data to Snowflake table push_data ( rooms , table_name , session ) Source code in ai/tilly/database/data/crud.py 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 @retry ( stop = stop_after_attempt ( 3 ), retry = retry_if_exception_type ( ProgrammingError ), retry_error_callback = refresh_session , ) def push_data ( rooms : pd . DataFrame , table_name : str , session : Session ) -> None : \"\"\" Push Data to a Snowflake Table. This function pushes a DataFrame to a specified Snowflake table. If the operation fails due to a ProgrammingError, the function will attempt to retry the operation with a new session. Args: rooms (pd.DataFrame): The DataFrame containing the data to push. table_name (str): The name of the Snowflake table to which data should be pushed. session (Session): The SQLAlchemy session used for the operation. Returns: None Examples: ```python import pandas as pd from sqlalchemy.orm import Session # Initialize SQLAlchemy session session = Session() # Example DataFrame # (See OUTPUT_COLUMNS constant in tilly.config for the # required column names) rooms = pd.DataFrame({ 'Column1': [1, 2, 3], 'Column2': ['a', 'b', 'c'], }) # Example table name table_name = \"YourSnowflakeTable\" # Push data to Snowflake table push_data(rooms, table_name, session) ``` \"\"\" try : logger . debug ( f \"Sending { rooms . shape [ 0 ] } rows to { table_name } ..\" ) session . write_pandas ( rooms [ OUTPUT_COLUMNS ], f '\" { table_name } \"' , overwrite = False , quote_identifiers = False , ) except ProgrammingError as e : logger . debug ( f \" { e } - Retrying with new session ..\" ) session . close () # Explicitly close the session then raise error for retry raise e","title":"push_data()"},{"location":"ai/reference/database/#ai.tilly.database.data.crud.retrieve_data","text":"Retrieve Data from a Table and Group by School and Room IDs. This function retrieves data from a specified table and groups it by a unique identifier generated using the 'SKOLE' and 'ID' fields from the table. Each group of data is stored in a DataFrame, and these DataFrames are then stored in a dictionary. Args: session (Session): The SQLAlchemy session used to interact with the database. table_name (str): The name of the table from which to retrieve data. Returns: dict[str, pd.DataFrame]: A dictionary where each key is a unique identifier for a room, and the associated value is a DataFrame containing the data for that room. Examples: ```python from sqlalchemy.orm import Session import pandas as pd # Initialize SQLAlchemy session session = Session() # Example table name table_name = \"YourTableName\" # Retrieve and group data data = retrieve_data(session, table_name) # Access a specific room's data using its unique identifier room_id = \"School123_Room456\" room_data = data.get(room_id) if room_data is not None: logger.debug(f\"Data for room {room_id}: {room_data.head()}\") ``` Source code in ai/tilly/database/data/crud.py 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 def retrieve_data ( session : Session , table_name : str ) -> dict [ str , pd . DataFrame ]: \"\"\" Retrieve Data from a Table and Group by School and Room IDs. This function retrieves data from a specified table and groups it by a unique identifier generated using the 'SKOLE' and 'ID' fields from the table. Each group of data is stored in a DataFrame, and these DataFrames are then stored in a dictionary. Args: session (Session): The SQLAlchemy session used to interact with the database. table_name (str): The name of the table from which to retrieve data. Returns: dict[str, pd.DataFrame]: A dictionary where each key is a unique identifier for a room, and the associated value is a DataFrame containing the data for that room. Examples: ```python from sqlalchemy.orm import Session import pandas as pd # Initialize SQLAlchemy session session = Session() # Example table name table_name = \"YourTableName\" # Retrieve and group data data = retrieve_data(session, table_name) # Access a specific room's data using its unique identifier room_id = \"School123_Room456\" room_data = data.get(room_id) if room_data is not None: logger.debug(f\"Data for room {room_id}:\\n{room_data.head()}\") ``` \"\"\" logger . debug ( f \"Retrieving data from { table_name } \" ) data = { school_room : df for school_room , df in ( session . table ( f '\" { table_name } \"' ) . to_pandas () . assign ( SKOLE_ID = lambda d : d . SKOLE + \"_\" + d . ID ) . pipe ( log_size ) . rename ( str , axis = \"columns\" ) . groupby ( \"SKOLE_ID\" ) ) } return data","title":"retrieve_data()"},{"location":"ai/reference/database/#ai.tilly.database.data.db","text":"Snowflake Database Connection Script This module contains functions for establishing and managing connections to a Snowflake database. It relies on the snowflake.snowpark package to handle the actual database operations.","title":"db"},{"location":"ai/reference/database/#ai.tilly.database.data.db.get_session","text":"Get a Snowflake Session for Database Interactions. This function yields a Snowflake session for interacting with the Snowflake database. The session is yielded as a generator and should be used within a 'with' context to ensure that resources are managed appropriately. Yields: Type Description Session Generator[Session, None, None]: A generator yielding a Snowflake session object. The session is automatically closed when exiting the 'with' context. Examples: from your_module import MyTable # Replace with actual table class with get_session () as session : # Perform database operations using the 'session' object # Replace 'MyTable' and 'column_name' with actual table and column result = session . query ( MyTable ) . filter_by ( column_name = 'value' ) . all () # The session is automatically closed when the 'with' block is exited. Source code in ai/tilly/database/data/db.py 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 def get_session () -> Generator [ Session , None , None ]: \"\"\" Get a Snowflake Session for Database Interactions. This function yields a Snowflake session for interacting with the Snowflake database. The session is yielded as a generator and should be used within a 'with' context to ensure that resources are managed appropriately. Yields: Generator[Session, None, None]: A generator yielding a Snowflake session object. The session is automatically closed when exiting the 'with' context. Examples: ```python from your_module import MyTable # Replace with actual table class with get_session() as session: # Perform database operations using the 'session' object # Replace 'MyTable' and 'column_name' with actual table and column result = session.query(MyTable).filter_by(column_name='value').all() # The session is automatically closed when the 'with' block is exited. ``` \"\"\" with Session . builder . configs ( SNOWFLAKE_CREDENTIALS ) . create () as session : yield session","title":"get_session()"},{"location":"ai/reference/database/#ai.tilly.database.data.db.refresh_session","text":"Create a new Snowflake session if an error occurs. using tenacity's before_retry hook Source code in ai/tilly/database/data/db.py 44 45 46 47 48 49 50 51 52 def refresh_session ( retry_state ): \"\"\"Create a new Snowflake session if an error occurs. using tenacity's before_retry hook\"\"\" old_session = retry_state . kwargs . get ( \"session\" ) if old_session : old_session . close () # Explicitly close the old session with get_session () as new_session : retry_state . kwargs [ \"session\" ] = new_session","title":"refresh_session()"},{"location":"ai/reference/database/#ai.tilly.database.data.models","text":"Database Tables Definition This script defines SQLAlchemy database tables for training, unscored, and scored timeslots data.","title":"models"},{"location":"ai/reference/database/#ai.tilly.database.data.models.ScoredTimeslots","text":"Bases: DeclarativeBase SQLAlchemy table definition for scored timeslots data. Parameters: Name Type Description Default DeclarativeBase Base class for declarative SQLAlchemy table definitions. required Source code in ai/tilly/database/data/models.py 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 class ScoredTimeslots ( DeclarativeBase ): \"\"\" SQLAlchemy table definition for scored timeslots data. Args: DeclarativeBase: Base class for declarative SQLAlchemy table definitions. \"\"\" __tablename__ = SCORED_TABLE_NAME id = Column ( Integer , primary_key = True ) ID = Column ( \"ID\" , String , nullable = False ) KOMMUNE = Column ( \"KOMMUNE\" , String , nullable = False ) DATE = Column ( \"DATE\" , Date , nullable = False ) TIME = Column ( \"TIME\" , Time , nullable = False ) ANOMALY_SCORE = Column ( \"ANOMALY_SCORE\" , Float , nullable = True ) IN_USE = Column ( \"IN_USE\" , Boolean , nullable = True ) def as_dict ( self ) -> dict : \"\"\" Convert the table row to a dictionary. Returns: dict: A dictionary representing the table row. \"\"\" return { c . key : getattr ( self , c . key ) for c in self . __table__ . columns }","title":"ScoredTimeslots"},{"location":"ai/reference/database/#ai.tilly.database.data.models.ScoredTimeslots.as_dict","text":"Convert the table row to a dictionary. Returns: Name Type Description dict dict A dictionary representing the table row. Source code in ai/tilly/database/data/models.py 155 156 157 158 159 160 161 162 def as_dict ( self ) -> dict : \"\"\" Convert the table row to a dictionary. Returns: dict: A dictionary representing the table row. \"\"\" return { c . key : getattr ( self , c . key ) for c in self . __table__ . columns }","title":"as_dict()"},{"location":"ai/reference/database/#ai.tilly.database.data.models.TrainingTimeslots","text":"Bases: DeclarativeBase SQLAlchemy table definition for training timeslots data. Parameters: Name Type Description Default DeclarativeBase Base class for declarative SQLAlchemy table definitions. required Example: # Create a new TrainingTimeslots object new_training_entry = TrainingTimeslots ( room_id = 123 , municipality = \"Example Municipality\" , school = \"Example School\" , date = \"2023-10-17\" , time = \"08:00 AM\" , dayname = \"Monday\" , time_type = \"Regular\" , scheduled = True , type = \"Classroom\" , name = \"Room 101\" , co2 = 400.0 , temp = 22.5 , motion = 1.0 , iaq = 0.95 , booked = True ) Source code in ai/tilly/database/data/models.py 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 class TrainingTimeslots ( DeclarativeBase ): \"\"\" SQLAlchemy table definition for training timeslots data. Args: DeclarativeBase: Base class for declarative SQLAlchemy table definitions. Example: ```python # Create a new TrainingTimeslots object new_training_entry = TrainingTimeslots( room_id=123, municipality=\"Example Municipality\", school=\"Example School\", date=\"2023-10-17\", time=\"08:00 AM\", dayname=\"Monday\", time_type=\"Regular\", scheduled=True, type=\"Classroom\", name=\"Room 101\", co2=400.0, temp=22.5, motion=1.0, iaq=0.95, booked=True ) ``` \"\"\" __tablename__ = TRAINING_TABLE_NAME id = Column ( Integer , primary_key = True ) room_id = Column ( \"ID\" , Integer , nullable = False ) municipality = Column ( \"KOMMUNE\" , String , nullable = False ) school = Column ( \"SKOLE\" , String , nullable = False ) date = Column ( \"DATE\" , String , nullable = False ) time = Column ( \"TIME\" , String , nullable = False ) dayname = Column ( \"DAYNAME\" , String , nullable = False ) time_type = Column ( \"TIDSPUNKT_TYPE\" , String , nullable = False ) scheduled = Column ( \"SKEMALAGT\" , Boolean , nullable = False ) type = Column ( \"TYPE\" , String , nullable = False ) name = Column ( \"NAVN\" , String , nullable = False ) co2 = Column ( \"CO2\" , Float , nullable = True ) temp = Column ( \"TEMP\" , Float , nullable = True ) motion = Column ( \"MOTION\" , Float , nullable = True ) iaq = Column ( \"IAQ\" , Float , nullable = True ) booked = Column ( \"BOOKET\" , Boolean , nullable = True ) def as_dict ( self ) -> dict : \"\"\" Convert the table row to a dictionary. Returns: dict: A dictionary representing the table row. \"\"\" return { c . key : getattr ( self , c . key ) for c in self . __table__ . columns }","title":"TrainingTimeslots"},{"location":"ai/reference/database/#ai.tilly.database.data.models.TrainingTimeslots.as_dict","text":"Convert the table row to a dictionary. Returns: Name Type Description dict dict A dictionary representing the table row. Source code in ai/tilly/database/data/models.py 66 67 68 69 70 71 72 73 def as_dict ( self ) -> dict : \"\"\" Convert the table row to a dictionary. Returns: dict: A dictionary representing the table row. \"\"\" return { c . key : getattr ( self , c . key ) for c in self . __table__ . columns }","title":"as_dict()"},{"location":"ai/reference/database/#ai.tilly.database.data.models.UnscoredTimeslots","text":"Bases: DeclarativeBase SQLAlchemy table definition for unscored timeslots data. Parameters: Name Type Description Default DeclarativeBase Base class for declarative SQLAlchemy table definitions. required Example: # Create a new UnscoredTimeslots object new_unscored_entry = UnscoredTimeslots ( room_id = \"Room101\" , municipality = \"Example Municipality\" , school = \"Example School\" , date = \"2023-10-17\" , time = \"08:00 AM\" , dayname = \"Monday\" , time_type = \"Regular\" , scheduled = True , type = \"Classroom\" , name = \"Room 101\" , co2 = 400.0 , temp = 22.5 , motion = 1.0 , iaq = 0.95 , booked = True ) Source code in ai/tilly/database/data/models.py 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 class UnscoredTimeslots ( DeclarativeBase ): \"\"\" SQLAlchemy table definition for unscored timeslots data. Args: DeclarativeBase: Base class for declarative SQLAlchemy table definitions. Example: ```python # Create a new UnscoredTimeslots object new_unscored_entry = UnscoredTimeslots( room_id=\"Room101\", municipality=\"Example Municipality\", school=\"Example School\", date=\"2023-10-17\", time=\"08:00 AM\", dayname=\"Monday\", time_type=\"Regular\", scheduled=True, type=\"Classroom\", name=\"Room 101\", co2=400.0, temp=22.5, motion=1.0, iaq=0.95, booked=True ) ``` \"\"\" __tablename__ = UNSCORED_TABLE_NAME id = Column ( Integer , primary_key = True ) room_id = Column ( \"ID\" , String , nullable = False ) municipality = Column ( \"KOMMUNE\" , String , nullable = False ) school = Column ( \"SKOLE\" , String , nullable = False ) date = Column ( \"DATE\" , String , nullable = False ) time = Column ( \"TIME\" , String , nullable = False ) dayname = Column ( \"DAYNAME\" , String , nullable = False ) time_type = Column ( \"TIDSPUNKT_TYPE\" , String , nullable = False ) scheduled = Column ( \"SKEMALAGT\" , Boolean , nullable = False ) type = Column ( \"TYPE\" , String , nullable = False ) name = Column ( \"NAVN\" , String , nullable = False ) co2 = Column ( \"CO2\" , Float , nullable = True ) temp = Column ( \"TEMP\" , Float , nullable = True ) motion = Column ( \"MOTION\" , Float , nullable = True ) iaq = Column ( \"IAQ\" , Float , nullable = True ) booked = Column ( \"BOOKET\" , Boolean , nullable = True ) def as_dict ( self ) -> dict : \"\"\" Convert the table row to a dictionary. Returns: dict: A dictionary representing the table row. \"\"\" return { c . key : getattr ( self , c . key ) for c in self . __table__ . columns }","title":"UnscoredTimeslots"},{"location":"ai/reference/database/#ai.tilly.database.data.models.UnscoredTimeslots.as_dict","text":"Convert the table row to a dictionary. Returns: Name Type Description dict dict A dictionary representing the table row. Source code in ai/tilly/database/data/models.py 126 127 128 129 130 131 132 133 def as_dict ( self ) -> dict : \"\"\" Convert the table row to a dictionary. Returns: dict: A dictionary representing the table row. \"\"\" return { c . key : getattr ( self , c . key ) for c in self . __table__ . columns }","title":"as_dict()"},{"location":"ai/reference/database/#ai.tilly.database.users","text":"The database.users module is responsible for all communication with the in-memory sqllite database that stores user credentials. The module is organized as follows: /database/users/crud.py : Contains the functions that are used to interact with the database. /database/users/models.py : Contains the SQLAlchemy schemas that are used to interact with the database. NOTE : This project uses fastapi-users to automate parts of the user management. The fastapi-users configuration can be found in /users/ .","title":"users"},{"location":"ai/reference/database/#ai.tilly.database.users.crud","text":"Asynchronous Database Connection and Session for user CRUD Operations. Defines an asynchronous database connection and session for user data. Also defines a function to create the database and tables on startup.","title":"crud"},{"location":"ai/reference/database/#ai.tilly.database.users.crud.create_db_and_tables","text":"Create the database and tables. Example: # Create the database and tables asynchronously await create_db_and_tables () Source code in ai/tilly/database/users/crud.py 24 25 26 27 28 29 30 31 32 33 34 35 async def create_db_and_tables (): \"\"\" Create the database and tables. Example: ```python # Create the database and tables asynchronously await create_db_and_tables() ``` \"\"\" async with engine . begin () as conn : await conn . run_sync ( Base . metadata . create_all )","title":"create_db_and_tables()"},{"location":"ai/reference/database/#ai.tilly.database.users.crud.get_async_session","text":"Get an asynchronous database session. Returns: Name Type Description AsyncSession AsyncGenerator [ AsyncSession , None] An asynchronous database session. Example: async with get_async_session () as session : # Use the database session for asynchronous operations user = await session . get ( User , user_id ) Source code in ai/tilly/database/users/crud.py 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 async def get_async_session () -> AsyncGenerator [ AsyncSession , None ]: \"\"\" Get an asynchronous database session. Returns: AsyncSession: An asynchronous database session. Example: ```python async with get_async_session() as session: # Use the database session for asynchronous operations user = await session.get(User, user_id) ``` \"\"\" async with Session () as session : yield session","title":"get_async_session()"},{"location":"ai/reference/database/#ai.tilly.database.users.models","text":"User Database Models Script This script defines database models for user data using fastapi-users and SQLAlchemy.","title":"models"},{"location":"ai/reference/database/#ai.tilly.database.users.models.Base","text":"Bases: DeclarativeBase Base class for database models. Parameters: Name Type Description Default DeclarativeBase Base class for declarative SQLAlchemy required Source code in ai/tilly/database/users/models.py 15 16 17 18 19 20 21 22 23 24 25 class Base ( DeclarativeBase ): \"\"\" Base class for database models. Args: DeclarativeBase: Base class for declarative SQLAlchemy table definitions. \"\"\" __abstract__ = True pass","title":"Base"},{"location":"ai/reference/database/#ai.tilly.database.users.models.User","text":"Bases: SQLAlchemyBaseUserTableUUID , Base Database model for user data. Parameters: Name Type Description Default SQLAlchemyBaseUserTableUUID Base class for fastapi-users user table definitions. required Base Base class for database models. required Example: # Create a new User object new_user = User ( email = \"example@example.com\" , hashed_password = \"hashed_password\" , is_active = True ) Source code in ai/tilly/database/users/models.py 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 class User ( SQLAlchemyBaseUserTableUUID , Base ): \"\"\" Database model for user data. Args: SQLAlchemyBaseUserTableUUID: Base class for fastapi-users user table definitions. Base: Base class for database models. Example: ```python # Create a new User object new_user = User( email=\"example@example.com\", hashed_password=\"hashed_password\", is_active=True ) ``` \"\"\" __tablename__ = \"users\" pass","title":"User"},{"location":"ai/reference/routes/","text":"Routes ai.tilly.routes dashboard FastAPI Router for Tilly Dashboard and Plots This module contains routes for serving the Tilly dashboard and for retrieving the directory structure of the plots. It uses FastAPI and depends on the Jinja2 templating engine to render HTML responses. create_plot_structure ( root_path : Path ) -> Optional [ Dict ] Generate Plot Directory Structure. This function takes a root path and returns a dictionary representing the directory structure rooted at that path. Parameters: Name Type Description Default root_path Path The root path from which to generate the directory structure. required Returns: Type Description Optional [ Dict ] Optional[Dict]: The dictionary representing the directory structure or None if the provided path is not a directory. Examples: from pathlib import Path root_path = Path ( \"/path/to/plots\" ) structure = create_plot_structure ( root_path ) Source code in ai/tilly/routes/dashboard.py 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 def create_plot_structure ( root_path : Path ) -> Optional [ Dict ]: \"\"\" Generate Plot Directory Structure. This function takes a root path and returns a dictionary representing the directory structure rooted at that path. Args: root_path (Path): The root path from which to generate the directory structure. Returns: Optional[Dict]: The dictionary representing the directory structure or `None` if the provided path is not a directory. Examples: ```python from pathlib import Path root_path = Path(\"/path/to/plots\") structure = create_plot_structure(root_path) ``` \"\"\" if not root_path . is_dir (): return None return { child . name : create_plot_structure ( child ) for child in root_path . iterdir ()} get_plots_structure () -> Optional [ Dict ] | HTTPException Get Plot Directory Structure. This route returns the directory structure of the plots as a JSON object. If the directory structure is invalid, a 404 HTTP error is raised. Returns: Type Description Optional [ Dict ] | HTTPException Optional[Dict]: The dictionary representing the directory structure. Examples: curl http://localhost:8000/plots_structure This will return a JSON object representing the directory structure of the plots. Source code in ai/tilly/routes/dashboard.py 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 @router . get ( \"/plots_structure\" , response_model = Optional [ Dict ]) def get_plots_structure () -> Optional [ Dict ] | HTTPException : \"\"\" Get Plot Directory Structure. This route returns the directory structure of the plots as a JSON object. If the directory structure is invalid, a 404 HTTP error is raised. Returns: Optional[Dict]: The dictionary representing the directory structure. Examples: ```bash curl http://localhost:8000/plots_structure ``` This will return a JSON object representing the directory structure of the plots. \"\"\" plot_dir = Path ( c . PLOTS_DIR ) result = create_plot_structure ( plot_dir ) if result is None : raise HTTPException ( status_code = 404 , detail = \"Invalid directory structure\" ) return result read_root ( request : Request ) -> HTMLResponse Serve the Tilly Dashboard. This route returns an HTML response that serves the Tilly dashboard. Parameters: Name Type Description Default request Request The FastAPI request object. required Returns: Name Type Description HTMLResponse HTMLResponse The HTML response containing the rendered Tilly dashboard. Examples: curl http://localhost:8000/ This will return the HTML content of the Tilly dashboard. Source code in ai/tilly/routes/dashboard.py 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 @router . get ( \"/\" , response_class = HTMLResponse ) def read_root ( request : Request ) -> HTMLResponse : \"\"\" Serve the Tilly Dashboard. This route returns an HTML response that serves the Tilly dashboard. Args: request (Request): The FastAPI request object. Returns: HTMLResponse: The HTML response containing the rendered Tilly dashboard. Examples: ```bash curl http://localhost:8000/ ``` This will return the HTML content of the Tilly dashboard. \"\"\" return templates . TemplateResponse ( \"dashboard.html\" , { \"request\" : request }) heartbeat Heartbeat Endpoint for Tilly Service This module contains a FastAPI router for serving a heartbeat endpoint. The endpoint returns the current version of the service based on the GIT metadata. heartbeat ( _ : Request ) -> dict Heartbeat Endpoint for Health Check and Versioning. This route returns a JSON object containing the current version of the service. The version information is retrieved from an environment variable, GIT_METADATA , which is set during a GitHub Actions job. Args: request (Request): The FastAPI request object. This argument is ignored but included for potential future use. Returns: dict: A dictionary containing the version information, with key \"version\" and value as the short version of the git hash of the last commit. Examples: ```bash curl http://localhost:8000/heartbeat/ ``` Output: ```json { \"version\": \"abc123\" # The git hash short version } ``` Note: The `GIT_METADATA` environment variable must be set, usually during a GitHub Actions job, for this endpoint to return accurate version information. If run locally, the version will be set to \"local\". Source code in ai/tilly/routes/heartbeat.py 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 @router . get ( \"/heartbeat/\" ) def heartbeat ( _ : Request ) -> dict : \"\"\" Heartbeat Endpoint for Health Check and Versioning. This route returns a JSON object containing the current version of the service. The version information is retrieved from an environment variable, `GIT_METADATA`, which is set during a GitHub Actions job. Args: request (Request): The FastAPI request object. This argument is ignored but included for potential future use. Returns: dict: A dictionary containing the version information, with key \"version\" and value as the short version of the git hash of the last commit. Examples: ```bash curl http://localhost:8000/heartbeat/ ``` Output: ```json { \"version\": \"abc123\" # The git hash short version } ``` Note: The `GIT_METADATA` environment variable must be set, usually during a GitHub Actions job, for this endpoint to return accurate version information. If run locally, the version will be set to \"local\". \"\"\" return { \"version\" : GIT_METADATA } predict Prediction Endpoint for Tilly Service This module contains a FastAPI router for serving an endpoint that performs predictions based on unscored timeslots. The data is processed, predicted, and then stored back into the database as scored timeslots. predict ( _ : Request , session : Session = Depends ( get_session ), model_registry : ModelRegistry = Depends ( get_current_registry )) -> dict [ str , str ] Initiatie prediction of room-specific ML models for all rooms in data source. The endpoint triggers a process to retrieve the unscored rooms, scores them using the designated room-specific machine learning model, and stores the scored data back into the database. Calls the prediction_flow function as a async background task. NOTE : Authentication is required for this endpoint. Args: request: The FastAPI request object. This argument is currently not used. session (Session, optional): SQLAlchemy session. Defaults to a new session from `get_session`. model_registry (ModelRegistry, optional): ML model registry. Defaults to the current model from `get_current_registry`. Returns: dict: A message indicating the completion status of the scoring sequence. Examples: curl -X POST http://localhost:8000/predict/ Output { \"message\" : \"Scoring sequence completed.\" } Source code in ai/tilly/routes/predict.py 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 @router . post ( \"/predict/\" ) def predict ( _ : Request , session : Session = Depends ( get_session ), model_registry : ModelRegistry = Depends ( get_current_registry ), ) -> dict [ str , str ]: \"\"\" Initiatie prediction of room-specific ML models for all rooms in data source. The endpoint triggers a process to retrieve the unscored rooms, scores them using the designated room-specific machine learning model, and stores the scored data back into the database. Calls the `prediction_flow` function as a async background task. **NOTE**: Authentication is required for this endpoint. Args: request: The FastAPI request object. This argument is currently not used. session (Session, optional): SQLAlchemy session. Defaults to a new session from `get_session`. model_registry (ModelRegistry, optional): ML model registry. Defaults to the current model from `get_current_registry`. Returns: dict: A message indicating the completion status of the scoring sequence. Examples: ```bash curl -X POST http://localhost:8000/predict/ ``` Output: ```json { \"message\": \"Scoring sequence completed.\" } ``` \"\"\" logger . debug ( \"Predict endpoint called\" ) if model_registry : prediction_flow ( session , model_registry ) response = \"Scoring sequence completed.\" else : response = \"No models available - please train first\" return { \"message\" : response } prediction_flow ( session : Session , model : ModelRegistry ) -> None Run the Prediction Workflow. This function orchestrates the steps for the prediction workflow, including data retrieval, prediction, and data storage. Parameters: Name Type Description Default session Session SQLAlchemy session to the Snowflake database. required model ModelRegistry Machine Learning model for performing the predictions. required Examples: from tilly.database.data.db import get_session from tilly.services.ml import get_current_registry with get_session () as session : model_registry = get_current_registry () prediction_flow ( session , model_registry ) Source code in ai/tilly/routes/predict.py 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 def prediction_flow ( session : Session , model : ModelRegistry ) -> None : \"\"\" Run the Prediction Workflow. This function orchestrates the steps for the prediction workflow, including data retrieval, prediction, and data storage. Args: session (Session): SQLAlchemy session to the Snowflake database. model (ModelRegistry): Machine Learning model for performing the predictions. Examples: ```python from tilly.database.data.db import get_session from tilly.services.ml import get_current_registry with get_session() as session: model_registry = get_current_registry() prediction_flow(session, model_registry) ``` \"\"\" rooms : dict [ str , DataFrame ] = crud . retrieve_data ( session , UnscoredTimeslots . __tablename__ ) scored_rooms : dict [ str , DataFrame ] = model . predict ( rooms ) combined_rooms : DataFrame = Transformer . combine_frames ( rooms , scored_rooms ) crud . push_data ( combined_rooms , table_name = ScoredTimeslots . __tablename__ , session = session ) train Machine Learning Training Endpoint This module contains a FastAPI router for initiating machine learning model training. The actual training is performed by the train_models function from the tilly.services.ml.trainer module and dashboard updates are performed by update_dashboard from tilly.services.dashboard . train ( _ : Request , background_tasks : BackgroundTasks , session : Session = Depends ( get_session )) Initiate training of room-specific ML models for all rooms in data source. Calls the training_flow function as a async background task. NOTE : Authentication is required for this endpoint. Args: request: Request: FastAPI Request object. Not used, but kept for FastAPI dependency injection. background_tasks: FastAPI BackgroundTasks for running functions in the background. session: SQLAlchemy Session object for database interactions (injected via FastAPI's dependency system). Returns: dict: A dictionary containing a message indicating that the training sequence has been initialized. Examples: curl -X POST http://localhost:8000/train This will initiate the training sequence and return: { \"message\" : \"Training sequence initialized\" } Source code in ai/tilly/routes/train.py 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 @router . post ( \"/train\" ) def train ( _ : Request , background_tasks : BackgroundTasks , session : Session = Depends ( get_session ), ): \"\"\" Initiate training of room-specific ML models for all rooms in data source. Calls the `training_flow` function as a async background task. **NOTE**: Authentication is required for this endpoint. Args: request: Request: FastAPI Request object. Not used, but kept for FastAPI dependency injection. background_tasks: FastAPI BackgroundTasks for running functions in the background. session: SQLAlchemy Session object for database interactions (injected via FastAPI's dependency system). Returns: dict: A dictionary containing a message indicating that the training sequence has been initialized. Examples: ```bash curl -X POST http://localhost:8000/train ``` This will initiate the training sequence and return: ```json { \"message\": \"Training sequence initialized\" } ``` \"\"\" background_tasks . add_task ( training_flow , session ) logger . info ( \"Training sequence initialized\" ) return { \"message\" : \"Training sequence initialized\" } training_flow ( session ) Initiates the Training Sequence. This function retrieves training data, trains machine learning models, and updates the dashboard based on the new models. Parameters: Name Type Description Default session Session SQLAlchemy session for database interactions. required Source code in ai/tilly/routes/train.py 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 def training_flow ( session ): \"\"\" Initiates the Training Sequence. This function retrieves training data, trains machine learning models, and updates the dashboard based on the new models. Args: session (Session): SQLAlchemy session for database interactions. \"\"\" training_data : dict [ str , DataFrame ] = crud . retrieve_data ( session , TrainingTimeslots . __tablename__ ) data_results : dict [ str , DataFrame ] = train_models ( training_data ) update_dashboard ( data_results )","title":"Routes"},{"location":"ai/reference/routes/#routes","text":"","title":"Routes"},{"location":"ai/reference/routes/#ai.tilly.routes","text":"","title":"routes"},{"location":"ai/reference/routes/#ai.tilly.routes.dashboard","text":"FastAPI Router for Tilly Dashboard and Plots This module contains routes for serving the Tilly dashboard and for retrieving the directory structure of the plots. It uses FastAPI and depends on the Jinja2 templating engine to render HTML responses.","title":"dashboard"},{"location":"ai/reference/routes/#ai.tilly.routes.dashboard.create_plot_structure","text":"Generate Plot Directory Structure. This function takes a root path and returns a dictionary representing the directory structure rooted at that path. Parameters: Name Type Description Default root_path Path The root path from which to generate the directory structure. required Returns: Type Description Optional [ Dict ] Optional[Dict]: The dictionary representing the directory structure or None if the provided path is not a directory. Examples: from pathlib import Path root_path = Path ( \"/path/to/plots\" ) structure = create_plot_structure ( root_path ) Source code in ai/tilly/routes/dashboard.py 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 def create_plot_structure ( root_path : Path ) -> Optional [ Dict ]: \"\"\" Generate Plot Directory Structure. This function takes a root path and returns a dictionary representing the directory structure rooted at that path. Args: root_path (Path): The root path from which to generate the directory structure. Returns: Optional[Dict]: The dictionary representing the directory structure or `None` if the provided path is not a directory. Examples: ```python from pathlib import Path root_path = Path(\"/path/to/plots\") structure = create_plot_structure(root_path) ``` \"\"\" if not root_path . is_dir (): return None return { child . name : create_plot_structure ( child ) for child in root_path . iterdir ()}","title":"create_plot_structure()"},{"location":"ai/reference/routes/#ai.tilly.routes.dashboard.get_plots_structure","text":"Get Plot Directory Structure. This route returns the directory structure of the plots as a JSON object. If the directory structure is invalid, a 404 HTTP error is raised. Returns: Type Description Optional [ Dict ] | HTTPException Optional[Dict]: The dictionary representing the directory structure. Examples: curl http://localhost:8000/plots_structure This will return a JSON object representing the directory structure of the plots. Source code in ai/tilly/routes/dashboard.py 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 @router . get ( \"/plots_structure\" , response_model = Optional [ Dict ]) def get_plots_structure () -> Optional [ Dict ] | HTTPException : \"\"\" Get Plot Directory Structure. This route returns the directory structure of the plots as a JSON object. If the directory structure is invalid, a 404 HTTP error is raised. Returns: Optional[Dict]: The dictionary representing the directory structure. Examples: ```bash curl http://localhost:8000/plots_structure ``` This will return a JSON object representing the directory structure of the plots. \"\"\" plot_dir = Path ( c . PLOTS_DIR ) result = create_plot_structure ( plot_dir ) if result is None : raise HTTPException ( status_code = 404 , detail = \"Invalid directory structure\" ) return result","title":"get_plots_structure()"},{"location":"ai/reference/routes/#ai.tilly.routes.dashboard.read_root","text":"Serve the Tilly Dashboard. This route returns an HTML response that serves the Tilly dashboard. Parameters: Name Type Description Default request Request The FastAPI request object. required Returns: Name Type Description HTMLResponse HTMLResponse The HTML response containing the rendered Tilly dashboard. Examples: curl http://localhost:8000/ This will return the HTML content of the Tilly dashboard. Source code in ai/tilly/routes/dashboard.py 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 @router . get ( \"/\" , response_class = HTMLResponse ) def read_root ( request : Request ) -> HTMLResponse : \"\"\" Serve the Tilly Dashboard. This route returns an HTML response that serves the Tilly dashboard. Args: request (Request): The FastAPI request object. Returns: HTMLResponse: The HTML response containing the rendered Tilly dashboard. Examples: ```bash curl http://localhost:8000/ ``` This will return the HTML content of the Tilly dashboard. \"\"\" return templates . TemplateResponse ( \"dashboard.html\" , { \"request\" : request })","title":"read_root()"},{"location":"ai/reference/routes/#ai.tilly.routes.heartbeat","text":"Heartbeat Endpoint for Tilly Service This module contains a FastAPI router for serving a heartbeat endpoint. The endpoint returns the current version of the service based on the GIT metadata.","title":"heartbeat"},{"location":"ai/reference/routes/#ai.tilly.routes.heartbeat.heartbeat","text":"Heartbeat Endpoint for Health Check and Versioning. This route returns a JSON object containing the current version of the service. The version information is retrieved from an environment variable, GIT_METADATA , which is set during a GitHub Actions job. Args: request (Request): The FastAPI request object. This argument is ignored but included for potential future use. Returns: dict: A dictionary containing the version information, with key \"version\" and value as the short version of the git hash of the last commit. Examples: ```bash curl http://localhost:8000/heartbeat/ ``` Output: ```json { \"version\": \"abc123\" # The git hash short version } ``` Note: The `GIT_METADATA` environment variable must be set, usually during a GitHub Actions job, for this endpoint to return accurate version information. If run locally, the version will be set to \"local\". Source code in ai/tilly/routes/heartbeat.py 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 @router . get ( \"/heartbeat/\" ) def heartbeat ( _ : Request ) -> dict : \"\"\" Heartbeat Endpoint for Health Check and Versioning. This route returns a JSON object containing the current version of the service. The version information is retrieved from an environment variable, `GIT_METADATA`, which is set during a GitHub Actions job. Args: request (Request): The FastAPI request object. This argument is ignored but included for potential future use. Returns: dict: A dictionary containing the version information, with key \"version\" and value as the short version of the git hash of the last commit. Examples: ```bash curl http://localhost:8000/heartbeat/ ``` Output: ```json { \"version\": \"abc123\" # The git hash short version } ``` Note: The `GIT_METADATA` environment variable must be set, usually during a GitHub Actions job, for this endpoint to return accurate version information. If run locally, the version will be set to \"local\". \"\"\" return { \"version\" : GIT_METADATA }","title":"heartbeat()"},{"location":"ai/reference/routes/#ai.tilly.routes.predict","text":"Prediction Endpoint for Tilly Service This module contains a FastAPI router for serving an endpoint that performs predictions based on unscored timeslots. The data is processed, predicted, and then stored back into the database as scored timeslots.","title":"predict"},{"location":"ai/reference/routes/#ai.tilly.routes.predict.predict","text":"Initiatie prediction of room-specific ML models for all rooms in data source. The endpoint triggers a process to retrieve the unscored rooms, scores them using the designated room-specific machine learning model, and stores the scored data back into the database. Calls the prediction_flow function as a async background task. NOTE : Authentication is required for this endpoint. Args: request: The FastAPI request object. This argument is currently not used. session (Session, optional): SQLAlchemy session. Defaults to a new session from `get_session`. model_registry (ModelRegistry, optional): ML model registry. Defaults to the current model from `get_current_registry`. Returns: dict: A message indicating the completion status of the scoring sequence. Examples: curl -X POST http://localhost:8000/predict/ Output { \"message\" : \"Scoring sequence completed.\" } Source code in ai/tilly/routes/predict.py 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 @router . post ( \"/predict/\" ) def predict ( _ : Request , session : Session = Depends ( get_session ), model_registry : ModelRegistry = Depends ( get_current_registry ), ) -> dict [ str , str ]: \"\"\" Initiatie prediction of room-specific ML models for all rooms in data source. The endpoint triggers a process to retrieve the unscored rooms, scores them using the designated room-specific machine learning model, and stores the scored data back into the database. Calls the `prediction_flow` function as a async background task. **NOTE**: Authentication is required for this endpoint. Args: request: The FastAPI request object. This argument is currently not used. session (Session, optional): SQLAlchemy session. Defaults to a new session from `get_session`. model_registry (ModelRegistry, optional): ML model registry. Defaults to the current model from `get_current_registry`. Returns: dict: A message indicating the completion status of the scoring sequence. Examples: ```bash curl -X POST http://localhost:8000/predict/ ``` Output: ```json { \"message\": \"Scoring sequence completed.\" } ``` \"\"\" logger . debug ( \"Predict endpoint called\" ) if model_registry : prediction_flow ( session , model_registry ) response = \"Scoring sequence completed.\" else : response = \"No models available - please train first\" return { \"message\" : response }","title":"predict()"},{"location":"ai/reference/routes/#ai.tilly.routes.predict.prediction_flow","text":"Run the Prediction Workflow. This function orchestrates the steps for the prediction workflow, including data retrieval, prediction, and data storage. Parameters: Name Type Description Default session Session SQLAlchemy session to the Snowflake database. required model ModelRegistry Machine Learning model for performing the predictions. required Examples: from tilly.database.data.db import get_session from tilly.services.ml import get_current_registry with get_session () as session : model_registry = get_current_registry () prediction_flow ( session , model_registry ) Source code in ai/tilly/routes/predict.py 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 def prediction_flow ( session : Session , model : ModelRegistry ) -> None : \"\"\" Run the Prediction Workflow. This function orchestrates the steps for the prediction workflow, including data retrieval, prediction, and data storage. Args: session (Session): SQLAlchemy session to the Snowflake database. model (ModelRegistry): Machine Learning model for performing the predictions. Examples: ```python from tilly.database.data.db import get_session from tilly.services.ml import get_current_registry with get_session() as session: model_registry = get_current_registry() prediction_flow(session, model_registry) ``` \"\"\" rooms : dict [ str , DataFrame ] = crud . retrieve_data ( session , UnscoredTimeslots . __tablename__ ) scored_rooms : dict [ str , DataFrame ] = model . predict ( rooms ) combined_rooms : DataFrame = Transformer . combine_frames ( rooms , scored_rooms ) crud . push_data ( combined_rooms , table_name = ScoredTimeslots . __tablename__ , session = session )","title":"prediction_flow()"},{"location":"ai/reference/routes/#ai.tilly.routes.train","text":"Machine Learning Training Endpoint This module contains a FastAPI router for initiating machine learning model training. The actual training is performed by the train_models function from the tilly.services.ml.trainer module and dashboard updates are performed by update_dashboard from tilly.services.dashboard .","title":"train"},{"location":"ai/reference/routes/#ai.tilly.routes.train.train","text":"Initiate training of room-specific ML models for all rooms in data source. Calls the training_flow function as a async background task. NOTE : Authentication is required for this endpoint. Args: request: Request: FastAPI Request object. Not used, but kept for FastAPI dependency injection. background_tasks: FastAPI BackgroundTasks for running functions in the background. session: SQLAlchemy Session object for database interactions (injected via FastAPI's dependency system). Returns: dict: A dictionary containing a message indicating that the training sequence has been initialized. Examples: curl -X POST http://localhost:8000/train This will initiate the training sequence and return: { \"message\" : \"Training sequence initialized\" } Source code in ai/tilly/routes/train.py 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 @router . post ( \"/train\" ) def train ( _ : Request , background_tasks : BackgroundTasks , session : Session = Depends ( get_session ), ): \"\"\" Initiate training of room-specific ML models for all rooms in data source. Calls the `training_flow` function as a async background task. **NOTE**: Authentication is required for this endpoint. Args: request: Request: FastAPI Request object. Not used, but kept for FastAPI dependency injection. background_tasks: FastAPI BackgroundTasks for running functions in the background. session: SQLAlchemy Session object for database interactions (injected via FastAPI's dependency system). Returns: dict: A dictionary containing a message indicating that the training sequence has been initialized. Examples: ```bash curl -X POST http://localhost:8000/train ``` This will initiate the training sequence and return: ```json { \"message\": \"Training sequence initialized\" } ``` \"\"\" background_tasks . add_task ( training_flow , session ) logger . info ( \"Training sequence initialized\" ) return { \"message\" : \"Training sequence initialized\" }","title":"train()"},{"location":"ai/reference/routes/#ai.tilly.routes.train.training_flow","text":"Initiates the Training Sequence. This function retrieves training data, trains machine learning models, and updates the dashboard based on the new models. Parameters: Name Type Description Default session Session SQLAlchemy session for database interactions. required Source code in ai/tilly/routes/train.py 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 def training_flow ( session ): \"\"\" Initiates the Training Sequence. This function retrieves training data, trains machine learning models, and updates the dashboard based on the new models. Args: session (Session): SQLAlchemy session for database interactions. \"\"\" training_data : dict [ str , DataFrame ] = crud . retrieve_data ( session , TrainingTimeslots . __tablename__ ) data_results : dict [ str , DataFrame ] = train_models ( training_data ) update_dashboard ( data_results )","title":"training_flow()"},{"location":"ai/reference/services/","text":"Services ai.tilly.services Machine Learning Services for Tilly This package provides a comprehensive suite of tools for managing room-specific machine learning models in Tilly. It includes functionalities for preprocessing, training, and postprocessing, as well as maintaining a global model registry. Main Components ModelRegistry: A singleton class that serves as an in-memory registry for trained machine learning models. Each model is room-specific and is used for batch predictions. This class also takes care of model training, fitting, and predictions. Model: A wrapper class around the scikit-learn Isolation Forest model. Each instance of this class represents a trained model for a specific room, and it provides functionalities to fit new data, make predictions, and calculate anomaly scores. Transformations: A collection of modules for pre- and post-processing steps. This includes feature extraction, normalization, and any other transformations required before and after model training or predictions. Trainer: A script responsible for orchestrating the model training flow. It takes in new data, triggers model training, and updates the global model registry. The package is designed to be robust, scalable, and thread-safe. Examples: >>> from tilly.services.ml import ModelRegistry >>> registry = ModelRegistry () >>> training_data = { ... } >>> registry . train ( training_data ) >>> from tilly.services.ml import train_models >>> train_models ( training_data ) dashboard Dashboard Updater Module This module contains functions for processing room data, generating plots, and updating the dashboard by saving the plots to disk. create_dir ( municipality : str , school : str ) -> None Create Municipality/School Directories Create the directories for storing plots related to a specific municipality and school if they do not already exist. Parameters: Name Type Description Default municipality str The name of the municipality. required school str The name of the school. required Source code in ai/tilly/services/dashboard/__init__.py 22 23 24 25 26 27 28 29 30 31 32 33 def create_dir ( municipality : str , school : str ) -> None : \"\"\" Create Municipality/School Directories Create the directories for storing plots related to a specific municipality and school if they do not already exist. Args: municipality (str): The name of the municipality. school (str): The name of the school. \"\"\" Path ( f \" { PLOTS_DIR } / { municipality } / { school } \" ) . mkdir ( parents = True , exist_ok = True ) process_for_dashboard ( rooms : dict [ str , pd . DataFrame ]) -> dict [ str , pd . DataFrame ] Process Room Data for Dashboard This function takes room data, processes it, and generates plotly figures. It also creates necessary directories for storing the plots. Parameters: Name Type Description Default rooms dict [ str , DataFrame ] Dictionary containing room data as Pandas DataFrames. required Returns: Type Description dict [ str , DataFrame ] dict[str, pd.DataFrame]: A dictionary containing processed plotly figures. Source code in ai/tilly/services/dashboard/__init__.py 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 def process_for_dashboard ( rooms : dict [ str , pd . DataFrame ]) -> dict [ str , pd . DataFrame ]: \"\"\" Process Room Data for Dashboard This function takes room data, processes it, and generates plotly figures. It also creates necessary directories for storing the plots. Args: rooms (dict[str, pd.DataFrame]): Dictionary containing room data as Pandas DataFrames. Returns: dict[str, pd.DataFrame]: A dictionary containing processed plotly figures. \"\"\" output = {} for _ , room in rooms . items (): # retrive the first instance of 'KOMMUNE' and 'SKOLE' municipality = room [ \"KOMMUNE\" ] . iloc [ 0 ] school = room [ \"SKOLE\" ] . iloc [ 0 ] room_id = room [ \"ID\" ] . iloc [ 0 ] . replace ( \".\" , \"_\" ) create_dir ( municipality , school ) fig = room . sort_values ( \"DATETIME\" , ascending = True ) . plot . bar ( x = \"DATETIME\" , y = \"CO2\" , color = \"IN_USE\" , title = ( f \"Lokale { room_id } \" + f \"- { school } ( { municipality } KOMMUNE)\" ), width = 2000 , hover_data = room [ FEATURES + [ \"ANOMALY_SCORE\" ]], ) # Update bar border width fig . update_traces ( dict ( marker_line_width = 0 )) # Update legend position fig . update_layout ( legend = dict ( yanchor = \"top\" , y = 0.99 , xanchor = \"left\" , x = 0.01 , title = None , ) ) output [( municipality , school , room_id )] = fig return output save_figures ( named_plots : dict [ tuple [ str , str , str ], object ]) -> None Save Figures to Disk Takes a dictionary of named plots and saves them to disk. Parameters: Name Type Description Default named_plots dict A dictionary containing plotly figures keyed by a tuple representing (municipality, school, room_id). required Source code in ai/tilly/services/dashboard/__init__.py 84 85 86 87 88 89 90 91 92 93 94 95 96 def save_figures ( named_plots : dict [ tuple [ str , str , str ], object ]) -> None : \"\"\" Save Figures to Disk Takes a dictionary of named plots and saves them to disk. Args: named_plots (dict): A dictionary containing plotly figures keyed by a tuple representing (municipality, school, room_id). \"\"\" for ( municipality , school , room_id ), fig in named_plots . items (): fig . write_html ( f \" { PLOTS_DIR } / { municipality } / { school } / { room_id } .html\" ) update_dashboard ( plot_data : dict [ str , pd . DataFrame ]) -> None Update Dashboard Update the dashboard by processing the provided room data and saving the generated plots. Parameters: Name Type Description Default plot_data dict [ str , DataFrame ] dict containing room data as required Side Effects Directories for storing plots may be created. Plot files may be written to disk. Source code in ai/tilly/services/dashboard/__init__.py 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 def update_dashboard ( plot_data : dict [ str , pd . DataFrame ]) -> None : \"\"\" Update Dashboard Update the dashboard by processing the provided room data and saving the generated plots. Args: plot_data (dict[str, pd.DataFrame]): dict containing room data as Pandas DataFrames, indexed by room name. Side Effects: - Directories for storing plots may be created. - Plot files may be written to disk. \"\"\" named_plots : dict [ tuple ( str , str , str ), object ] = process_for_dashboard ( plot_data ) save_figures ( named_plots ) ml THis is a test! model Model Source code in ai/tilly/services/ml/model.py 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 class Model : def __init__ ( self , estimated_usage : str | float = \"auto\" , model_params = MODEL_PARAMS ) -> None : \"\"\"Initializes the Model instance with the given parameters. Args: estimated_usage (str | float, optional): The contamination factor for the IsolationForest model. Defaults to \"auto\". model_params (dict, optional): Additional parameters for the IsolationForest model. Defaults to MODEL_PARAMS. Returns: None \"\"\" self . model = IsolationForest ( contamination = estimated_usage , ** model_params , ) def fit ( self , X : DataFrame ) -> \"Model\" : \"\"\"Fits the model with the given features. Args: X (DataFrame): The feature matrix to train on. Returns: Model: The trained model instance. \"\"\" self . model . fit ( X ) return self def predict ( self , X : DataFrame ) -> list [ float ]: \"\"\"Predicts whether each data point is anomalous or not. Returns 1 if the point is an outlier, and 0 otherwise. Args: X (DataFrame): The feature matrix to predict on. Returns: list[float]: A list of prediction results. \"\"\" y_hats = self . model . predict ( X ) return [ 1 if y_hat == - 1 else 0 for y_hat in y_hats ] def score ( self , X : DataFrame ) -> list [ float ]: \"\"\"Calculates and returns the normalized anomaly scores for each data point. Args: X (DataFrame): The feature matrix to score. Returns: list[float]: A list of normalized anomaly scores. \"\"\" y_hat = self . model . decision_function ( X ) return 1 - interp ( y_hat , ( min ( y_hat ), max ( y_hat )), ( 0 , 1 )) __init__ ( estimated_usage : str | float = 'auto' , model_params = MODEL_PARAMS ) -> None Initializes the Model instance with the given parameters. Parameters: Name Type Description Default estimated_usage str | float The contamination factor for the IsolationForest model. Defaults to \"auto\". 'auto' model_params dict Additional parameters for the IsolationForest model. Defaults to MODEL_PARAMS. MODEL_PARAMS Returns: Type Description None None Source code in ai/tilly/services/ml/model.py 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 def __init__ ( self , estimated_usage : str | float = \"auto\" , model_params = MODEL_PARAMS ) -> None : \"\"\"Initializes the Model instance with the given parameters. Args: estimated_usage (str | float, optional): The contamination factor for the IsolationForest model. Defaults to \"auto\". model_params (dict, optional): Additional parameters for the IsolationForest model. Defaults to MODEL_PARAMS. Returns: None \"\"\" self . model = IsolationForest ( contamination = estimated_usage , ** model_params , ) fit ( X : DataFrame ) -> Model Fits the model with the given features. Parameters: Name Type Description Default X DataFrame The feature matrix to train on. required Returns: Name Type Description Model Model The trained model instance. Source code in ai/tilly/services/ml/model.py 27 28 29 30 31 32 33 34 35 36 37 def fit ( self , X : DataFrame ) -> \"Model\" : \"\"\"Fits the model with the given features. Args: X (DataFrame): The feature matrix to train on. Returns: Model: The trained model instance. \"\"\" self . model . fit ( X ) return self predict ( X : DataFrame ) -> list [ float ] Predicts whether each data point is anomalous or not. Returns 1 if the point is an outlier, and 0 otherwise. Parameters: Name Type Description Default X DataFrame The feature matrix to predict on. required Returns: Type Description list [ float ] list[float]: A list of prediction results. Source code in ai/tilly/services/ml/model.py 39 40 41 42 43 44 45 46 47 48 49 50 51 def predict ( self , X : DataFrame ) -> list [ float ]: \"\"\"Predicts whether each data point is anomalous or not. Returns 1 if the point is an outlier, and 0 otherwise. Args: X (DataFrame): The feature matrix to predict on. Returns: list[float]: A list of prediction results. \"\"\" y_hats = self . model . predict ( X ) return [ 1 if y_hat == - 1 else 0 for y_hat in y_hats ] score ( X : DataFrame ) -> list [ float ] Calculates and returns the normalized anomaly scores for each data point. Parameters: Name Type Description Default X DataFrame The feature matrix to score. required Returns: Type Description list [ float ] list[float]: A list of normalized anomaly scores. Source code in ai/tilly/services/ml/model.py 53 54 55 56 57 58 59 60 61 62 63 def score ( self , X : DataFrame ) -> list [ float ]: \"\"\"Calculates and returns the normalized anomaly scores for each data point. Args: X (DataFrame): The feature matrix to score. Returns: list[float]: A list of normalized anomaly scores. \"\"\" y_hat = self . model . decision_function ( X ) return 1 - interp ( y_hat , ( min ( y_hat ), max ( y_hat )), ( 0 , 1 )) model_registry Model Registry Module This module manages the model registry for machine learning models in the Tilly system. It contains utilities for training, predicting, and handling models that are specific to each room. The ModelRegistry is a singleton class that holds a dictionary of trained models, ensuring only one instance exists across the application. Modules: Name Description - update_registry Function to update the global model registry. - get_current_registry Function to fetch the current model registry. - ModelRegistry Singleton class to manage room-specific models. ModelRegistry A singleton class representing the model registry. This class is responsible for training, fitting, predicting, and managing each model. It holds a dictionary of models that are specific to each room. Attributes: Name Type Description - models (Dict[str, Model] A dictionary holding room-specific machine learning models. Methods: Name Description - train Train models based on new timeslot data. - fit_predict Train and predict on new room data. - predict Make predictions using pre-trained models. - preprocess Preprocesses the input data for each room. - postprocess Postprocesses the predicted data for each room. Source code in ai/tilly/services/ml/model_registry.py 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 class ModelRegistry : \"\"\"A singleton class representing the model registry. This class is responsible for training, fitting, predicting, and managing each model. It holds a dictionary of models that are specific to each room. Attributes: - models (Dict[str, Model]): A dictionary holding room-specific machine learning models. Methods: - train: Train models based on new timeslot data. - fit_predict: Train and predict on new room data. - predict: Make predictions using pre-trained models. - preprocess: Preprocesses the input data for each room. - postprocess: Postprocesses the predicted data for each room. \"\"\" _instance = None _lock = Lock () def __new__ ( cls ): \"\"\"Ensures that only one instance of the class exists.\"\"\" with cls . _lock : if cls . _instance is None : cls . _instance = super ( ModelRegistry , cls ) . __new__ ( cls ) return cls . _instance def __init__ ( self ): \"\"\"Initializes an empty model dictionary.\"\"\" self . models : Dict [ str , Model ] = {} def train ( self , timeslots : dict [ str , DataFrame ]) -> None : \"\"\"Train models based on new timeslot data and store them in the registry. Args: timeslots (dict[str, DataFrame]): The timeslots data to train on, per room. \"\"\" _preprocessed : dict [ str , DataFrame ] = self . preprocess ( timeslots ) room_results : dict [ str , DataFrame ] = self . fit_predict ( _preprocessed ) _postprocessed : dict [ str , DataFrame ] = self . postprocess ( room_results ) return _postprocessed def fit_predict ( self , rooms : dict [ str , DataFrame ]) -> Dict [ str , DataFrame ]: \"\"\"Train and predict on new room data. Args: rooms (dict[str, DataFrame]): Room data to fit and predict on. Returns: Dict[str, DataFrame]: The predicted DataFrame for each room. \"\"\" output = {} with tqdm ( total = len ( rooms ), desc = \"Initial\" ) as pbar : for name , timeslots in rooms . items (): pbar . set_postfix_str ( f \"Running fit_predict | Room: { name } \" ) pbar . update ( 1 ) if not timeslots . empty : features = timeslots [ FEATURES ] # extract features model = Model ( estimated_usage = 0.3 ) . fit ( X = features ) # fit model self . models [ name ] = model # add model to registry # make predictions output [ name ]: DataFrame = self . _predict ( name , timeslots ) return output def predict ( self , rooms : dict [ str , DataFrame ]) -> dict [ str , DataFrame ]: \"\"\"Make predictions using pre-trained models in the registry. Args: rooms (dict[str, DataFrame]): Room data to predict on. Returns: dict[str, DataFrame]: The predicted DataFrame for each room. \"\"\" _preprocessed : dict [ str , DataFrame ] = self . preprocess ( rooms ) _predictions = { name : self . _predict ( name , room ) for name , room in tqdm ( _preprocessed . items ()) if not room . empty } return self . postprocess ( _predictions ) def _predict ( self , name : str , room : DataFrame ) -> DataFrame : \"\"\" Make predictions for a specific room using its corresponding model in the registry. Args: name (str): The name of the room. room (DataFrame): The room data to predict on. Returns: DataFrame: The predicted DataFrame for the room. \"\"\" # extract features features = room [ FEATURES ] # load model from registry if model := self . models . get ( name ): # extract scores and predictions scores : list [ float ] = model . score ( features ) preds : list [ int ] = model . predict ( features ) else : scores , preds = T . handle_missing_model ( room_name = name , room = room , models = self . models . keys () ) return room . assign ( ANOMALY_SCORE = scores , IN_USE = preds , ) def preprocess ( self , timeslots : dict [ str , DataFrame ]) -> dict [ str , DataFrame ]: \"\"\" Preprocesses the input timeslot data for each room. Args: timeslots (dict[str, DataFrame]): The timeslot data for each room. Returns: dict[str, DataFrame]: The preprocessed DataFrame for each room. \"\"\" logger . info ( \"Preprocessing data...\" ) return { name : room . pipe ( T . featurize ) for name , room in tqdm ( timeslots . items ())} def postprocess ( self , predictions : dict [ str , DataFrame ]) -> dict [ str , DataFrame ]: \"\"\"Postprocesses the prediction results for each room. Args: predictions (dict[str, DataFrame]): The predicted DataFrame for each room. Returns: dict[str, DataFrame]: The postprocessed DataFrame for each room. \"\"\" logger . info ( \"Postprocessing data...\" ) return { name : room . pipe ( T . heuristics ) for name , room in tqdm ( predictions . items ()) } __init__ () Initializes an empty model dictionary. Source code in ai/tilly/services/ml/model_registry.py 60 61 62 def __init__ ( self ): \"\"\"Initializes an empty model dictionary.\"\"\" self . models : Dict [ str , Model ] = {} __new__ () Ensures that only one instance of the class exists. Source code in ai/tilly/services/ml/model_registry.py 53 54 55 56 57 58 def __new__ ( cls ): \"\"\"Ensures that only one instance of the class exists.\"\"\" with cls . _lock : if cls . _instance is None : cls . _instance = super ( ModelRegistry , cls ) . __new__ ( cls ) return cls . _instance fit_predict ( rooms : dict [ str , DataFrame ]) -> Dict [ str , DataFrame ] Train and predict on new room data. Parameters: Name Type Description Default rooms dict [ str , DataFrame ] Room data to fit and predict on. required Returns: Type Description Dict [ str , DataFrame ] Dict[str, DataFrame]: The predicted DataFrame for each room. Source code in ai/tilly/services/ml/model_registry.py 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 def fit_predict ( self , rooms : dict [ str , DataFrame ]) -> Dict [ str , DataFrame ]: \"\"\"Train and predict on new room data. Args: rooms (dict[str, DataFrame]): Room data to fit and predict on. Returns: Dict[str, DataFrame]: The predicted DataFrame for each room. \"\"\" output = {} with tqdm ( total = len ( rooms ), desc = \"Initial\" ) as pbar : for name , timeslots in rooms . items (): pbar . set_postfix_str ( f \"Running fit_predict | Room: { name } \" ) pbar . update ( 1 ) if not timeslots . empty : features = timeslots [ FEATURES ] # extract features model = Model ( estimated_usage = 0.3 ) . fit ( X = features ) # fit model self . models [ name ] = model # add model to registry # make predictions output [ name ]: DataFrame = self . _predict ( name , timeslots ) return output postprocess ( predictions : dict [ str , DataFrame ]) -> dict [ str , DataFrame ] Postprocesses the prediction results for each room. Parameters: Name Type Description Default predictions dict [ str , DataFrame ] The predicted DataFrame for each room. required Returns: Type Description dict [ str , DataFrame ] dict[str, DataFrame]: The postprocessed DataFrame for each room. Source code in ai/tilly/services/ml/model_registry.py 165 166 167 168 169 170 171 172 173 174 175 176 177 def postprocess ( self , predictions : dict [ str , DataFrame ]) -> dict [ str , DataFrame ]: \"\"\"Postprocesses the prediction results for each room. Args: predictions (dict[str, DataFrame]): The predicted DataFrame for each room. Returns: dict[str, DataFrame]: The postprocessed DataFrame for each room. \"\"\" logger . info ( \"Postprocessing data...\" ) return { name : room . pipe ( T . heuristics ) for name , room in tqdm ( predictions . items ()) } predict ( rooms : dict [ str , DataFrame ]) -> dict [ str , DataFrame ] Make predictions using pre-trained models in the registry. Parameters: Name Type Description Default rooms dict [ str , DataFrame ] Room data to predict on. required Returns: Type Description dict [ str , DataFrame ] dict[str, DataFrame]: The predicted DataFrame for each room. Source code in ai/tilly/services/ml/model_registry.py 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 def predict ( self , rooms : dict [ str , DataFrame ]) -> dict [ str , DataFrame ]: \"\"\"Make predictions using pre-trained models in the registry. Args: rooms (dict[str, DataFrame]): Room data to predict on. Returns: dict[str, DataFrame]: The predicted DataFrame for each room. \"\"\" _preprocessed : dict [ str , DataFrame ] = self . preprocess ( rooms ) _predictions = { name : self . _predict ( name , room ) for name , room in tqdm ( _preprocessed . items ()) if not room . empty } return self . postprocess ( _predictions ) preprocess ( timeslots : dict [ str , DataFrame ]) -> dict [ str , DataFrame ] Preprocesses the input timeslot data for each room. Parameters: Name Type Description Default timeslots dict [ str , DataFrame ] The timeslot data for each room. required Returns: Type Description dict [ str , DataFrame ] dict[str, DataFrame]: The preprocessed DataFrame for each room. Source code in ai/tilly/services/ml/model_registry.py 152 153 154 155 156 157 158 159 160 161 162 163 def preprocess ( self , timeslots : dict [ str , DataFrame ]) -> dict [ str , DataFrame ]: \"\"\" Preprocesses the input timeslot data for each room. Args: timeslots (dict[str, DataFrame]): The timeslot data for each room. Returns: dict[str, DataFrame]: The preprocessed DataFrame for each room. \"\"\" logger . info ( \"Preprocessing data...\" ) return { name : room . pipe ( T . featurize ) for name , room in tqdm ( timeslots . items ())} train ( timeslots : dict [ str , DataFrame ]) -> None Train models based on new timeslot data and store them in the registry. Parameters: Name Type Description Default timeslots dict [ str , DataFrame ] The timeslots data to train on, per room. required Source code in ai/tilly/services/ml/model_registry.py 64 65 66 67 68 69 70 71 72 73 74 75 def train ( self , timeslots : dict [ str , DataFrame ]) -> None : \"\"\"Train models based on new timeslot data and store them in the registry. Args: timeslots (dict[str, DataFrame]): The timeslots data to train on, per room. \"\"\" _preprocessed : dict [ str , DataFrame ] = self . preprocess ( timeslots ) room_results : dict [ str , DataFrame ] = self . fit_predict ( _preprocessed ) _postprocessed : dict [ str , DataFrame ] = self . postprocess ( room_results ) return _postprocessed get_current_registry () -> ModelRegistry Fetches the current model registry. Returns: Type Description ModelRegistry The current model registry instance. Source code in ai/tilly/services/ml/model_registry.py 199 200 201 202 203 204 205 def get_current_registry () -> ModelRegistry : \"\"\"Fetches the current model registry. Returns: The current model registry instance. \"\"\" return current_registry update_registry ( new_registry : ModelRegistry ) Updates the global model registry. Parameters: Name Type Description Default new_registry ModelRegistry The new model registry to set as global. required Source code in ai/tilly/services/ml/model_registry.py 189 190 191 192 193 194 195 196 def update_registry ( new_registry : ModelRegistry ): \"\"\"Updates the global model registry. Args: new_registry: The new model registry to set as global. \"\"\" global current_registry current_registry = new_registry trainer train_models ( training_data : dict [ str , DataFrame ]) -> list [ dict [ str , DataFrame ]] Trains new models based on the given training data and updates the global model registry. This function performs the following steps: 1. Create a new instance of ModelRegistry. 2. Train the models using the training data. 3. Update the global model registry with the newly trained models. Parameters: Name Type Description Default training_data dict [ str , DataFrame ] A dictionary containing the training data for each room, keyed by room name. required Returns: Type Description list [ dict [ str , DataFrame ]] list[dict[str, DataFrame]]: A list of dictionaries, each containing the list [ dict [ str , DataFrame ]] predicted DataFrame and anomaly scores for each room. Source code in ai/tilly/services/ml/trainer.py 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 def train_models ( training_data : dict [ str , DataFrame ]) -> list [ dict [ str , DataFrame ]]: \"\"\"Trains new models based on the given training data and updates the global model registry. This function performs the following steps: 1. Create a new instance of ModelRegistry. 2. Train the models using the training data. 3. Update the global model registry with the newly trained models. Args: training_data (dict[str, DataFrame]): A dictionary containing the training data for each room, keyed by room name. Returns: list[dict[str, DataFrame]]: A list of dictionaries, each containing the predicted DataFrame and anomaly scores for each room. \"\"\" # Create a new model registry model_registry = ModelRegistry () # Train models and receive the results results : dict [ str , tuple [ list [ float ], list [ int ]]] = model_registry . train ( training_data ) # Update the global model registry update_registry ( model_registry ) logger . info ( \"Training flow completed\" ) return results transformations ML Transformations Module Initializer This initializer script in the tilly/services/ml/transformations folder imports and combines the Preprocessor and Postprocessor classes into a single Transformer class, inheriting the methods and attributes of both. Transformer Bases: Preprocessor , Postprocessor Transformer Class This class inherits from both Preprocessor and Postprocessor classes, effectively combining their functionalities. Instances of this class can be used for both preprocessing and postprocessing steps in a machine learning pipeline. Inherits Preprocessor : Class containing methods and attributes for data preprocessing. Postprocessor : Class containing methods and attributes for data postprocessing. Source code in ai/tilly/services/ml/transformations/__init__.py 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 class Transformer ( Preprocessor , Postprocessor ): \"\"\" Transformer Class This class inherits from both `Preprocessor` and `Postprocessor` classes, effectively combining their functionalities. Instances of this class can be used for both preprocessing and postprocessing steps in a machine learning pipeline. Inherits: - `Preprocessor`: Class containing methods and attributes for data preprocessing. - `Postprocessor`: Class containing methods and attributes for data postprocessing. \"\"\" pass postprocessing Postprocessor Postprocessor for handling and enhancing model output data. This class contains a series of static methods intended for postprocessing the output of predictive models. These methods apply heuristic rules to DataFrames containing room usage information and anomaly scores, handle cases where predictive models are missing, and combine original and scored data into enriched DataFrames. Methods heuristics(cls, room: pd.DataFrame) -> pd.DataFrame: Applies heuristic rules to modify the predicted data in a DataFrame representing room usage. It has the following rules: - Night Time Filtering: Filters out false positives during midnight to 6 AM. - Stand-Alone Instances: Removes isolated instances of \"IN_USE\" being 1. - Low CO2 Levels: Sets \"IN_USE\" to 0 if CO2 levels are low. The method also updates the \"ANOMALY_SCORE\" based on the modified \"IN_USE\" values. handle_missing_model(room_name: str, room: pd.DataFrame, models: list[str]) -> tuple[list[np.nan], list[np.nan]]: Returns null values for anomaly scores and predictions if the predictive model for a room is missing. It also logs a warning message about the missing model. combine_frames(original: dict[str, pd.DataFrame], scored: dict[str, pd.DataFrame], merge_cols: list = [\"DATE\", \"TIME\", \"ID\", \"KOMMUNE\", \"SKOLE\"]) -> pd.DataFrame: Merges the original data and the scored data for each room into a single DataFrame. If a room does not have corresponding scored data, it will be filled with null values in the 'IN_USE' and 'ANOMALY_SCORE' columns. Examples df = pd.DataFrame({ ... 'DATETIME': pd.date_range(start='2022-01-01', periods=4, freq='15T'), ... 'ANOMALY_SCORE': [0.2, 0.8, 0.5, 0.7], ... 'IN_USE': [0, 1, 1, 0], ... 'CO2': [300, 400, 500, 200] ... }) processed_df = Postprocessor.heuristics(df) Source code in ai/tilly/services/ml/transformations/postprocessing.py 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 @make_all_methods_static class Postprocessor : \"\"\" Postprocessor for handling and enhancing model output data. This class contains a series of static methods intended for postprocessing the output of predictive models. These methods apply heuristic rules to DataFrames containing room usage information and anomaly scores, handle cases where predictive models are missing, and combine original and scored data into enriched DataFrames. Methods ------- heuristics(cls, room: pd.DataFrame) -> pd.DataFrame: Applies heuristic rules to modify the predicted data in a DataFrame representing room usage. It has the following rules: - Night Time Filtering: Filters out false positives during midnight to 6 AM. - Stand-Alone Instances: Removes isolated instances of \"IN_USE\" being 1. - Low CO2 Levels: Sets \"IN_USE\" to 0 if CO2 levels are low. The method also updates the \"ANOMALY_SCORE\" based on the modified \"IN_USE\" values. handle_missing_model(room_name: str, room: pd.DataFrame, models: list[str]) -> tuple[list[np.nan], list[np.nan]]: Returns null values for anomaly scores and predictions if the predictive model for a room is missing. It also logs a warning message about the missing model. combine_frames(original: dict[str, pd.DataFrame], scored: dict[str, pd.DataFrame], merge_cols: list = [\"DATE\", \"TIME\", \"ID\", \"KOMMUNE\", \"SKOLE\"]) -> pd.DataFrame: Merges the original data and the scored data for each room into a single DataFrame. If a room does not have corresponding scored data, it will be filled with null values in the 'IN_USE' and 'ANOMALY_SCORE' columns. Examples -------- >>> df = pd.DataFrame({ ... 'DATETIME': pd.date_range(start='2022-01-01', periods=4, freq='15T'), ... 'ANOMALY_SCORE': [0.2, 0.8, 0.5, 0.7], ... 'IN_USE': [0, 1, 1, 0], ... 'CO2': [300, 400, 500, 200] ... }) >>> processed_df = Postprocessor.heuristics(df) \"\"\" @classmethod def heuristics ( cls , room : pd . DataFrame ) -> pd . DataFrame : \"\"\" Apply heuristic rules to modify the predicted data in a room DataFrame. This class method applies a series of heuristic rules to a DataFrame containing predicted room usage and anomaly scores. The rules are applied in the following order: 1. Night Time Filtering: If the time is between midnight and 6 AM and the anomaly score is less than or equal to 0.7, set \"IN_USE\" to 0. 2. Stand-Alone Instances: If an instance of \"IN_USE\" being 1 is surrounded by instances of \"IN_USE\" being 0, set that isolated \"IN_USE\" to 0. 3. Low CO2 Levels: If the CO2 level is less than or equal to 325, set \"IN_USE\" to 0. The anomaly score is then updated based on the modified \"IN_USE\" values. Parameters ---------- cls : class The class to which this class method belongs. room : pd.DataFrame Input DataFrame containing at least the following columns: - \"DATETIME\": Timestamps for each 15-minute interval. - \"ANOMALY_SCORE\": Anomaly scores ranging from 0 to 1. - \"IN_USE\": Binary values indicating room usage (1 for in use, 0 for not in use). - \"CO2\": CO2 levels. Returns ------- pd.DataFrame Modified DataFrame after applying the heuristic rules. Examples -------- >>> df = pd.DataFrame({ ... 'DATETIME': pd.date_range(start='2022-01-01', periods=4, freq='15T'), ... 'ANOMALY_SCORE': [0.2, 0.8, 0.5, 0.7], ... 'IN_USE': [0, 1, 1, 0], ... 'CO2': [300, 400, 500, 200] ... }) >>> Postprocessor.heuristics(df) \"\"\" def apply_night_time_filter ( df ): \"\"\"Filters out false positives during midnight to 6 AM.\"\"\" hour = df [ \"DATETIME\" ] . dt . hour mask = ( hour >= 0 ) & ( hour < 6 ) & ( df [ \"ANOMALY_SCORE\" ] <= 0.7 ) df . loc [ mask , \"IN_USE\" ] = 0 return df def apply_stand_alone_instances_filter ( df ): \"\"\"Removes isolated instances of \"IN_USE\" being 1.\"\"\" prev_IN_USE = df [ \"IN_USE\" ] . shift ( 1 , fill_value = 0 ) next_IN_USE = df [ \"IN_USE\" ] . shift ( - 1 , fill_value = 0 ) mask = ( prev_IN_USE == 0 ) & ( df [ \"IN_USE\" ] == 1 ) & ( next_IN_USE == 0 ) df . loc [ mask , \"IN_USE\" ] = 0 return df def apply_low_co2_filter ( df ): \"\"\"Sets \"IN_USE\" to 0 if CO2 levels are low.\"\"\" mask = df [ \"CO2\" ] <= 325 df . loc [ mask , \"IN_USE\" ] = 0 return df def update_anomaly_score ( df ): \"\"\"Updates the anomaly score based on the modified \"IN_USE\" values.\"\"\" mask = (( df [ \"IN_USE\" ] == 1 ) & ( df [ \"ANOMALY_SCORE\" ] < 0.5 )) | ( ( df [ \"IN_USE\" ] == 0 ) & ( df [ \"ANOMALY_SCORE\" ] > 0.5 ) ) df . loc [ mask , \"ANOMALY_SCORE\" ] = 1 - df . loc [ mask , \"ANOMALY_SCORE\" ] return df return ( room . pipe ( apply_night_time_filter ) . pipe ( apply_stand_alone_instances_filter ) . pipe ( apply_low_co2_filter ) . pipe ( update_anomaly_score ) ) def handle_missing_model ( room_name : str , room : pd . DataFrame , models : list [ str ] ) -> tuple [ list [ np . nan ], list [ np . nan ]]: \"\"\"Returns null values for scores and predictions if a model is missing. Also, logs a warning message. Args: room_name (str): Room name room (pd.DataFrame): Room data models (dict[str, object]): Available models Returns: tuple[array[np.nan], array[np.nan]]: Scores and predictions \"\"\" n_rows = room . shape [ 0 ] school = room_name . split ( \"_\" )[ 0 ] logger . warning ( f \"Model for { room_name } ( { school } ) not found. Returning null values. \\n \" + f \"Available models in registry for { school } : \\n \" + f \" { [ key for key in models if school in key ] } \" ) return ( np . full ( n_rows , np . nan ), np . full ( n_rows , np . nan )) def combine_frames ( original : dict [ str , pd . DataFrame ], scored : dict [ str , pd . DataFrame ], merge_cols : list = [ \"DATE\" , \"TIME\" , \"ID\" , \"KOMMUNE\" , \"SKOLE\" ], ) -> pd . DataFrame : \"\"\" Combines original and scored data into a single pd.DataFrame. The function takes two dictionaries, where the keys represent room identifiers and the values are Pandas DataFrames containing the original and scored data. It enriches the original data with the anomaly scores and 'IN_USE' indicators from the scored data. If a room does not have corresponding scored data, it will be filled with null values in the 'IN_USE' and 'ANOMALY_SCORE' columns. Args: - original (dict[str, pd.DataFrame]): A dictionary containing the original data. The keys are room identifiers, and the values are pd.DataFrames with the original data. - scored (dict[str, pd.DataFrame]): A dictionary containing the scored data. The keys are room identifiers, and the values are pd.DataFrames with the anomaly scores and 'IN_USE' indicators. - merge_cols (list): The list of column names to use for merging the original and scored data. Returns: - pd.DataFrame: A concatenated DataFrame containing all the enriched data. \"\"\" combined_frames = [] for key , orig_df in original . items (): scored_df = scored . get ( key , None ) if scored_df is not None : # Merge based on multiple common columns combined_df = orig_df . merge ( scored_df [[ \"ANOMALY_SCORE\" , \"IN_USE\" ] + merge_cols ], how = \"left\" , on = merge_cols , ) else : # If there's no corresponding scored pd.dataframe, # copy the original pd.dataframe and add null columns # for 'IN_USE' and 'ANOMALY_SCORE' combined_df = orig_df . copy () combined_df [ \"ANOMALY_SCORE\" ] = None combined_df [ \"IN_USE\" ] = None # Add an identifier for the original key (i.e., room name) combined_df [ \"ROOM\" ] = key combined_frames . append ( combined_df ) # Concatenate all frames vertically return pd . concat ( combined_frames , ignore_index = True ) combine_frames ( original : dict [ str , pd . DataFrame ], scored : dict [ str , pd . DataFrame ], merge_cols : list = [ 'DATE' , 'TIME' , 'ID' , 'KOMMUNE' , 'SKOLE' ]) -> pd . DataFrame Combines original and scored data into a single pd.DataFrame. The function takes two dictionaries, where the keys represent room identifiers and the values are Pandas DataFrames containing the original and scored data. It enriches the original data with the anomaly scores and 'IN_USE' indicators from the scored data. If a room does not have corresponding scored data, it will be filled with null values in the 'IN_USE' and 'ANOMALY_SCORE' columns. Args: - original (dict[str, pd.DataFrame]): A dictionary containing the original data. The keys are room identifiers, and the values are pd.DataFrames with the original data. - scored (dict[str, pd.DataFrame]): A dictionary containing the scored data. The keys are room identifiers, and the values are pd.DataFrames with the anomaly scores and 'IN_USE' indicators. - merge_cols (list): The list of column names to use for merging the original and scored data. Returns: - pd.DataFrame: A concatenated DataFrame containing all the enriched data. Source code in ai/tilly/services/ml/transformations/postprocessing.py 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 def combine_frames ( original : dict [ str , pd . DataFrame ], scored : dict [ str , pd . DataFrame ], merge_cols : list = [ \"DATE\" , \"TIME\" , \"ID\" , \"KOMMUNE\" , \"SKOLE\" ], ) -> pd . DataFrame : \"\"\" Combines original and scored data into a single pd.DataFrame. The function takes two dictionaries, where the keys represent room identifiers and the values are Pandas DataFrames containing the original and scored data. It enriches the original data with the anomaly scores and 'IN_USE' indicators from the scored data. If a room does not have corresponding scored data, it will be filled with null values in the 'IN_USE' and 'ANOMALY_SCORE' columns. Args: - original (dict[str, pd.DataFrame]): A dictionary containing the original data. The keys are room identifiers, and the values are pd.DataFrames with the original data. - scored (dict[str, pd.DataFrame]): A dictionary containing the scored data. The keys are room identifiers, and the values are pd.DataFrames with the anomaly scores and 'IN_USE' indicators. - merge_cols (list): The list of column names to use for merging the original and scored data. Returns: - pd.DataFrame: A concatenated DataFrame containing all the enriched data. \"\"\" combined_frames = [] for key , orig_df in original . items (): scored_df = scored . get ( key , None ) if scored_df is not None : # Merge based on multiple common columns combined_df = orig_df . merge ( scored_df [[ \"ANOMALY_SCORE\" , \"IN_USE\" ] + merge_cols ], how = \"left\" , on = merge_cols , ) else : # If there's no corresponding scored pd.dataframe, # copy the original pd.dataframe and add null columns # for 'IN_USE' and 'ANOMALY_SCORE' combined_df = orig_df . copy () combined_df [ \"ANOMALY_SCORE\" ] = None combined_df [ \"IN_USE\" ] = None # Add an identifier for the original key (i.e., room name) combined_df [ \"ROOM\" ] = key combined_frames . append ( combined_df ) # Concatenate all frames vertically return pd . concat ( combined_frames , ignore_index = True ) handle_missing_model ( room_name : str , room : pd . DataFrame , models : list [ str ]) -> tuple [ list [ np . nan ], list [ np . nan ]] Returns null values for scores and predictions if a model is missing. Also, logs a warning message. Parameters: Name Type Description Default room_name str Room name required room DataFrame Room data required models dict [ str , object ] Available models required Returns: Type Description tuple [ list [ nan ], list [ nan ]] tuple[array[np.nan], array[np.nan]]: Scores and predictions Source code in ai/tilly/services/ml/transformations/postprocessing.py 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 def handle_missing_model ( room_name : str , room : pd . DataFrame , models : list [ str ] ) -> tuple [ list [ np . nan ], list [ np . nan ]]: \"\"\"Returns null values for scores and predictions if a model is missing. Also, logs a warning message. Args: room_name (str): Room name room (pd.DataFrame): Room data models (dict[str, object]): Available models Returns: tuple[array[np.nan], array[np.nan]]: Scores and predictions \"\"\" n_rows = room . shape [ 0 ] school = room_name . split ( \"_\" )[ 0 ] logger . warning ( f \"Model for { room_name } ( { school } ) not found. Returning null values. \\n \" + f \"Available models in registry for { school } : \\n \" + f \" { [ key for key in models if school in key ] } \" ) return ( np . full ( n_rows , np . nan ), np . full ( n_rows , np . nan )) heuristics ( room : pd . DataFrame ) -> pd . DataFrame classmethod Apply heuristic rules to modify the predicted data in a room DataFrame. This class method applies a series of heuristic rules to a DataFrame containing predicted room usage and anomaly scores. The rules are applied in the following order: Night Time Filtering: If the time is between midnight and 6 AM and the anomaly score is less than or equal to 0.7, set \"IN_USE\" to 0. Stand-Alone Instances: If an instance of \"IN_USE\" being 1 is surrounded by instances of \"IN_USE\" being 0, set that isolated \"IN_USE\" to 0. Low CO2 Levels: If the CO2 level is less than or equal to 325, set \"IN_USE\" to 0. The anomaly score is then updated based on the modified \"IN_USE\" values. Parameters cls : class The class to which this class method belongs. room : pd.DataFrame Input DataFrame containing at least the following columns: - \"DATETIME\": Timestamps for each 15-minute interval. - \"ANOMALY_SCORE\": Anomaly scores ranging from 0 to 1. - \"IN_USE\": Binary values indicating room usage (1 for in use, 0 for not in use). - \"CO2\": CO2 levels. Returns pd.DataFrame Modified DataFrame after applying the heuristic rules. Examples df = pd.DataFrame({ ... 'DATETIME': pd.date_range(start='2022-01-01', periods=4, freq='15T'), ... 'ANOMALY_SCORE': [0.2, 0.8, 0.5, 0.7], ... 'IN_USE': [0, 1, 1, 0], ... 'CO2': [300, 400, 500, 200] ... }) Postprocessor.heuristics(df) Source code in ai/tilly/services/ml/transformations/postprocessing.py 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 @classmethod def heuristics ( cls , room : pd . DataFrame ) -> pd . DataFrame : \"\"\" Apply heuristic rules to modify the predicted data in a room DataFrame. This class method applies a series of heuristic rules to a DataFrame containing predicted room usage and anomaly scores. The rules are applied in the following order: 1. Night Time Filtering: If the time is between midnight and 6 AM and the anomaly score is less than or equal to 0.7, set \"IN_USE\" to 0. 2. Stand-Alone Instances: If an instance of \"IN_USE\" being 1 is surrounded by instances of \"IN_USE\" being 0, set that isolated \"IN_USE\" to 0. 3. Low CO2 Levels: If the CO2 level is less than or equal to 325, set \"IN_USE\" to 0. The anomaly score is then updated based on the modified \"IN_USE\" values. Parameters ---------- cls : class The class to which this class method belongs. room : pd.DataFrame Input DataFrame containing at least the following columns: - \"DATETIME\": Timestamps for each 15-minute interval. - \"ANOMALY_SCORE\": Anomaly scores ranging from 0 to 1. - \"IN_USE\": Binary values indicating room usage (1 for in use, 0 for not in use). - \"CO2\": CO2 levels. Returns ------- pd.DataFrame Modified DataFrame after applying the heuristic rules. Examples -------- >>> df = pd.DataFrame({ ... 'DATETIME': pd.date_range(start='2022-01-01', periods=4, freq='15T'), ... 'ANOMALY_SCORE': [0.2, 0.8, 0.5, 0.7], ... 'IN_USE': [0, 1, 1, 0], ... 'CO2': [300, 400, 500, 200] ... }) >>> Postprocessor.heuristics(df) \"\"\" def apply_night_time_filter ( df ): \"\"\"Filters out false positives during midnight to 6 AM.\"\"\" hour = df [ \"DATETIME\" ] . dt . hour mask = ( hour >= 0 ) & ( hour < 6 ) & ( df [ \"ANOMALY_SCORE\" ] <= 0.7 ) df . loc [ mask , \"IN_USE\" ] = 0 return df def apply_stand_alone_instances_filter ( df ): \"\"\"Removes isolated instances of \"IN_USE\" being 1.\"\"\" prev_IN_USE = df [ \"IN_USE\" ] . shift ( 1 , fill_value = 0 ) next_IN_USE = df [ \"IN_USE\" ] . shift ( - 1 , fill_value = 0 ) mask = ( prev_IN_USE == 0 ) & ( df [ \"IN_USE\" ] == 1 ) & ( next_IN_USE == 0 ) df . loc [ mask , \"IN_USE\" ] = 0 return df def apply_low_co2_filter ( df ): \"\"\"Sets \"IN_USE\" to 0 if CO2 levels are low.\"\"\" mask = df [ \"CO2\" ] <= 325 df . loc [ mask , \"IN_USE\" ] = 0 return df def update_anomaly_score ( df ): \"\"\"Updates the anomaly score based on the modified \"IN_USE\" values.\"\"\" mask = (( df [ \"IN_USE\" ] == 1 ) & ( df [ \"ANOMALY_SCORE\" ] < 0.5 )) | ( ( df [ \"IN_USE\" ] == 0 ) & ( df [ \"ANOMALY_SCORE\" ] > 0.5 ) ) df . loc [ mask , \"ANOMALY_SCORE\" ] = 1 - df . loc [ mask , \"ANOMALY_SCORE\" ] return df return ( room . pipe ( apply_night_time_filter ) . pipe ( apply_stand_alone_instances_filter ) . pipe ( apply_low_co2_filter ) . pipe ( update_anomaly_score ) ) make_all_methods_static ( cls ) Decorator to make all methods in a class static. Source code in ai/tilly/services/ml/transformations/postprocessing.py 6 7 8 9 10 11 def make_all_methods_static ( cls ): \"\"\"Decorator to make all methods in a class static.\"\"\" for attr_name , attr_value in cls . __dict__ . items (): if callable ( attr_value ): setattr ( cls , attr_name , staticmethod ( attr_value )) return cls preprocessing Preprocessor A class that contains all the preprocessing logic for the model input Source code in ai/tilly/services/ml/transformations/preprocessing.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 class Preprocessor : \"\"\"A class that contains all the preprocessing logic for the model input\"\"\" @classmethod def estimate_usage ( cls , data : pd . DataFrame , usage_coeff = 2.1 , usage_min = 0.1 , usage_max = 0.4 ) -> float : \"\"\"Estimates the usage of a given room from the union of - booked timeslots: Timeslots within a registered booking, and - Scheduled timeslots: TImeslots within the school schema. This is used as a prior for our anomaly detection model. Args: data (pd.DataFrame): Timeslots usage_coeff (float, optional): Usage coefficient. Defaults to 2.1. This is a heuristic measure used to scale the estimated usage usage_min (float, optional): Minimum usage. Defaults to 0.1. This is a heuristic lower bound for the net estimated usage of a room. usage_max (float, optional): Maximum usage. Defaults to 0.4. This is a heuristic upper bound for the net estimated usage of a room. \"\"\" try : used_slots = ( data [ \"SKEMALAGT\" ] | data [ \"BOOKET\" ] . fillna ( False )) . sum () used_slots = max ( used_slots , usage_min ) return min ( usage_coeff * used_slots / len ( data ), usage_max ) except ZeroDivisionError : return \"auto\" @classmethod @log_pipeline def merge_dt ( cls , df , date , time , name , sep = \" \" ): return df . assign ( ** { name : lambda d : pd . to_datetime ( d [ date ] . astype ( str ) + sep + d [ time ] . astype ( str ) ) } ) @classmethod @log_pipeline def add_missing_timeslots ( cls , df : pd . DataFrame , freq : str = \"15T\" ) -> pd . DataFrame : \"\"\"Adds the rows that are missing from the DataFrame, by merging it with a DataFrame containing all the timeslots.\"\"\" static_values = ( df . head ( 1 )[[ \"ID\" , \"KOMMUNE\" , \"SKOLE\" , \"SKOLE_ID\" ]] . squeeze () . to_dict () ) return ( pd . DataFrame ( { \"DATETIME\" : pd . date_range ( start = df [ \"DATETIME\" ] . min (), end = df [ \"DATETIME\" ] . max (), freq = freq ) } ) . assign ( ** static_values ) . merge ( df , on = [ \"DATETIME\" , \"ID\" , \"KOMMUNE\" , \"SKOLE\" , \"SKOLE_ID\" ], how = \"left\" ) ) @classmethod @log_pipeline def interpolate_missing_islands ( cls , df : pd . DataFrame , * , target_col : str = \"CO2\" , limit : int = 3 , direction : str = \"forward\" , method : str = \"cubic\" , ** kwargs , ) -> pd . DataFrame : \"\"\"Interpolate missing values in a dataframe, but only for islands of missing values, ie. rows where there are no more than `limit` consecutive missing values in the `target_col` column.\"\"\" try : return df . assign ( CO2 = lambda d : d [ target_col ] . interpolate ( method = method , limit = limit , limit_direction = direction , ** kwargs , ) ) except ValueError : logger . warning ( f \"[ { df . iloc [ 0 ][ 'SKOLE_ID' ] } ] \" + \"Interpolation failed, falling back to linear\" ) return df . assign ( CO2 = lambda d : d [ target_col ] . interpolate ( method = \"linear\" , limit = limit , limit_direction = direction , ** kwargs , ) ) @classmethod @log_pipeline def remove_stagnate_intervals ( cls , df , target_col : str = \"CO2\" , threshold = 4 ) -> pd . DataFrame : \"\"\"Remove intervals where the CO2 value is the same for consecutive rows within time-contiguous blocks\"\"\" return ( df . assign ( time_diff = lambda d : d [ \"DATETIME\" ] . diff (), new_block = lambda d : ( d [ \"time_diff\" ] > pd . Timedelta ( minutes = 15 )) | ( d [ target_col ] != d [ target_col ] . shift ( 1 )), block_id = lambda d : d [ \"new_block\" ] . cumsum (), ) . assign ( block_count = lambda d : d . groupby ( \"block_id\" )[ \"block_id\" ] . transform ( \"count\" ) )[ lambda d : d [ \"block_count\" ] . lt ( threshold )] . drop ([ \"time_diff\" , \"new_block\" , \"block_id\" , \"block_count\" ], axis = 1 ) ) @classmethod @log_pipeline def drop_outliers ( cls , df , bounds : dict [ str , tuple [ float | None , float | None ]] ) -> pd . DataFrame : \"\"\"Drop rows where values are outside the given bounds. Args: df (pd.DataFrame): DataFrame to filter bounds (dict[str, tuple[float | None, float | None]]): Dictionary of column names and their lower and upper bounds. If a bound is None, then it is not applied. Returns: df (pd.DataFrame): Filtered DataFrame \"\"\" mask = np . ones ( df . shape [ 0 ], dtype = bool ) for col_name , ( lo , hi ) in bounds . items (): if lo is not None and hi is not None : mask &= ( df [ col_name ] . values >= lo ) & ( df [ col_name ] . values <= hi ) elif lo is not None : mask &= df [ col_name ] . values >= lo elif hi is not None : mask &= df [ col_name ] . values <= hi else : raise ValueError ( f \"Bounds for { col_name } are both None\" ) return df [ mask ] @classmethod @log_pipeline def day_filter ( cls , df : pd . DataFrame , * , min_ratio : float = 0.25 ) -> pd . DataFrame : \"\"\"Filter out days with too few data points (days with less than min_ratio of the data points). This is done to avoid overfitting on days with too few data points,where the kinematic quantities are not calculated correctly. Min ratio is multiplied by 4*24 to get the number of data points required for a day (4*24 is the number of 15 minute intervals in a day) Args: df (pd.DataFrame): DataFrame to filter min_ratio (float, optional): Minimum ratio of data points required for a day. Defaults to 0.25. Returns: df (pd.DataFrame): Filtered DataFrame \"\"\" min_data_points_required = int ( min_ratio * ( 4 * 24 )) return df . groupby ( \"DATE\" ) . filter ( lambda x : len ( x ) >= min_data_points_required ) @classmethod def calculate_kinematic_quantities ( cls , df , metric , * , window , prefix = None ) -> pd . DataFrame : \"\"\"Add rolling velocity, acceleration, and jerk for a metric. The rolling quantities are calculated using the gradient of the metric and the given window size. Null values are filled with zeros.\"\"\" if prefix is None : prefix = metric # Calculate the rolling window mean for the metric rolling_metric = df [ metric ] . rolling ( window = window ) . mean () # First order derivative (velocity) df [ f \" { prefix } _velocity\" ] = rolling_metric . diff () # Second order derivative (acceleration) df [ f \" { prefix } _acceleration\" ] = df [ f \" { prefix } _velocity\" ] . diff () # Third order derivative (jerk) df [ f \" { prefix } _jerk\" ] = df [ f \" { prefix } _acceleration\" ] . diff () # Log of the metric df [ f \" { prefix } _log\" ] = np . log ( df [ metric ] . fillna ( 1 ) + 1 ) # Fill NAs fill_cols = [ f \" { prefix } _velocity\" , f \" { prefix } _acceleration\" , f \" { prefix } _jerk\" ] df [ fill_cols ] = df [ fill_cols ] . fillna ( 0 ) return df @classmethod def gaussian_smooth ( cls , df , metric , * , std_dev = 2 ): \"\"\"Apply a gaussian filter to a given metric\"\"\" df [ f \" { metric } _smoothed\" ] = gaussian_filter1d ( df [ metric ], sigma = std_dev ) return df @classmethod @log_pipeline def apply_time_group_funcs ( cls , df , funcs ) -> pd . DataFrame : \"\"\"Apply a list of functions to each time-contiguous block of data in the DataFrame\"\"\" # Calculate all the needed columns in one go df [ \"time_diff\" ] = df [ \"DATETIME\" ] . diff () df [ \"new_block\" ] = df [ \"time_diff\" ] > pd . Timedelta ( minutes = 15 ) df [ \"block_id\" ] = df [ \"new_block\" ] . cumsum () for func , kwargs in funcs : df = df . groupby ( \"block_id\" ) . apply ( func , ** kwargs ) . reset_index ( drop = True ) df . drop ( columns = [ \"block_id\" , \"new_block\" , \"time_diff\" ], inplace = True ) return df @classmethod @log_pipeline def add_time_features ( cls , df , * , night_start = 23 , night_end = 6 ): \"\"\"Add time features to the DataFrame\"\"\" return df . assign ( is_night = lambda d : ( ( d [ \"DATETIME\" ] . dt . hour >= night_start ) & ( d [ \"DATETIME\" ] . dt . hour <= night_end ) ) . astype ( int ), ) @classmethod @log_pipeline def featurize ( cls , df : pd . DataFrame ) -> pd . DataFrame : \"\"\"Run the full preprocessing flow on a DataFrame\"\"\" return ( df . pipe ( cls . merge_dt , date = \"DATE\" , time = \"TIME\" , name = \"DATETIME\" ) . pipe ( cls . add_missing_timeslots ) . pipe ( cls . interpolate_missing_islands , target_col = \"CO2\" , limit = 4 ) . pipe ( cls . remove_stagnate_intervals , target_col = \"CO2\" , threshold = 5 ) . dropna ( subset = [ \"CO2\" ]) . pipe ( cls . drop_outliers , bounds = { \"CO2\" : ( 1 , 8000 )}) . pipe ( cls . day_filter , min_ratio = 0.25 ) . pipe ( cls . apply_time_group_funcs , funcs = [ ( cls . gaussian_smooth , dict ( metric = \"CO2\" , std_dev = 2 )), ( cls . calculate_kinematic_quantities , dict ( metric = \"CO2_smoothed\" , window = 4 , prefix = \"CO2\" ), ), ], ) . pipe ( cls . add_time_features , night_start = 22 , night_end = 6 ) ) add_missing_timeslots ( df : pd . DataFrame , freq : str = '15T' ) -> pd . DataFrame classmethod Adds the rows that are missing from the DataFrame, by merging it with a DataFrame containing all the timeslots. Source code in ai/tilly/services/ml/transformations/preprocessing.py 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 @classmethod @log_pipeline def add_missing_timeslots ( cls , df : pd . DataFrame , freq : str = \"15T\" ) -> pd . DataFrame : \"\"\"Adds the rows that are missing from the DataFrame, by merging it with a DataFrame containing all the timeslots.\"\"\" static_values = ( df . head ( 1 )[[ \"ID\" , \"KOMMUNE\" , \"SKOLE\" , \"SKOLE_ID\" ]] . squeeze () . to_dict () ) return ( pd . DataFrame ( { \"DATETIME\" : pd . date_range ( start = df [ \"DATETIME\" ] . min (), end = df [ \"DATETIME\" ] . max (), freq = freq ) } ) . assign ( ** static_values ) . merge ( df , on = [ \"DATETIME\" , \"ID\" , \"KOMMUNE\" , \"SKOLE\" , \"SKOLE_ID\" ], how = \"left\" ) ) add_time_features ( df , * , night_start = 23 , night_end = 6 ) classmethod Add time features to the DataFrame Source code in ai/tilly/services/ml/transformations/preprocessing.py 245 246 247 248 249 250 251 252 253 254 @classmethod @log_pipeline def add_time_features ( cls , df , * , night_start = 23 , night_end = 6 ): \"\"\"Add time features to the DataFrame\"\"\" return df . assign ( is_night = lambda d : ( ( d [ \"DATETIME\" ] . dt . hour >= night_start ) & ( d [ \"DATETIME\" ] . dt . hour <= night_end ) ) . astype ( int ), ) apply_time_group_funcs ( df , funcs ) -> pd . DataFrame classmethod Apply a list of functions to each time-contiguous block of data in the DataFrame Source code in ai/tilly/services/ml/transformations/preprocessing.py 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 @classmethod @log_pipeline def apply_time_group_funcs ( cls , df , funcs ) -> pd . DataFrame : \"\"\"Apply a list of functions to each time-contiguous block of data in the DataFrame\"\"\" # Calculate all the needed columns in one go df [ \"time_diff\" ] = df [ \"DATETIME\" ] . diff () df [ \"new_block\" ] = df [ \"time_diff\" ] > pd . Timedelta ( minutes = 15 ) df [ \"block_id\" ] = df [ \"new_block\" ] . cumsum () for func , kwargs in funcs : df = df . groupby ( \"block_id\" ) . apply ( func , ** kwargs ) . reset_index ( drop = True ) df . drop ( columns = [ \"block_id\" , \"new_block\" , \"time_diff\" ], inplace = True ) return df calculate_kinematic_quantities ( df , metric , * , window , prefix = None ) -> pd . DataFrame classmethod Add rolling velocity, acceleration, and jerk for a metric. The rolling quantities are calculated using the gradient of the metric and the given window size. Null values are filled with zeros. Source code in ai/tilly/services/ml/transformations/preprocessing.py 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 @classmethod def calculate_kinematic_quantities ( cls , df , metric , * , window , prefix = None ) -> pd . DataFrame : \"\"\"Add rolling velocity, acceleration, and jerk for a metric. The rolling quantities are calculated using the gradient of the metric and the given window size. Null values are filled with zeros.\"\"\" if prefix is None : prefix = metric # Calculate the rolling window mean for the metric rolling_metric = df [ metric ] . rolling ( window = window ) . mean () # First order derivative (velocity) df [ f \" { prefix } _velocity\" ] = rolling_metric . diff () # Second order derivative (acceleration) df [ f \" { prefix } _acceleration\" ] = df [ f \" { prefix } _velocity\" ] . diff () # Third order derivative (jerk) df [ f \" { prefix } _jerk\" ] = df [ f \" { prefix } _acceleration\" ] . diff () # Log of the metric df [ f \" { prefix } _log\" ] = np . log ( df [ metric ] . fillna ( 1 ) + 1 ) # Fill NAs fill_cols = [ f \" { prefix } _velocity\" , f \" { prefix } _acceleration\" , f \" { prefix } _jerk\" ] df [ fill_cols ] = df [ fill_cols ] . fillna ( 0 ) return df day_filter ( df : pd . DataFrame , * , min_ratio : float = 0.25 ) -> pd . DataFrame classmethod Filter out days with too few data points (days with less than min_ratio of the data points). This is done to avoid overfitting on days with too few data points,where the kinematic quantities are not calculated correctly. Min ratio is multiplied by 4 24 to get the number of data points required for a day (4 24 is the number of 15 minute intervals in a day) Parameters: Name Type Description Default df DataFrame DataFrame to filter required min_ratio float Minimum ratio of data points required for a day. Defaults to 0.25. 0.25 Returns: Name Type Description df DataFrame Filtered DataFrame Source code in ai/tilly/services/ml/transformations/preprocessing.py 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 @classmethod @log_pipeline def day_filter ( cls , df : pd . DataFrame , * , min_ratio : float = 0.25 ) -> pd . DataFrame : \"\"\"Filter out days with too few data points (days with less than min_ratio of the data points). This is done to avoid overfitting on days with too few data points,where the kinematic quantities are not calculated correctly. Min ratio is multiplied by 4*24 to get the number of data points required for a day (4*24 is the number of 15 minute intervals in a day) Args: df (pd.DataFrame): DataFrame to filter min_ratio (float, optional): Minimum ratio of data points required for a day. Defaults to 0.25. Returns: df (pd.DataFrame): Filtered DataFrame \"\"\" min_data_points_required = int ( min_ratio * ( 4 * 24 )) return df . groupby ( \"DATE\" ) . filter ( lambda x : len ( x ) >= min_data_points_required ) drop_outliers ( df , bounds : dict [ str , tuple [ float | None , float | None ]]) -> pd . DataFrame classmethod Drop rows where values are outside the given bounds. Parameters: Name Type Description Default df DataFrame DataFrame to filter required bounds dict [ str , tuple [ float | None, float | None]] Dictionary of column names and their lower and upper bounds. If a bound is None, then it is not applied. required Returns: Name Type Description df DataFrame Filtered DataFrame Source code in ai/tilly/services/ml/transformations/preprocessing.py 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 @classmethod @log_pipeline def drop_outliers ( cls , df , bounds : dict [ str , tuple [ float | None , float | None ]] ) -> pd . DataFrame : \"\"\"Drop rows where values are outside the given bounds. Args: df (pd.DataFrame): DataFrame to filter bounds (dict[str, tuple[float | None, float | None]]): Dictionary of column names and their lower and upper bounds. If a bound is None, then it is not applied. Returns: df (pd.DataFrame): Filtered DataFrame \"\"\" mask = np . ones ( df . shape [ 0 ], dtype = bool ) for col_name , ( lo , hi ) in bounds . items (): if lo is not None and hi is not None : mask &= ( df [ col_name ] . values >= lo ) & ( df [ col_name ] . values <= hi ) elif lo is not None : mask &= df [ col_name ] . values >= lo elif hi is not None : mask &= df [ col_name ] . values <= hi else : raise ValueError ( f \"Bounds for { col_name } are both None\" ) return df [ mask ] estimate_usage ( data : pd . DataFrame , usage_coeff = 2.1 , usage_min = 0.1 , usage_max = 0.4 ) -> float classmethod Estimates the usage of a given room from the union of - booked timeslots: Timeslots within a registered booking, and - Scheduled timeslots: TImeslots within the school schema. This is used as a prior for our anomaly detection model. Parameters: Name Type Description Default data DataFrame Timeslots required usage_coeff float Usage coefficient. Defaults to 2.1. This is a heuristic measure used to scale the estimated usage 2.1 usage_min float Minimum usage. Defaults to 0.1. This is a heuristic lower bound for the net estimated usage of a room. 0.1 usage_max float Maximum usage. Defaults to 0.4. This is a heuristic upper bound for the net estimated usage of a room. 0.4 Source code in ai/tilly/services/ml/transformations/preprocessing.py 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 @classmethod def estimate_usage ( cls , data : pd . DataFrame , usage_coeff = 2.1 , usage_min = 0.1 , usage_max = 0.4 ) -> float : \"\"\"Estimates the usage of a given room from the union of - booked timeslots: Timeslots within a registered booking, and - Scheduled timeslots: TImeslots within the school schema. This is used as a prior for our anomaly detection model. Args: data (pd.DataFrame): Timeslots usage_coeff (float, optional): Usage coefficient. Defaults to 2.1. This is a heuristic measure used to scale the estimated usage usage_min (float, optional): Minimum usage. Defaults to 0.1. This is a heuristic lower bound for the net estimated usage of a room. usage_max (float, optional): Maximum usage. Defaults to 0.4. This is a heuristic upper bound for the net estimated usage of a room. \"\"\" try : used_slots = ( data [ \"SKEMALAGT\" ] | data [ \"BOOKET\" ] . fillna ( False )) . sum () used_slots = max ( used_slots , usage_min ) return min ( usage_coeff * used_slots / len ( data ), usage_max ) except ZeroDivisionError : return \"auto\" featurize ( df : pd . DataFrame ) -> pd . DataFrame classmethod Run the full preprocessing flow on a DataFrame Source code in ai/tilly/services/ml/transformations/preprocessing.py 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 @classmethod @log_pipeline def featurize ( cls , df : pd . DataFrame ) -> pd . DataFrame : \"\"\"Run the full preprocessing flow on a DataFrame\"\"\" return ( df . pipe ( cls . merge_dt , date = \"DATE\" , time = \"TIME\" , name = \"DATETIME\" ) . pipe ( cls . add_missing_timeslots ) . pipe ( cls . interpolate_missing_islands , target_col = \"CO2\" , limit = 4 ) . pipe ( cls . remove_stagnate_intervals , target_col = \"CO2\" , threshold = 5 ) . dropna ( subset = [ \"CO2\" ]) . pipe ( cls . drop_outliers , bounds = { \"CO2\" : ( 1 , 8000 )}) . pipe ( cls . day_filter , min_ratio = 0.25 ) . pipe ( cls . apply_time_group_funcs , funcs = [ ( cls . gaussian_smooth , dict ( metric = \"CO2\" , std_dev = 2 )), ( cls . calculate_kinematic_quantities , dict ( metric = \"CO2_smoothed\" , window = 4 , prefix = \"CO2\" ), ), ], ) . pipe ( cls . add_time_features , night_start = 22 , night_end = 6 ) ) gaussian_smooth ( df , metric , * , std_dev = 2 ) classmethod Apply a gaussian filter to a given metric Source code in ai/tilly/services/ml/transformations/preprocessing.py 223 224 225 226 227 @classmethod def gaussian_smooth ( cls , df , metric , * , std_dev = 2 ): \"\"\"Apply a gaussian filter to a given metric\"\"\" df [ f \" { metric } _smoothed\" ] = gaussian_filter1d ( df [ metric ], sigma = std_dev ) return df interpolate_missing_islands ( df : pd . DataFrame , * , target_col : str = 'CO2' , limit : int = 3 , direction : str = 'forward' , method : str = 'cubic' , ** kwargs ) -> pd . DataFrame classmethod Interpolate missing values in a dataframe, but only for islands of missing values, ie. rows where there are no more than limit consecutive missing values in the target_col column. Source code in ai/tilly/services/ml/transformations/preprocessing.py 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 @classmethod @log_pipeline def interpolate_missing_islands ( cls , df : pd . DataFrame , * , target_col : str = \"CO2\" , limit : int = 3 , direction : str = \"forward\" , method : str = \"cubic\" , ** kwargs , ) -> pd . DataFrame : \"\"\"Interpolate missing values in a dataframe, but only for islands of missing values, ie. rows where there are no more than `limit` consecutive missing values in the `target_col` column.\"\"\" try : return df . assign ( CO2 = lambda d : d [ target_col ] . interpolate ( method = method , limit = limit , limit_direction = direction , ** kwargs , ) ) except ValueError : logger . warning ( f \"[ { df . iloc [ 0 ][ 'SKOLE_ID' ] } ] \" + \"Interpolation failed, falling back to linear\" ) return df . assign ( CO2 = lambda d : d [ target_col ] . interpolate ( method = \"linear\" , limit = limit , limit_direction = direction , ** kwargs , ) ) remove_stagnate_intervals ( df , target_col : str = 'CO2' , threshold = 4 ) -> pd . DataFrame classmethod Remove intervals where the CO2 value is the same for consecutive rows within time-contiguous blocks Source code in ai/tilly/services/ml/transformations/preprocessing.py 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 @classmethod @log_pipeline def remove_stagnate_intervals ( cls , df , target_col : str = \"CO2\" , threshold = 4 ) -> pd . DataFrame : \"\"\"Remove intervals where the CO2 value is the same for consecutive rows within time-contiguous blocks\"\"\" return ( df . assign ( time_diff = lambda d : d [ \"DATETIME\" ] . diff (), new_block = lambda d : ( d [ \"time_diff\" ] > pd . Timedelta ( minutes = 15 )) | ( d [ target_col ] != d [ target_col ] . shift ( 1 )), block_id = lambda d : d [ \"new_block\" ] . cumsum (), ) . assign ( block_count = lambda d : d . groupby ( \"block_id\" )[ \"block_id\" ] . transform ( \"count\" ) )[ lambda d : d [ \"block_count\" ] . lt ( threshold )] . drop ([ \"time_diff\" , \"new_block\" , \"block_id\" , \"block_count\" ], axis = 1 ) )","title":"Services"},{"location":"ai/reference/services/#services","text":"","title":"Services"},{"location":"ai/reference/services/#ai.tilly.services","text":"Machine Learning Services for Tilly This package provides a comprehensive suite of tools for managing room-specific machine learning models in Tilly. It includes functionalities for preprocessing, training, and postprocessing, as well as maintaining a global model registry. Main Components ModelRegistry: A singleton class that serves as an in-memory registry for trained machine learning models. Each model is room-specific and is used for batch predictions. This class also takes care of model training, fitting, and predictions. Model: A wrapper class around the scikit-learn Isolation Forest model. Each instance of this class represents a trained model for a specific room, and it provides functionalities to fit new data, make predictions, and calculate anomaly scores. Transformations: A collection of modules for pre- and post-processing steps. This includes feature extraction, normalization, and any other transformations required before and after model training or predictions. Trainer: A script responsible for orchestrating the model training flow. It takes in new data, triggers model training, and updates the global model registry. The package is designed to be robust, scalable, and thread-safe. Examples: >>> from tilly.services.ml import ModelRegistry >>> registry = ModelRegistry () >>> training_data = { ... } >>> registry . train ( training_data ) >>> from tilly.services.ml import train_models >>> train_models ( training_data )","title":"services"},{"location":"ai/reference/services/#ai.tilly.services.dashboard","text":"Dashboard Updater Module This module contains functions for processing room data, generating plots, and updating the dashboard by saving the plots to disk.","title":"dashboard"},{"location":"ai/reference/services/#ai.tilly.services.dashboard.create_dir","text":"Create Municipality/School Directories Create the directories for storing plots related to a specific municipality and school if they do not already exist. Parameters: Name Type Description Default municipality str The name of the municipality. required school str The name of the school. required Source code in ai/tilly/services/dashboard/__init__.py 22 23 24 25 26 27 28 29 30 31 32 33 def create_dir ( municipality : str , school : str ) -> None : \"\"\" Create Municipality/School Directories Create the directories for storing plots related to a specific municipality and school if they do not already exist. Args: municipality (str): The name of the municipality. school (str): The name of the school. \"\"\" Path ( f \" { PLOTS_DIR } / { municipality } / { school } \" ) . mkdir ( parents = True , exist_ok = True )","title":"create_dir()"},{"location":"ai/reference/services/#ai.tilly.services.dashboard.process_for_dashboard","text":"Process Room Data for Dashboard This function takes room data, processes it, and generates plotly figures. It also creates necessary directories for storing the plots. Parameters: Name Type Description Default rooms dict [ str , DataFrame ] Dictionary containing room data as Pandas DataFrames. required Returns: Type Description dict [ str , DataFrame ] dict[str, pd.DataFrame]: A dictionary containing processed plotly figures. Source code in ai/tilly/services/dashboard/__init__.py 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 def process_for_dashboard ( rooms : dict [ str , pd . DataFrame ]) -> dict [ str , pd . DataFrame ]: \"\"\" Process Room Data for Dashboard This function takes room data, processes it, and generates plotly figures. It also creates necessary directories for storing the plots. Args: rooms (dict[str, pd.DataFrame]): Dictionary containing room data as Pandas DataFrames. Returns: dict[str, pd.DataFrame]: A dictionary containing processed plotly figures. \"\"\" output = {} for _ , room in rooms . items (): # retrive the first instance of 'KOMMUNE' and 'SKOLE' municipality = room [ \"KOMMUNE\" ] . iloc [ 0 ] school = room [ \"SKOLE\" ] . iloc [ 0 ] room_id = room [ \"ID\" ] . iloc [ 0 ] . replace ( \".\" , \"_\" ) create_dir ( municipality , school ) fig = room . sort_values ( \"DATETIME\" , ascending = True ) . plot . bar ( x = \"DATETIME\" , y = \"CO2\" , color = \"IN_USE\" , title = ( f \"Lokale { room_id } \" + f \"- { school } ( { municipality } KOMMUNE)\" ), width = 2000 , hover_data = room [ FEATURES + [ \"ANOMALY_SCORE\" ]], ) # Update bar border width fig . update_traces ( dict ( marker_line_width = 0 )) # Update legend position fig . update_layout ( legend = dict ( yanchor = \"top\" , y = 0.99 , xanchor = \"left\" , x = 0.01 , title = None , ) ) output [( municipality , school , room_id )] = fig return output","title":"process_for_dashboard()"},{"location":"ai/reference/services/#ai.tilly.services.dashboard.save_figures","text":"Save Figures to Disk Takes a dictionary of named plots and saves them to disk. Parameters: Name Type Description Default named_plots dict A dictionary containing plotly figures keyed by a tuple representing (municipality, school, room_id). required Source code in ai/tilly/services/dashboard/__init__.py 84 85 86 87 88 89 90 91 92 93 94 95 96 def save_figures ( named_plots : dict [ tuple [ str , str , str ], object ]) -> None : \"\"\" Save Figures to Disk Takes a dictionary of named plots and saves them to disk. Args: named_plots (dict): A dictionary containing plotly figures keyed by a tuple representing (municipality, school, room_id). \"\"\" for ( municipality , school , room_id ), fig in named_plots . items (): fig . write_html ( f \" { PLOTS_DIR } / { municipality } / { school } / { room_id } .html\" )","title":"save_figures()"},{"location":"ai/reference/services/#ai.tilly.services.dashboard.update_dashboard","text":"Update Dashboard Update the dashboard by processing the provided room data and saving the generated plots. Parameters: Name Type Description Default plot_data dict [ str , DataFrame ] dict containing room data as required Side Effects Directories for storing plots may be created. Plot files may be written to disk. Source code in ai/tilly/services/dashboard/__init__.py 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 def update_dashboard ( plot_data : dict [ str , pd . DataFrame ]) -> None : \"\"\" Update Dashboard Update the dashboard by processing the provided room data and saving the generated plots. Args: plot_data (dict[str, pd.DataFrame]): dict containing room data as Pandas DataFrames, indexed by room name. Side Effects: - Directories for storing plots may be created. - Plot files may be written to disk. \"\"\" named_plots : dict [ tuple ( str , str , str ), object ] = process_for_dashboard ( plot_data ) save_figures ( named_plots )","title":"update_dashboard()"},{"location":"ai/reference/services/#ai.tilly.services.ml","text":"THis is a test!","title":"ml"},{"location":"ai/reference/services/#ai.tilly.services.ml.model","text":"","title":"model"},{"location":"ai/reference/services/#ai.tilly.services.ml.model.Model","text":"Source code in ai/tilly/services/ml/model.py 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 class Model : def __init__ ( self , estimated_usage : str | float = \"auto\" , model_params = MODEL_PARAMS ) -> None : \"\"\"Initializes the Model instance with the given parameters. Args: estimated_usage (str | float, optional): The contamination factor for the IsolationForest model. Defaults to \"auto\". model_params (dict, optional): Additional parameters for the IsolationForest model. Defaults to MODEL_PARAMS. Returns: None \"\"\" self . model = IsolationForest ( contamination = estimated_usage , ** model_params , ) def fit ( self , X : DataFrame ) -> \"Model\" : \"\"\"Fits the model with the given features. Args: X (DataFrame): The feature matrix to train on. Returns: Model: The trained model instance. \"\"\" self . model . fit ( X ) return self def predict ( self , X : DataFrame ) -> list [ float ]: \"\"\"Predicts whether each data point is anomalous or not. Returns 1 if the point is an outlier, and 0 otherwise. Args: X (DataFrame): The feature matrix to predict on. Returns: list[float]: A list of prediction results. \"\"\" y_hats = self . model . predict ( X ) return [ 1 if y_hat == - 1 else 0 for y_hat in y_hats ] def score ( self , X : DataFrame ) -> list [ float ]: \"\"\"Calculates and returns the normalized anomaly scores for each data point. Args: X (DataFrame): The feature matrix to score. Returns: list[float]: A list of normalized anomaly scores. \"\"\" y_hat = self . model . decision_function ( X ) return 1 - interp ( y_hat , ( min ( y_hat ), max ( y_hat )), ( 0 , 1 ))","title":"Model"},{"location":"ai/reference/services/#ai.tilly.services.ml.model.Model.__init__","text":"Initializes the Model instance with the given parameters. Parameters: Name Type Description Default estimated_usage str | float The contamination factor for the IsolationForest model. Defaults to \"auto\". 'auto' model_params dict Additional parameters for the IsolationForest model. Defaults to MODEL_PARAMS. MODEL_PARAMS Returns: Type Description None None Source code in ai/tilly/services/ml/model.py 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 def __init__ ( self , estimated_usage : str | float = \"auto\" , model_params = MODEL_PARAMS ) -> None : \"\"\"Initializes the Model instance with the given parameters. Args: estimated_usage (str | float, optional): The contamination factor for the IsolationForest model. Defaults to \"auto\". model_params (dict, optional): Additional parameters for the IsolationForest model. Defaults to MODEL_PARAMS. Returns: None \"\"\" self . model = IsolationForest ( contamination = estimated_usage , ** model_params , )","title":"__init__()"},{"location":"ai/reference/services/#ai.tilly.services.ml.model.Model.fit","text":"Fits the model with the given features. Parameters: Name Type Description Default X DataFrame The feature matrix to train on. required Returns: Name Type Description Model Model The trained model instance. Source code in ai/tilly/services/ml/model.py 27 28 29 30 31 32 33 34 35 36 37 def fit ( self , X : DataFrame ) -> \"Model\" : \"\"\"Fits the model with the given features. Args: X (DataFrame): The feature matrix to train on. Returns: Model: The trained model instance. \"\"\" self . model . fit ( X ) return self","title":"fit()"},{"location":"ai/reference/services/#ai.tilly.services.ml.model.Model.predict","text":"Predicts whether each data point is anomalous or not. Returns 1 if the point is an outlier, and 0 otherwise. Parameters: Name Type Description Default X DataFrame The feature matrix to predict on. required Returns: Type Description list [ float ] list[float]: A list of prediction results. Source code in ai/tilly/services/ml/model.py 39 40 41 42 43 44 45 46 47 48 49 50 51 def predict ( self , X : DataFrame ) -> list [ float ]: \"\"\"Predicts whether each data point is anomalous or not. Returns 1 if the point is an outlier, and 0 otherwise. Args: X (DataFrame): The feature matrix to predict on. Returns: list[float]: A list of prediction results. \"\"\" y_hats = self . model . predict ( X ) return [ 1 if y_hat == - 1 else 0 for y_hat in y_hats ]","title":"predict()"},{"location":"ai/reference/services/#ai.tilly.services.ml.model.Model.score","text":"Calculates and returns the normalized anomaly scores for each data point. Parameters: Name Type Description Default X DataFrame The feature matrix to score. required Returns: Type Description list [ float ] list[float]: A list of normalized anomaly scores. Source code in ai/tilly/services/ml/model.py 53 54 55 56 57 58 59 60 61 62 63 def score ( self , X : DataFrame ) -> list [ float ]: \"\"\"Calculates and returns the normalized anomaly scores for each data point. Args: X (DataFrame): The feature matrix to score. Returns: list[float]: A list of normalized anomaly scores. \"\"\" y_hat = self . model . decision_function ( X ) return 1 - interp ( y_hat , ( min ( y_hat ), max ( y_hat )), ( 0 , 1 ))","title":"score()"},{"location":"ai/reference/services/#ai.tilly.services.ml.model_registry","text":"Model Registry Module This module manages the model registry for machine learning models in the Tilly system. It contains utilities for training, predicting, and handling models that are specific to each room. The ModelRegistry is a singleton class that holds a dictionary of trained models, ensuring only one instance exists across the application. Modules: Name Description - update_registry Function to update the global model registry. - get_current_registry Function to fetch the current model registry. - ModelRegistry Singleton class to manage room-specific models.","title":"model_registry"},{"location":"ai/reference/services/#ai.tilly.services.ml.model_registry.ModelRegistry","text":"A singleton class representing the model registry. This class is responsible for training, fitting, predicting, and managing each model. It holds a dictionary of models that are specific to each room. Attributes: Name Type Description - models (Dict[str, Model] A dictionary holding room-specific machine learning models. Methods: Name Description - train Train models based on new timeslot data. - fit_predict Train and predict on new room data. - predict Make predictions using pre-trained models. - preprocess Preprocesses the input data for each room. - postprocess Postprocesses the predicted data for each room. Source code in ai/tilly/services/ml/model_registry.py 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 class ModelRegistry : \"\"\"A singleton class representing the model registry. This class is responsible for training, fitting, predicting, and managing each model. It holds a dictionary of models that are specific to each room. Attributes: - models (Dict[str, Model]): A dictionary holding room-specific machine learning models. Methods: - train: Train models based on new timeslot data. - fit_predict: Train and predict on new room data. - predict: Make predictions using pre-trained models. - preprocess: Preprocesses the input data for each room. - postprocess: Postprocesses the predicted data for each room. \"\"\" _instance = None _lock = Lock () def __new__ ( cls ): \"\"\"Ensures that only one instance of the class exists.\"\"\" with cls . _lock : if cls . _instance is None : cls . _instance = super ( ModelRegistry , cls ) . __new__ ( cls ) return cls . _instance def __init__ ( self ): \"\"\"Initializes an empty model dictionary.\"\"\" self . models : Dict [ str , Model ] = {} def train ( self , timeslots : dict [ str , DataFrame ]) -> None : \"\"\"Train models based on new timeslot data and store them in the registry. Args: timeslots (dict[str, DataFrame]): The timeslots data to train on, per room. \"\"\" _preprocessed : dict [ str , DataFrame ] = self . preprocess ( timeslots ) room_results : dict [ str , DataFrame ] = self . fit_predict ( _preprocessed ) _postprocessed : dict [ str , DataFrame ] = self . postprocess ( room_results ) return _postprocessed def fit_predict ( self , rooms : dict [ str , DataFrame ]) -> Dict [ str , DataFrame ]: \"\"\"Train and predict on new room data. Args: rooms (dict[str, DataFrame]): Room data to fit and predict on. Returns: Dict[str, DataFrame]: The predicted DataFrame for each room. \"\"\" output = {} with tqdm ( total = len ( rooms ), desc = \"Initial\" ) as pbar : for name , timeslots in rooms . items (): pbar . set_postfix_str ( f \"Running fit_predict | Room: { name } \" ) pbar . update ( 1 ) if not timeslots . empty : features = timeslots [ FEATURES ] # extract features model = Model ( estimated_usage = 0.3 ) . fit ( X = features ) # fit model self . models [ name ] = model # add model to registry # make predictions output [ name ]: DataFrame = self . _predict ( name , timeslots ) return output def predict ( self , rooms : dict [ str , DataFrame ]) -> dict [ str , DataFrame ]: \"\"\"Make predictions using pre-trained models in the registry. Args: rooms (dict[str, DataFrame]): Room data to predict on. Returns: dict[str, DataFrame]: The predicted DataFrame for each room. \"\"\" _preprocessed : dict [ str , DataFrame ] = self . preprocess ( rooms ) _predictions = { name : self . _predict ( name , room ) for name , room in tqdm ( _preprocessed . items ()) if not room . empty } return self . postprocess ( _predictions ) def _predict ( self , name : str , room : DataFrame ) -> DataFrame : \"\"\" Make predictions for a specific room using its corresponding model in the registry. Args: name (str): The name of the room. room (DataFrame): The room data to predict on. Returns: DataFrame: The predicted DataFrame for the room. \"\"\" # extract features features = room [ FEATURES ] # load model from registry if model := self . models . get ( name ): # extract scores and predictions scores : list [ float ] = model . score ( features ) preds : list [ int ] = model . predict ( features ) else : scores , preds = T . handle_missing_model ( room_name = name , room = room , models = self . models . keys () ) return room . assign ( ANOMALY_SCORE = scores , IN_USE = preds , ) def preprocess ( self , timeslots : dict [ str , DataFrame ]) -> dict [ str , DataFrame ]: \"\"\" Preprocesses the input timeslot data for each room. Args: timeslots (dict[str, DataFrame]): The timeslot data for each room. Returns: dict[str, DataFrame]: The preprocessed DataFrame for each room. \"\"\" logger . info ( \"Preprocessing data...\" ) return { name : room . pipe ( T . featurize ) for name , room in tqdm ( timeslots . items ())} def postprocess ( self , predictions : dict [ str , DataFrame ]) -> dict [ str , DataFrame ]: \"\"\"Postprocesses the prediction results for each room. Args: predictions (dict[str, DataFrame]): The predicted DataFrame for each room. Returns: dict[str, DataFrame]: The postprocessed DataFrame for each room. \"\"\" logger . info ( \"Postprocessing data...\" ) return { name : room . pipe ( T . heuristics ) for name , room in tqdm ( predictions . items ()) }","title":"ModelRegistry"},{"location":"ai/reference/services/#ai.tilly.services.ml.model_registry.ModelRegistry.__init__","text":"Initializes an empty model dictionary. Source code in ai/tilly/services/ml/model_registry.py 60 61 62 def __init__ ( self ): \"\"\"Initializes an empty model dictionary.\"\"\" self . models : Dict [ str , Model ] = {}","title":"__init__()"},{"location":"ai/reference/services/#ai.tilly.services.ml.model_registry.ModelRegistry.__new__","text":"Ensures that only one instance of the class exists. Source code in ai/tilly/services/ml/model_registry.py 53 54 55 56 57 58 def __new__ ( cls ): \"\"\"Ensures that only one instance of the class exists.\"\"\" with cls . _lock : if cls . _instance is None : cls . _instance = super ( ModelRegistry , cls ) . __new__ ( cls ) return cls . _instance","title":"__new__()"},{"location":"ai/reference/services/#ai.tilly.services.ml.model_registry.ModelRegistry.fit_predict","text":"Train and predict on new room data. Parameters: Name Type Description Default rooms dict [ str , DataFrame ] Room data to fit and predict on. required Returns: Type Description Dict [ str , DataFrame ] Dict[str, DataFrame]: The predicted DataFrame for each room. Source code in ai/tilly/services/ml/model_registry.py 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 def fit_predict ( self , rooms : dict [ str , DataFrame ]) -> Dict [ str , DataFrame ]: \"\"\"Train and predict on new room data. Args: rooms (dict[str, DataFrame]): Room data to fit and predict on. Returns: Dict[str, DataFrame]: The predicted DataFrame for each room. \"\"\" output = {} with tqdm ( total = len ( rooms ), desc = \"Initial\" ) as pbar : for name , timeslots in rooms . items (): pbar . set_postfix_str ( f \"Running fit_predict | Room: { name } \" ) pbar . update ( 1 ) if not timeslots . empty : features = timeslots [ FEATURES ] # extract features model = Model ( estimated_usage = 0.3 ) . fit ( X = features ) # fit model self . models [ name ] = model # add model to registry # make predictions output [ name ]: DataFrame = self . _predict ( name , timeslots ) return output","title":"fit_predict()"},{"location":"ai/reference/services/#ai.tilly.services.ml.model_registry.ModelRegistry.postprocess","text":"Postprocesses the prediction results for each room. Parameters: Name Type Description Default predictions dict [ str , DataFrame ] The predicted DataFrame for each room. required Returns: Type Description dict [ str , DataFrame ] dict[str, DataFrame]: The postprocessed DataFrame for each room. Source code in ai/tilly/services/ml/model_registry.py 165 166 167 168 169 170 171 172 173 174 175 176 177 def postprocess ( self , predictions : dict [ str , DataFrame ]) -> dict [ str , DataFrame ]: \"\"\"Postprocesses the prediction results for each room. Args: predictions (dict[str, DataFrame]): The predicted DataFrame for each room. Returns: dict[str, DataFrame]: The postprocessed DataFrame for each room. \"\"\" logger . info ( \"Postprocessing data...\" ) return { name : room . pipe ( T . heuristics ) for name , room in tqdm ( predictions . items ()) }","title":"postprocess()"},{"location":"ai/reference/services/#ai.tilly.services.ml.model_registry.ModelRegistry.predict","text":"Make predictions using pre-trained models in the registry. Parameters: Name Type Description Default rooms dict [ str , DataFrame ] Room data to predict on. required Returns: Type Description dict [ str , DataFrame ] dict[str, DataFrame]: The predicted DataFrame for each room. Source code in ai/tilly/services/ml/model_registry.py 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 def predict ( self , rooms : dict [ str , DataFrame ]) -> dict [ str , DataFrame ]: \"\"\"Make predictions using pre-trained models in the registry. Args: rooms (dict[str, DataFrame]): Room data to predict on. Returns: dict[str, DataFrame]: The predicted DataFrame for each room. \"\"\" _preprocessed : dict [ str , DataFrame ] = self . preprocess ( rooms ) _predictions = { name : self . _predict ( name , room ) for name , room in tqdm ( _preprocessed . items ()) if not room . empty } return self . postprocess ( _predictions )","title":"predict()"},{"location":"ai/reference/services/#ai.tilly.services.ml.model_registry.ModelRegistry.preprocess","text":"Preprocesses the input timeslot data for each room. Parameters: Name Type Description Default timeslots dict [ str , DataFrame ] The timeslot data for each room. required Returns: Type Description dict [ str , DataFrame ] dict[str, DataFrame]: The preprocessed DataFrame for each room. Source code in ai/tilly/services/ml/model_registry.py 152 153 154 155 156 157 158 159 160 161 162 163 def preprocess ( self , timeslots : dict [ str , DataFrame ]) -> dict [ str , DataFrame ]: \"\"\" Preprocesses the input timeslot data for each room. Args: timeslots (dict[str, DataFrame]): The timeslot data for each room. Returns: dict[str, DataFrame]: The preprocessed DataFrame for each room. \"\"\" logger . info ( \"Preprocessing data...\" ) return { name : room . pipe ( T . featurize ) for name , room in tqdm ( timeslots . items ())}","title":"preprocess()"},{"location":"ai/reference/services/#ai.tilly.services.ml.model_registry.ModelRegistry.train","text":"Train models based on new timeslot data and store them in the registry. Parameters: Name Type Description Default timeslots dict [ str , DataFrame ] The timeslots data to train on, per room. required Source code in ai/tilly/services/ml/model_registry.py 64 65 66 67 68 69 70 71 72 73 74 75 def train ( self , timeslots : dict [ str , DataFrame ]) -> None : \"\"\"Train models based on new timeslot data and store them in the registry. Args: timeslots (dict[str, DataFrame]): The timeslots data to train on, per room. \"\"\" _preprocessed : dict [ str , DataFrame ] = self . preprocess ( timeslots ) room_results : dict [ str , DataFrame ] = self . fit_predict ( _preprocessed ) _postprocessed : dict [ str , DataFrame ] = self . postprocess ( room_results ) return _postprocessed","title":"train()"},{"location":"ai/reference/services/#ai.tilly.services.ml.model_registry.get_current_registry","text":"Fetches the current model registry. Returns: Type Description ModelRegistry The current model registry instance. Source code in ai/tilly/services/ml/model_registry.py 199 200 201 202 203 204 205 def get_current_registry () -> ModelRegistry : \"\"\"Fetches the current model registry. Returns: The current model registry instance. \"\"\" return current_registry","title":"get_current_registry()"},{"location":"ai/reference/services/#ai.tilly.services.ml.model_registry.update_registry","text":"Updates the global model registry. Parameters: Name Type Description Default new_registry ModelRegistry The new model registry to set as global. required Source code in ai/tilly/services/ml/model_registry.py 189 190 191 192 193 194 195 196 def update_registry ( new_registry : ModelRegistry ): \"\"\"Updates the global model registry. Args: new_registry: The new model registry to set as global. \"\"\" global current_registry current_registry = new_registry","title":"update_registry()"},{"location":"ai/reference/services/#ai.tilly.services.ml.trainer","text":"","title":"trainer"},{"location":"ai/reference/services/#ai.tilly.services.ml.trainer.train_models","text":"Trains new models based on the given training data and updates the global model registry. This function performs the following steps: 1. Create a new instance of ModelRegistry. 2. Train the models using the training data. 3. Update the global model registry with the newly trained models. Parameters: Name Type Description Default training_data dict [ str , DataFrame ] A dictionary containing the training data for each room, keyed by room name. required Returns: Type Description list [ dict [ str , DataFrame ]] list[dict[str, DataFrame]]: A list of dictionaries, each containing the list [ dict [ str , DataFrame ]] predicted DataFrame and anomaly scores for each room. Source code in ai/tilly/services/ml/trainer.py 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 def train_models ( training_data : dict [ str , DataFrame ]) -> list [ dict [ str , DataFrame ]]: \"\"\"Trains new models based on the given training data and updates the global model registry. This function performs the following steps: 1. Create a new instance of ModelRegistry. 2. Train the models using the training data. 3. Update the global model registry with the newly trained models. Args: training_data (dict[str, DataFrame]): A dictionary containing the training data for each room, keyed by room name. Returns: list[dict[str, DataFrame]]: A list of dictionaries, each containing the predicted DataFrame and anomaly scores for each room. \"\"\" # Create a new model registry model_registry = ModelRegistry () # Train models and receive the results results : dict [ str , tuple [ list [ float ], list [ int ]]] = model_registry . train ( training_data ) # Update the global model registry update_registry ( model_registry ) logger . info ( \"Training flow completed\" ) return results","title":"train_models()"},{"location":"ai/reference/services/#ai.tilly.services.ml.transformations","text":"ML Transformations Module Initializer This initializer script in the tilly/services/ml/transformations folder imports and combines the Preprocessor and Postprocessor classes into a single Transformer class, inheriting the methods and attributes of both.","title":"transformations"},{"location":"ai/reference/services/#ai.tilly.services.ml.transformations.Transformer","text":"Bases: Preprocessor , Postprocessor Transformer Class This class inherits from both Preprocessor and Postprocessor classes, effectively combining their functionalities. Instances of this class can be used for both preprocessing and postprocessing steps in a machine learning pipeline. Inherits Preprocessor : Class containing methods and attributes for data preprocessing. Postprocessor : Class containing methods and attributes for data postprocessing. Source code in ai/tilly/services/ml/transformations/__init__.py 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 class Transformer ( Preprocessor , Postprocessor ): \"\"\" Transformer Class This class inherits from both `Preprocessor` and `Postprocessor` classes, effectively combining their functionalities. Instances of this class can be used for both preprocessing and postprocessing steps in a machine learning pipeline. Inherits: - `Preprocessor`: Class containing methods and attributes for data preprocessing. - `Postprocessor`: Class containing methods and attributes for data postprocessing. \"\"\" pass","title":"Transformer"},{"location":"ai/reference/services/#ai.tilly.services.ml.transformations.postprocessing","text":"","title":"postprocessing"},{"location":"ai/reference/services/#ai.tilly.services.ml.transformations.postprocessing.Postprocessor","text":"Postprocessor for handling and enhancing model output data. This class contains a series of static methods intended for postprocessing the output of predictive models. These methods apply heuristic rules to DataFrames containing room usage information and anomaly scores, handle cases where predictive models are missing, and combine original and scored data into enriched DataFrames.","title":"Postprocessor"},{"location":"ai/reference/services/#ai.tilly.services.ml.transformations.postprocessing.Postprocessor--methods","text":"heuristics(cls, room: pd.DataFrame) -> pd.DataFrame: Applies heuristic rules to modify the predicted data in a DataFrame representing room usage. It has the following rules: - Night Time Filtering: Filters out false positives during midnight to 6 AM. - Stand-Alone Instances: Removes isolated instances of \"IN_USE\" being 1. - Low CO2 Levels: Sets \"IN_USE\" to 0 if CO2 levels are low. The method also updates the \"ANOMALY_SCORE\" based on the modified \"IN_USE\" values. handle_missing_model(room_name: str, room: pd.DataFrame, models: list[str]) -> tuple[list[np.nan], list[np.nan]]: Returns null values for anomaly scores and predictions if the predictive model for a room is missing. It also logs a warning message about the missing model. combine_frames(original: dict[str, pd.DataFrame], scored: dict[str, pd.DataFrame], merge_cols: list = [\"DATE\", \"TIME\", \"ID\", \"KOMMUNE\", \"SKOLE\"]) -> pd.DataFrame: Merges the original data and the scored data for each room into a single DataFrame. If a room does not have corresponding scored data, it will be filled with null values in the 'IN_USE' and 'ANOMALY_SCORE' columns.","title":"Methods"},{"location":"ai/reference/services/#ai.tilly.services.ml.transformations.postprocessing.Postprocessor--examples","text":"df = pd.DataFrame({ ... 'DATETIME': pd.date_range(start='2022-01-01', periods=4, freq='15T'), ... 'ANOMALY_SCORE': [0.2, 0.8, 0.5, 0.7], ... 'IN_USE': [0, 1, 1, 0], ... 'CO2': [300, 400, 500, 200] ... }) processed_df = Postprocessor.heuristics(df) Source code in ai/tilly/services/ml/transformations/postprocessing.py 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 @make_all_methods_static class Postprocessor : \"\"\" Postprocessor for handling and enhancing model output data. This class contains a series of static methods intended for postprocessing the output of predictive models. These methods apply heuristic rules to DataFrames containing room usage information and anomaly scores, handle cases where predictive models are missing, and combine original and scored data into enriched DataFrames. Methods ------- heuristics(cls, room: pd.DataFrame) -> pd.DataFrame: Applies heuristic rules to modify the predicted data in a DataFrame representing room usage. It has the following rules: - Night Time Filtering: Filters out false positives during midnight to 6 AM. - Stand-Alone Instances: Removes isolated instances of \"IN_USE\" being 1. - Low CO2 Levels: Sets \"IN_USE\" to 0 if CO2 levels are low. The method also updates the \"ANOMALY_SCORE\" based on the modified \"IN_USE\" values. handle_missing_model(room_name: str, room: pd.DataFrame, models: list[str]) -> tuple[list[np.nan], list[np.nan]]: Returns null values for anomaly scores and predictions if the predictive model for a room is missing. It also logs a warning message about the missing model. combine_frames(original: dict[str, pd.DataFrame], scored: dict[str, pd.DataFrame], merge_cols: list = [\"DATE\", \"TIME\", \"ID\", \"KOMMUNE\", \"SKOLE\"]) -> pd.DataFrame: Merges the original data and the scored data for each room into a single DataFrame. If a room does not have corresponding scored data, it will be filled with null values in the 'IN_USE' and 'ANOMALY_SCORE' columns. Examples -------- >>> df = pd.DataFrame({ ... 'DATETIME': pd.date_range(start='2022-01-01', periods=4, freq='15T'), ... 'ANOMALY_SCORE': [0.2, 0.8, 0.5, 0.7], ... 'IN_USE': [0, 1, 1, 0], ... 'CO2': [300, 400, 500, 200] ... }) >>> processed_df = Postprocessor.heuristics(df) \"\"\" @classmethod def heuristics ( cls , room : pd . DataFrame ) -> pd . DataFrame : \"\"\" Apply heuristic rules to modify the predicted data in a room DataFrame. This class method applies a series of heuristic rules to a DataFrame containing predicted room usage and anomaly scores. The rules are applied in the following order: 1. Night Time Filtering: If the time is between midnight and 6 AM and the anomaly score is less than or equal to 0.7, set \"IN_USE\" to 0. 2. Stand-Alone Instances: If an instance of \"IN_USE\" being 1 is surrounded by instances of \"IN_USE\" being 0, set that isolated \"IN_USE\" to 0. 3. Low CO2 Levels: If the CO2 level is less than or equal to 325, set \"IN_USE\" to 0. The anomaly score is then updated based on the modified \"IN_USE\" values. Parameters ---------- cls : class The class to which this class method belongs. room : pd.DataFrame Input DataFrame containing at least the following columns: - \"DATETIME\": Timestamps for each 15-minute interval. - \"ANOMALY_SCORE\": Anomaly scores ranging from 0 to 1. - \"IN_USE\": Binary values indicating room usage (1 for in use, 0 for not in use). - \"CO2\": CO2 levels. Returns ------- pd.DataFrame Modified DataFrame after applying the heuristic rules. Examples -------- >>> df = pd.DataFrame({ ... 'DATETIME': pd.date_range(start='2022-01-01', periods=4, freq='15T'), ... 'ANOMALY_SCORE': [0.2, 0.8, 0.5, 0.7], ... 'IN_USE': [0, 1, 1, 0], ... 'CO2': [300, 400, 500, 200] ... }) >>> Postprocessor.heuristics(df) \"\"\" def apply_night_time_filter ( df ): \"\"\"Filters out false positives during midnight to 6 AM.\"\"\" hour = df [ \"DATETIME\" ] . dt . hour mask = ( hour >= 0 ) & ( hour < 6 ) & ( df [ \"ANOMALY_SCORE\" ] <= 0.7 ) df . loc [ mask , \"IN_USE\" ] = 0 return df def apply_stand_alone_instances_filter ( df ): \"\"\"Removes isolated instances of \"IN_USE\" being 1.\"\"\" prev_IN_USE = df [ \"IN_USE\" ] . shift ( 1 , fill_value = 0 ) next_IN_USE = df [ \"IN_USE\" ] . shift ( - 1 , fill_value = 0 ) mask = ( prev_IN_USE == 0 ) & ( df [ \"IN_USE\" ] == 1 ) & ( next_IN_USE == 0 ) df . loc [ mask , \"IN_USE\" ] = 0 return df def apply_low_co2_filter ( df ): \"\"\"Sets \"IN_USE\" to 0 if CO2 levels are low.\"\"\" mask = df [ \"CO2\" ] <= 325 df . loc [ mask , \"IN_USE\" ] = 0 return df def update_anomaly_score ( df ): \"\"\"Updates the anomaly score based on the modified \"IN_USE\" values.\"\"\" mask = (( df [ \"IN_USE\" ] == 1 ) & ( df [ \"ANOMALY_SCORE\" ] < 0.5 )) | ( ( df [ \"IN_USE\" ] == 0 ) & ( df [ \"ANOMALY_SCORE\" ] > 0.5 ) ) df . loc [ mask , \"ANOMALY_SCORE\" ] = 1 - df . loc [ mask , \"ANOMALY_SCORE\" ] return df return ( room . pipe ( apply_night_time_filter ) . pipe ( apply_stand_alone_instances_filter ) . pipe ( apply_low_co2_filter ) . pipe ( update_anomaly_score ) ) def handle_missing_model ( room_name : str , room : pd . DataFrame , models : list [ str ] ) -> tuple [ list [ np . nan ], list [ np . nan ]]: \"\"\"Returns null values for scores and predictions if a model is missing. Also, logs a warning message. Args: room_name (str): Room name room (pd.DataFrame): Room data models (dict[str, object]): Available models Returns: tuple[array[np.nan], array[np.nan]]: Scores and predictions \"\"\" n_rows = room . shape [ 0 ] school = room_name . split ( \"_\" )[ 0 ] logger . warning ( f \"Model for { room_name } ( { school } ) not found. Returning null values. \\n \" + f \"Available models in registry for { school } : \\n \" + f \" { [ key for key in models if school in key ] } \" ) return ( np . full ( n_rows , np . nan ), np . full ( n_rows , np . nan )) def combine_frames ( original : dict [ str , pd . DataFrame ], scored : dict [ str , pd . DataFrame ], merge_cols : list = [ \"DATE\" , \"TIME\" , \"ID\" , \"KOMMUNE\" , \"SKOLE\" ], ) -> pd . DataFrame : \"\"\" Combines original and scored data into a single pd.DataFrame. The function takes two dictionaries, where the keys represent room identifiers and the values are Pandas DataFrames containing the original and scored data. It enriches the original data with the anomaly scores and 'IN_USE' indicators from the scored data. If a room does not have corresponding scored data, it will be filled with null values in the 'IN_USE' and 'ANOMALY_SCORE' columns. Args: - original (dict[str, pd.DataFrame]): A dictionary containing the original data. The keys are room identifiers, and the values are pd.DataFrames with the original data. - scored (dict[str, pd.DataFrame]): A dictionary containing the scored data. The keys are room identifiers, and the values are pd.DataFrames with the anomaly scores and 'IN_USE' indicators. - merge_cols (list): The list of column names to use for merging the original and scored data. Returns: - pd.DataFrame: A concatenated DataFrame containing all the enriched data. \"\"\" combined_frames = [] for key , orig_df in original . items (): scored_df = scored . get ( key , None ) if scored_df is not None : # Merge based on multiple common columns combined_df = orig_df . merge ( scored_df [[ \"ANOMALY_SCORE\" , \"IN_USE\" ] + merge_cols ], how = \"left\" , on = merge_cols , ) else : # If there's no corresponding scored pd.dataframe, # copy the original pd.dataframe and add null columns # for 'IN_USE' and 'ANOMALY_SCORE' combined_df = orig_df . copy () combined_df [ \"ANOMALY_SCORE\" ] = None combined_df [ \"IN_USE\" ] = None # Add an identifier for the original key (i.e., room name) combined_df [ \"ROOM\" ] = key combined_frames . append ( combined_df ) # Concatenate all frames vertically return pd . concat ( combined_frames , ignore_index = True ) combine_frames ( original : dict [ str , pd . DataFrame ], scored : dict [ str , pd . DataFrame ], merge_cols : list = [ 'DATE' , 'TIME' , 'ID' , 'KOMMUNE' , 'SKOLE' ]) -> pd . DataFrame Combines original and scored data into a single pd.DataFrame. The function takes two dictionaries, where the keys represent room identifiers and the values are Pandas DataFrames containing the original and scored data. It enriches the original data with the anomaly scores and 'IN_USE' indicators from the scored data. If a room does not have corresponding scored data, it will be filled with null values in the 'IN_USE' and 'ANOMALY_SCORE' columns. Args: - original (dict[str, pd.DataFrame]): A dictionary containing the original data. The keys are room identifiers, and the values are pd.DataFrames with the original data. - scored (dict[str, pd.DataFrame]): A dictionary containing the scored data. The keys are room identifiers, and the values are pd.DataFrames with the anomaly scores and 'IN_USE' indicators. - merge_cols (list): The list of column names to use for merging the original and scored data. Returns: - pd.DataFrame: A concatenated DataFrame containing all the enriched data. Source code in ai/tilly/services/ml/transformations/postprocessing.py 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 def combine_frames ( original : dict [ str , pd . DataFrame ], scored : dict [ str , pd . DataFrame ], merge_cols : list = [ \"DATE\" , \"TIME\" , \"ID\" , \"KOMMUNE\" , \"SKOLE\" ], ) -> pd . DataFrame : \"\"\" Combines original and scored data into a single pd.DataFrame. The function takes two dictionaries, where the keys represent room identifiers and the values are Pandas DataFrames containing the original and scored data. It enriches the original data with the anomaly scores and 'IN_USE' indicators from the scored data. If a room does not have corresponding scored data, it will be filled with null values in the 'IN_USE' and 'ANOMALY_SCORE' columns. Args: - original (dict[str, pd.DataFrame]): A dictionary containing the original data. The keys are room identifiers, and the values are pd.DataFrames with the original data. - scored (dict[str, pd.DataFrame]): A dictionary containing the scored data. The keys are room identifiers, and the values are pd.DataFrames with the anomaly scores and 'IN_USE' indicators. - merge_cols (list): The list of column names to use for merging the original and scored data. Returns: - pd.DataFrame: A concatenated DataFrame containing all the enriched data. \"\"\" combined_frames = [] for key , orig_df in original . items (): scored_df = scored . get ( key , None ) if scored_df is not None : # Merge based on multiple common columns combined_df = orig_df . merge ( scored_df [[ \"ANOMALY_SCORE\" , \"IN_USE\" ] + merge_cols ], how = \"left\" , on = merge_cols , ) else : # If there's no corresponding scored pd.dataframe, # copy the original pd.dataframe and add null columns # for 'IN_USE' and 'ANOMALY_SCORE' combined_df = orig_df . copy () combined_df [ \"ANOMALY_SCORE\" ] = None combined_df [ \"IN_USE\" ] = None # Add an identifier for the original key (i.e., room name) combined_df [ \"ROOM\" ] = key combined_frames . append ( combined_df ) # Concatenate all frames vertically return pd . concat ( combined_frames , ignore_index = True ) handle_missing_model ( room_name : str , room : pd . DataFrame , models : list [ str ]) -> tuple [ list [ np . nan ], list [ np . nan ]] Returns null values for scores and predictions if a model is missing. Also, logs a warning message. Parameters: Name Type Description Default room_name str Room name required room DataFrame Room data required models dict [ str , object ] Available models required Returns: Type Description tuple [ list [ nan ], list [ nan ]] tuple[array[np.nan], array[np.nan]]: Scores and predictions Source code in ai/tilly/services/ml/transformations/postprocessing.py 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 def handle_missing_model ( room_name : str , room : pd . DataFrame , models : list [ str ] ) -> tuple [ list [ np . nan ], list [ np . nan ]]: \"\"\"Returns null values for scores and predictions if a model is missing. Also, logs a warning message. Args: room_name (str): Room name room (pd.DataFrame): Room data models (dict[str, object]): Available models Returns: tuple[array[np.nan], array[np.nan]]: Scores and predictions \"\"\" n_rows = room . shape [ 0 ] school = room_name . split ( \"_\" )[ 0 ] logger . warning ( f \"Model for { room_name } ( { school } ) not found. Returning null values. \\n \" + f \"Available models in registry for { school } : \\n \" + f \" { [ key for key in models if school in key ] } \" ) return ( np . full ( n_rows , np . nan ), np . full ( n_rows , np . nan )) heuristics ( room : pd . DataFrame ) -> pd . DataFrame classmethod Apply heuristic rules to modify the predicted data in a room DataFrame. This class method applies a series of heuristic rules to a DataFrame containing predicted room usage and anomaly scores. The rules are applied in the following order: Night Time Filtering: If the time is between midnight and 6 AM and the anomaly score is less than or equal to 0.7, set \"IN_USE\" to 0. Stand-Alone Instances: If an instance of \"IN_USE\" being 1 is surrounded by instances of \"IN_USE\" being 0, set that isolated \"IN_USE\" to 0. Low CO2 Levels: If the CO2 level is less than or equal to 325, set \"IN_USE\" to 0. The anomaly score is then updated based on the modified \"IN_USE\" values.","title":"Examples"},{"location":"ai/reference/services/#ai.tilly.services.ml.transformations.postprocessing.Postprocessor.heuristics--parameters","text":"cls : class The class to which this class method belongs. room : pd.DataFrame Input DataFrame containing at least the following columns: - \"DATETIME\": Timestamps for each 15-minute interval. - \"ANOMALY_SCORE\": Anomaly scores ranging from 0 to 1. - \"IN_USE\": Binary values indicating room usage (1 for in use, 0 for not in use). - \"CO2\": CO2 levels.","title":"Parameters"},{"location":"ai/reference/services/#ai.tilly.services.ml.transformations.postprocessing.Postprocessor.heuristics--returns","text":"pd.DataFrame Modified DataFrame after applying the heuristic rules.","title":"Returns"},{"location":"ai/reference/services/#ai.tilly.services.ml.transformations.postprocessing.Postprocessor.heuristics--examples","text":"df = pd.DataFrame({ ... 'DATETIME': pd.date_range(start='2022-01-01', periods=4, freq='15T'), ... 'ANOMALY_SCORE': [0.2, 0.8, 0.5, 0.7], ... 'IN_USE': [0, 1, 1, 0], ... 'CO2': [300, 400, 500, 200] ... }) Postprocessor.heuristics(df) Source code in ai/tilly/services/ml/transformations/postprocessing.py 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 @classmethod def heuristics ( cls , room : pd . DataFrame ) -> pd . DataFrame : \"\"\" Apply heuristic rules to modify the predicted data in a room DataFrame. This class method applies a series of heuristic rules to a DataFrame containing predicted room usage and anomaly scores. The rules are applied in the following order: 1. Night Time Filtering: If the time is between midnight and 6 AM and the anomaly score is less than or equal to 0.7, set \"IN_USE\" to 0. 2. Stand-Alone Instances: If an instance of \"IN_USE\" being 1 is surrounded by instances of \"IN_USE\" being 0, set that isolated \"IN_USE\" to 0. 3. Low CO2 Levels: If the CO2 level is less than or equal to 325, set \"IN_USE\" to 0. The anomaly score is then updated based on the modified \"IN_USE\" values. Parameters ---------- cls : class The class to which this class method belongs. room : pd.DataFrame Input DataFrame containing at least the following columns: - \"DATETIME\": Timestamps for each 15-minute interval. - \"ANOMALY_SCORE\": Anomaly scores ranging from 0 to 1. - \"IN_USE\": Binary values indicating room usage (1 for in use, 0 for not in use). - \"CO2\": CO2 levels. Returns ------- pd.DataFrame Modified DataFrame after applying the heuristic rules. Examples -------- >>> df = pd.DataFrame({ ... 'DATETIME': pd.date_range(start='2022-01-01', periods=4, freq='15T'), ... 'ANOMALY_SCORE': [0.2, 0.8, 0.5, 0.7], ... 'IN_USE': [0, 1, 1, 0], ... 'CO2': [300, 400, 500, 200] ... }) >>> Postprocessor.heuristics(df) \"\"\" def apply_night_time_filter ( df ): \"\"\"Filters out false positives during midnight to 6 AM.\"\"\" hour = df [ \"DATETIME\" ] . dt . hour mask = ( hour >= 0 ) & ( hour < 6 ) & ( df [ \"ANOMALY_SCORE\" ] <= 0.7 ) df . loc [ mask , \"IN_USE\" ] = 0 return df def apply_stand_alone_instances_filter ( df ): \"\"\"Removes isolated instances of \"IN_USE\" being 1.\"\"\" prev_IN_USE = df [ \"IN_USE\" ] . shift ( 1 , fill_value = 0 ) next_IN_USE = df [ \"IN_USE\" ] . shift ( - 1 , fill_value = 0 ) mask = ( prev_IN_USE == 0 ) & ( df [ \"IN_USE\" ] == 1 ) & ( next_IN_USE == 0 ) df . loc [ mask , \"IN_USE\" ] = 0 return df def apply_low_co2_filter ( df ): \"\"\"Sets \"IN_USE\" to 0 if CO2 levels are low.\"\"\" mask = df [ \"CO2\" ] <= 325 df . loc [ mask , \"IN_USE\" ] = 0 return df def update_anomaly_score ( df ): \"\"\"Updates the anomaly score based on the modified \"IN_USE\" values.\"\"\" mask = (( df [ \"IN_USE\" ] == 1 ) & ( df [ \"ANOMALY_SCORE\" ] < 0.5 )) | ( ( df [ \"IN_USE\" ] == 0 ) & ( df [ \"ANOMALY_SCORE\" ] > 0.5 ) ) df . loc [ mask , \"ANOMALY_SCORE\" ] = 1 - df . loc [ mask , \"ANOMALY_SCORE\" ] return df return ( room . pipe ( apply_night_time_filter ) . pipe ( apply_stand_alone_instances_filter ) . pipe ( apply_low_co2_filter ) . pipe ( update_anomaly_score ) )","title":"Examples"},{"location":"ai/reference/services/#ai.tilly.services.ml.transformations.postprocessing.make_all_methods_static","text":"Decorator to make all methods in a class static. Source code in ai/tilly/services/ml/transformations/postprocessing.py 6 7 8 9 10 11 def make_all_methods_static ( cls ): \"\"\"Decorator to make all methods in a class static.\"\"\" for attr_name , attr_value in cls . __dict__ . items (): if callable ( attr_value ): setattr ( cls , attr_name , staticmethod ( attr_value )) return cls","title":"make_all_methods_static()"},{"location":"ai/reference/services/#ai.tilly.services.ml.transformations.preprocessing","text":"","title":"preprocessing"},{"location":"ai/reference/services/#ai.tilly.services.ml.transformations.preprocessing.Preprocessor","text":"A class that contains all the preprocessing logic for the model input Source code in ai/tilly/services/ml/transformations/preprocessing.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 class Preprocessor : \"\"\"A class that contains all the preprocessing logic for the model input\"\"\" @classmethod def estimate_usage ( cls , data : pd . DataFrame , usage_coeff = 2.1 , usage_min = 0.1 , usage_max = 0.4 ) -> float : \"\"\"Estimates the usage of a given room from the union of - booked timeslots: Timeslots within a registered booking, and - Scheduled timeslots: TImeslots within the school schema. This is used as a prior for our anomaly detection model. Args: data (pd.DataFrame): Timeslots usage_coeff (float, optional): Usage coefficient. Defaults to 2.1. This is a heuristic measure used to scale the estimated usage usage_min (float, optional): Minimum usage. Defaults to 0.1. This is a heuristic lower bound for the net estimated usage of a room. usage_max (float, optional): Maximum usage. Defaults to 0.4. This is a heuristic upper bound for the net estimated usage of a room. \"\"\" try : used_slots = ( data [ \"SKEMALAGT\" ] | data [ \"BOOKET\" ] . fillna ( False )) . sum () used_slots = max ( used_slots , usage_min ) return min ( usage_coeff * used_slots / len ( data ), usage_max ) except ZeroDivisionError : return \"auto\" @classmethod @log_pipeline def merge_dt ( cls , df , date , time , name , sep = \" \" ): return df . assign ( ** { name : lambda d : pd . to_datetime ( d [ date ] . astype ( str ) + sep + d [ time ] . astype ( str ) ) } ) @classmethod @log_pipeline def add_missing_timeslots ( cls , df : pd . DataFrame , freq : str = \"15T\" ) -> pd . DataFrame : \"\"\"Adds the rows that are missing from the DataFrame, by merging it with a DataFrame containing all the timeslots.\"\"\" static_values = ( df . head ( 1 )[[ \"ID\" , \"KOMMUNE\" , \"SKOLE\" , \"SKOLE_ID\" ]] . squeeze () . to_dict () ) return ( pd . DataFrame ( { \"DATETIME\" : pd . date_range ( start = df [ \"DATETIME\" ] . min (), end = df [ \"DATETIME\" ] . max (), freq = freq ) } ) . assign ( ** static_values ) . merge ( df , on = [ \"DATETIME\" , \"ID\" , \"KOMMUNE\" , \"SKOLE\" , \"SKOLE_ID\" ], how = \"left\" ) ) @classmethod @log_pipeline def interpolate_missing_islands ( cls , df : pd . DataFrame , * , target_col : str = \"CO2\" , limit : int = 3 , direction : str = \"forward\" , method : str = \"cubic\" , ** kwargs , ) -> pd . DataFrame : \"\"\"Interpolate missing values in a dataframe, but only for islands of missing values, ie. rows where there are no more than `limit` consecutive missing values in the `target_col` column.\"\"\" try : return df . assign ( CO2 = lambda d : d [ target_col ] . interpolate ( method = method , limit = limit , limit_direction = direction , ** kwargs , ) ) except ValueError : logger . warning ( f \"[ { df . iloc [ 0 ][ 'SKOLE_ID' ] } ] \" + \"Interpolation failed, falling back to linear\" ) return df . assign ( CO2 = lambda d : d [ target_col ] . interpolate ( method = \"linear\" , limit = limit , limit_direction = direction , ** kwargs , ) ) @classmethod @log_pipeline def remove_stagnate_intervals ( cls , df , target_col : str = \"CO2\" , threshold = 4 ) -> pd . DataFrame : \"\"\"Remove intervals where the CO2 value is the same for consecutive rows within time-contiguous blocks\"\"\" return ( df . assign ( time_diff = lambda d : d [ \"DATETIME\" ] . diff (), new_block = lambda d : ( d [ \"time_diff\" ] > pd . Timedelta ( minutes = 15 )) | ( d [ target_col ] != d [ target_col ] . shift ( 1 )), block_id = lambda d : d [ \"new_block\" ] . cumsum (), ) . assign ( block_count = lambda d : d . groupby ( \"block_id\" )[ \"block_id\" ] . transform ( \"count\" ) )[ lambda d : d [ \"block_count\" ] . lt ( threshold )] . drop ([ \"time_diff\" , \"new_block\" , \"block_id\" , \"block_count\" ], axis = 1 ) ) @classmethod @log_pipeline def drop_outliers ( cls , df , bounds : dict [ str , tuple [ float | None , float | None ]] ) -> pd . DataFrame : \"\"\"Drop rows where values are outside the given bounds. Args: df (pd.DataFrame): DataFrame to filter bounds (dict[str, tuple[float | None, float | None]]): Dictionary of column names and their lower and upper bounds. If a bound is None, then it is not applied. Returns: df (pd.DataFrame): Filtered DataFrame \"\"\" mask = np . ones ( df . shape [ 0 ], dtype = bool ) for col_name , ( lo , hi ) in bounds . items (): if lo is not None and hi is not None : mask &= ( df [ col_name ] . values >= lo ) & ( df [ col_name ] . values <= hi ) elif lo is not None : mask &= df [ col_name ] . values >= lo elif hi is not None : mask &= df [ col_name ] . values <= hi else : raise ValueError ( f \"Bounds for { col_name } are both None\" ) return df [ mask ] @classmethod @log_pipeline def day_filter ( cls , df : pd . DataFrame , * , min_ratio : float = 0.25 ) -> pd . DataFrame : \"\"\"Filter out days with too few data points (days with less than min_ratio of the data points). This is done to avoid overfitting on days with too few data points,where the kinematic quantities are not calculated correctly. Min ratio is multiplied by 4*24 to get the number of data points required for a day (4*24 is the number of 15 minute intervals in a day) Args: df (pd.DataFrame): DataFrame to filter min_ratio (float, optional): Minimum ratio of data points required for a day. Defaults to 0.25. Returns: df (pd.DataFrame): Filtered DataFrame \"\"\" min_data_points_required = int ( min_ratio * ( 4 * 24 )) return df . groupby ( \"DATE\" ) . filter ( lambda x : len ( x ) >= min_data_points_required ) @classmethod def calculate_kinematic_quantities ( cls , df , metric , * , window , prefix = None ) -> pd . DataFrame : \"\"\"Add rolling velocity, acceleration, and jerk for a metric. The rolling quantities are calculated using the gradient of the metric and the given window size. Null values are filled with zeros.\"\"\" if prefix is None : prefix = metric # Calculate the rolling window mean for the metric rolling_metric = df [ metric ] . rolling ( window = window ) . mean () # First order derivative (velocity) df [ f \" { prefix } _velocity\" ] = rolling_metric . diff () # Second order derivative (acceleration) df [ f \" { prefix } _acceleration\" ] = df [ f \" { prefix } _velocity\" ] . diff () # Third order derivative (jerk) df [ f \" { prefix } _jerk\" ] = df [ f \" { prefix } _acceleration\" ] . diff () # Log of the metric df [ f \" { prefix } _log\" ] = np . log ( df [ metric ] . fillna ( 1 ) + 1 ) # Fill NAs fill_cols = [ f \" { prefix } _velocity\" , f \" { prefix } _acceleration\" , f \" { prefix } _jerk\" ] df [ fill_cols ] = df [ fill_cols ] . fillna ( 0 ) return df @classmethod def gaussian_smooth ( cls , df , metric , * , std_dev = 2 ): \"\"\"Apply a gaussian filter to a given metric\"\"\" df [ f \" { metric } _smoothed\" ] = gaussian_filter1d ( df [ metric ], sigma = std_dev ) return df @classmethod @log_pipeline def apply_time_group_funcs ( cls , df , funcs ) -> pd . DataFrame : \"\"\"Apply a list of functions to each time-contiguous block of data in the DataFrame\"\"\" # Calculate all the needed columns in one go df [ \"time_diff\" ] = df [ \"DATETIME\" ] . diff () df [ \"new_block\" ] = df [ \"time_diff\" ] > pd . Timedelta ( minutes = 15 ) df [ \"block_id\" ] = df [ \"new_block\" ] . cumsum () for func , kwargs in funcs : df = df . groupby ( \"block_id\" ) . apply ( func , ** kwargs ) . reset_index ( drop = True ) df . drop ( columns = [ \"block_id\" , \"new_block\" , \"time_diff\" ], inplace = True ) return df @classmethod @log_pipeline def add_time_features ( cls , df , * , night_start = 23 , night_end = 6 ): \"\"\"Add time features to the DataFrame\"\"\" return df . assign ( is_night = lambda d : ( ( d [ \"DATETIME\" ] . dt . hour >= night_start ) & ( d [ \"DATETIME\" ] . dt . hour <= night_end ) ) . astype ( int ), ) @classmethod @log_pipeline def featurize ( cls , df : pd . DataFrame ) -> pd . DataFrame : \"\"\"Run the full preprocessing flow on a DataFrame\"\"\" return ( df . pipe ( cls . merge_dt , date = \"DATE\" , time = \"TIME\" , name = \"DATETIME\" ) . pipe ( cls . add_missing_timeslots ) . pipe ( cls . interpolate_missing_islands , target_col = \"CO2\" , limit = 4 ) . pipe ( cls . remove_stagnate_intervals , target_col = \"CO2\" , threshold = 5 ) . dropna ( subset = [ \"CO2\" ]) . pipe ( cls . drop_outliers , bounds = { \"CO2\" : ( 1 , 8000 )}) . pipe ( cls . day_filter , min_ratio = 0.25 ) . pipe ( cls . apply_time_group_funcs , funcs = [ ( cls . gaussian_smooth , dict ( metric = \"CO2\" , std_dev = 2 )), ( cls . calculate_kinematic_quantities , dict ( metric = \"CO2_smoothed\" , window = 4 , prefix = \"CO2\" ), ), ], ) . pipe ( cls . add_time_features , night_start = 22 , night_end = 6 ) ) add_missing_timeslots ( df : pd . DataFrame , freq : str = '15T' ) -> pd . DataFrame classmethod Adds the rows that are missing from the DataFrame, by merging it with a DataFrame containing all the timeslots. Source code in ai/tilly/services/ml/transformations/preprocessing.py 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 @classmethod @log_pipeline def add_missing_timeslots ( cls , df : pd . DataFrame , freq : str = \"15T\" ) -> pd . DataFrame : \"\"\"Adds the rows that are missing from the DataFrame, by merging it with a DataFrame containing all the timeslots.\"\"\" static_values = ( df . head ( 1 )[[ \"ID\" , \"KOMMUNE\" , \"SKOLE\" , \"SKOLE_ID\" ]] . squeeze () . to_dict () ) return ( pd . DataFrame ( { \"DATETIME\" : pd . date_range ( start = df [ \"DATETIME\" ] . min (), end = df [ \"DATETIME\" ] . max (), freq = freq ) } ) . assign ( ** static_values ) . merge ( df , on = [ \"DATETIME\" , \"ID\" , \"KOMMUNE\" , \"SKOLE\" , \"SKOLE_ID\" ], how = \"left\" ) ) add_time_features ( df , * , night_start = 23 , night_end = 6 ) classmethod Add time features to the DataFrame Source code in ai/tilly/services/ml/transformations/preprocessing.py 245 246 247 248 249 250 251 252 253 254 @classmethod @log_pipeline def add_time_features ( cls , df , * , night_start = 23 , night_end = 6 ): \"\"\"Add time features to the DataFrame\"\"\" return df . assign ( is_night = lambda d : ( ( d [ \"DATETIME\" ] . dt . hour >= night_start ) & ( d [ \"DATETIME\" ] . dt . hour <= night_end ) ) . astype ( int ), ) apply_time_group_funcs ( df , funcs ) -> pd . DataFrame classmethod Apply a list of functions to each time-contiguous block of data in the DataFrame Source code in ai/tilly/services/ml/transformations/preprocessing.py 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 @classmethod @log_pipeline def apply_time_group_funcs ( cls , df , funcs ) -> pd . DataFrame : \"\"\"Apply a list of functions to each time-contiguous block of data in the DataFrame\"\"\" # Calculate all the needed columns in one go df [ \"time_diff\" ] = df [ \"DATETIME\" ] . diff () df [ \"new_block\" ] = df [ \"time_diff\" ] > pd . Timedelta ( minutes = 15 ) df [ \"block_id\" ] = df [ \"new_block\" ] . cumsum () for func , kwargs in funcs : df = df . groupby ( \"block_id\" ) . apply ( func , ** kwargs ) . reset_index ( drop = True ) df . drop ( columns = [ \"block_id\" , \"new_block\" , \"time_diff\" ], inplace = True ) return df calculate_kinematic_quantities ( df , metric , * , window , prefix = None ) -> pd . DataFrame classmethod Add rolling velocity, acceleration, and jerk for a metric. The rolling quantities are calculated using the gradient of the metric and the given window size. Null values are filled with zeros. Source code in ai/tilly/services/ml/transformations/preprocessing.py 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 @classmethod def calculate_kinematic_quantities ( cls , df , metric , * , window , prefix = None ) -> pd . DataFrame : \"\"\"Add rolling velocity, acceleration, and jerk for a metric. The rolling quantities are calculated using the gradient of the metric and the given window size. Null values are filled with zeros.\"\"\" if prefix is None : prefix = metric # Calculate the rolling window mean for the metric rolling_metric = df [ metric ] . rolling ( window = window ) . mean () # First order derivative (velocity) df [ f \" { prefix } _velocity\" ] = rolling_metric . diff () # Second order derivative (acceleration) df [ f \" { prefix } _acceleration\" ] = df [ f \" { prefix } _velocity\" ] . diff () # Third order derivative (jerk) df [ f \" { prefix } _jerk\" ] = df [ f \" { prefix } _acceleration\" ] . diff () # Log of the metric df [ f \" { prefix } _log\" ] = np . log ( df [ metric ] . fillna ( 1 ) + 1 ) # Fill NAs fill_cols = [ f \" { prefix } _velocity\" , f \" { prefix } _acceleration\" , f \" { prefix } _jerk\" ] df [ fill_cols ] = df [ fill_cols ] . fillna ( 0 ) return df day_filter ( df : pd . DataFrame , * , min_ratio : float = 0.25 ) -> pd . DataFrame classmethod Filter out days with too few data points (days with less than min_ratio of the data points). This is done to avoid overfitting on days with too few data points,where the kinematic quantities are not calculated correctly. Min ratio is multiplied by 4 24 to get the number of data points required for a day (4 24 is the number of 15 minute intervals in a day) Parameters: Name Type Description Default df DataFrame DataFrame to filter required min_ratio float Minimum ratio of data points required for a day. Defaults to 0.25. 0.25 Returns: Name Type Description df DataFrame Filtered DataFrame Source code in ai/tilly/services/ml/transformations/preprocessing.py 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 @classmethod @log_pipeline def day_filter ( cls , df : pd . DataFrame , * , min_ratio : float = 0.25 ) -> pd . DataFrame : \"\"\"Filter out days with too few data points (days with less than min_ratio of the data points). This is done to avoid overfitting on days with too few data points,where the kinematic quantities are not calculated correctly. Min ratio is multiplied by 4*24 to get the number of data points required for a day (4*24 is the number of 15 minute intervals in a day) Args: df (pd.DataFrame): DataFrame to filter min_ratio (float, optional): Minimum ratio of data points required for a day. Defaults to 0.25. Returns: df (pd.DataFrame): Filtered DataFrame \"\"\" min_data_points_required = int ( min_ratio * ( 4 * 24 )) return df . groupby ( \"DATE\" ) . filter ( lambda x : len ( x ) >= min_data_points_required ) drop_outliers ( df , bounds : dict [ str , tuple [ float | None , float | None ]]) -> pd . DataFrame classmethod Drop rows where values are outside the given bounds. Parameters: Name Type Description Default df DataFrame DataFrame to filter required bounds dict [ str , tuple [ float | None, float | None]] Dictionary of column names and their lower and upper bounds. If a bound is None, then it is not applied. required Returns: Name Type Description df DataFrame Filtered DataFrame Source code in ai/tilly/services/ml/transformations/preprocessing.py 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 @classmethod @log_pipeline def drop_outliers ( cls , df , bounds : dict [ str , tuple [ float | None , float | None ]] ) -> pd . DataFrame : \"\"\"Drop rows where values are outside the given bounds. Args: df (pd.DataFrame): DataFrame to filter bounds (dict[str, tuple[float | None, float | None]]): Dictionary of column names and their lower and upper bounds. If a bound is None, then it is not applied. Returns: df (pd.DataFrame): Filtered DataFrame \"\"\" mask = np . ones ( df . shape [ 0 ], dtype = bool ) for col_name , ( lo , hi ) in bounds . items (): if lo is not None and hi is not None : mask &= ( df [ col_name ] . values >= lo ) & ( df [ col_name ] . values <= hi ) elif lo is not None : mask &= df [ col_name ] . values >= lo elif hi is not None : mask &= df [ col_name ] . values <= hi else : raise ValueError ( f \"Bounds for { col_name } are both None\" ) return df [ mask ] estimate_usage ( data : pd . DataFrame , usage_coeff = 2.1 , usage_min = 0.1 , usage_max = 0.4 ) -> float classmethod Estimates the usage of a given room from the union of - booked timeslots: Timeslots within a registered booking, and - Scheduled timeslots: TImeslots within the school schema. This is used as a prior for our anomaly detection model. Parameters: Name Type Description Default data DataFrame Timeslots required usage_coeff float Usage coefficient. Defaults to 2.1. This is a heuristic measure used to scale the estimated usage 2.1 usage_min float Minimum usage. Defaults to 0.1. This is a heuristic lower bound for the net estimated usage of a room. 0.1 usage_max float Maximum usage. Defaults to 0.4. This is a heuristic upper bound for the net estimated usage of a room. 0.4 Source code in ai/tilly/services/ml/transformations/preprocessing.py 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 @classmethod def estimate_usage ( cls , data : pd . DataFrame , usage_coeff = 2.1 , usage_min = 0.1 , usage_max = 0.4 ) -> float : \"\"\"Estimates the usage of a given room from the union of - booked timeslots: Timeslots within a registered booking, and - Scheduled timeslots: TImeslots within the school schema. This is used as a prior for our anomaly detection model. Args: data (pd.DataFrame): Timeslots usage_coeff (float, optional): Usage coefficient. Defaults to 2.1. This is a heuristic measure used to scale the estimated usage usage_min (float, optional): Minimum usage. Defaults to 0.1. This is a heuristic lower bound for the net estimated usage of a room. usage_max (float, optional): Maximum usage. Defaults to 0.4. This is a heuristic upper bound for the net estimated usage of a room. \"\"\" try : used_slots = ( data [ \"SKEMALAGT\" ] | data [ \"BOOKET\" ] . fillna ( False )) . sum () used_slots = max ( used_slots , usage_min ) return min ( usage_coeff * used_slots / len ( data ), usage_max ) except ZeroDivisionError : return \"auto\" featurize ( df : pd . DataFrame ) -> pd . DataFrame classmethod Run the full preprocessing flow on a DataFrame Source code in ai/tilly/services/ml/transformations/preprocessing.py 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 @classmethod @log_pipeline def featurize ( cls , df : pd . DataFrame ) -> pd . DataFrame : \"\"\"Run the full preprocessing flow on a DataFrame\"\"\" return ( df . pipe ( cls . merge_dt , date = \"DATE\" , time = \"TIME\" , name = \"DATETIME\" ) . pipe ( cls . add_missing_timeslots ) . pipe ( cls . interpolate_missing_islands , target_col = \"CO2\" , limit = 4 ) . pipe ( cls . remove_stagnate_intervals , target_col = \"CO2\" , threshold = 5 ) . dropna ( subset = [ \"CO2\" ]) . pipe ( cls . drop_outliers , bounds = { \"CO2\" : ( 1 , 8000 )}) . pipe ( cls . day_filter , min_ratio = 0.25 ) . pipe ( cls . apply_time_group_funcs , funcs = [ ( cls . gaussian_smooth , dict ( metric = \"CO2\" , std_dev = 2 )), ( cls . calculate_kinematic_quantities , dict ( metric = \"CO2_smoothed\" , window = 4 , prefix = \"CO2\" ), ), ], ) . pipe ( cls . add_time_features , night_start = 22 , night_end = 6 ) ) gaussian_smooth ( df , metric , * , std_dev = 2 ) classmethod Apply a gaussian filter to a given metric Source code in ai/tilly/services/ml/transformations/preprocessing.py 223 224 225 226 227 @classmethod def gaussian_smooth ( cls , df , metric , * , std_dev = 2 ): \"\"\"Apply a gaussian filter to a given metric\"\"\" df [ f \" { metric } _smoothed\" ] = gaussian_filter1d ( df [ metric ], sigma = std_dev ) return df interpolate_missing_islands ( df : pd . DataFrame , * , target_col : str = 'CO2' , limit : int = 3 , direction : str = 'forward' , method : str = 'cubic' , ** kwargs ) -> pd . DataFrame classmethod Interpolate missing values in a dataframe, but only for islands of missing values, ie. rows where there are no more than limit consecutive missing values in the target_col column. Source code in ai/tilly/services/ml/transformations/preprocessing.py 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 @classmethod @log_pipeline def interpolate_missing_islands ( cls , df : pd . DataFrame , * , target_col : str = \"CO2\" , limit : int = 3 , direction : str = \"forward\" , method : str = \"cubic\" , ** kwargs , ) -> pd . DataFrame : \"\"\"Interpolate missing values in a dataframe, but only for islands of missing values, ie. rows where there are no more than `limit` consecutive missing values in the `target_col` column.\"\"\" try : return df . assign ( CO2 = lambda d : d [ target_col ] . interpolate ( method = method , limit = limit , limit_direction = direction , ** kwargs , ) ) except ValueError : logger . warning ( f \"[ { df . iloc [ 0 ][ 'SKOLE_ID' ] } ] \" + \"Interpolation failed, falling back to linear\" ) return df . assign ( CO2 = lambda d : d [ target_col ] . interpolate ( method = \"linear\" , limit = limit , limit_direction = direction , ** kwargs , ) ) remove_stagnate_intervals ( df , target_col : str = 'CO2' , threshold = 4 ) -> pd . DataFrame classmethod Remove intervals where the CO2 value is the same for consecutive rows within time-contiguous blocks Source code in ai/tilly/services/ml/transformations/preprocessing.py 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 @classmethod @log_pipeline def remove_stagnate_intervals ( cls , df , target_col : str = \"CO2\" , threshold = 4 ) -> pd . DataFrame : \"\"\"Remove intervals where the CO2 value is the same for consecutive rows within time-contiguous blocks\"\"\" return ( df . assign ( time_diff = lambda d : d [ \"DATETIME\" ] . diff (), new_block = lambda d : ( d [ \"time_diff\" ] > pd . Timedelta ( minutes = 15 )) | ( d [ target_col ] != d [ target_col ] . shift ( 1 )), block_id = lambda d : d [ \"new_block\" ] . cumsum (), ) . assign ( block_count = lambda d : d . groupby ( \"block_id\" )[ \"block_id\" ] . transform ( \"count\" ) )[ lambda d : d [ \"block_count\" ] . lt ( threshold )] . drop ([ \"time_diff\" , \"new_block\" , \"block_id\" , \"block_count\" ], axis = 1 ) )","title":"Preprocessor"},{"location":"ai/reference/users/","text":"Users ai.tilly.users The users module handles all user-related functionality including authentication, user management, and initial user setup for the Tilly application. The module includes the following scripts auth.py : Contains utilities for user authentication, JWT strategy, and user management. Defines the UserManager class to handle custom behavior during user registration and implements FastAPI dependencies to get instances of user database and user manager. initial_users.py : A script that populates the database with initial users based on configuration. Handles exceptions for duplicate users and other integrity errors. schemas.py : Defines the schema classes that extend the FastAPI Users library schemas for reading, creating, and updating users. Each of these scripts plays a crucial role in managing user-related operations and ensures secure and consistent behavior throughout the application. Example Usage Import UserManager from auth to use in other modules for custom user behavior: from users.auth import UserManager Import schemas to validate user-related data: from users.schemas import UserCreate, UserRead Use create_initial_users function to populate initial users in the database: from users.initial_users import create_initial_users auth User Authentication Module for Tilly This module provides utilities for user authentication and session management using FastAPI and JWT (JSON Web Tokens). It uses SQLAlchemy to interact with the user database. Main Components get_user_db: An asynchronous function that yields the SQLAlchemyUserDatabase instance tied to the user model. UserManager: A class inheriting from BaseUserManager and UUIDIDMixin tha provides additional functionalities like handling events after user registration. It uses a secret token for both password reset and email verification. get_user_manager: An asynchronous function that yields an instance of UserManager. It depends on get_user_db. get_jwt_strategy: A function that returns an instance of JWTStrategy, with the secret and lifetime set. auth_backend: An instance of AuthenticationBackend, utilizing the JWT strategy and bearer token transport. fastapi_users: An instance of FastAPIUsers that is initialized with the UserManager and authentication backend. This is the core utility for user operations like login, registration, and more. current_active_user: A dependency that fetches the currently active user. Logging User-related events such as successful registration are logged using the loguru logger. Example from tilly.users.auth import fastapi_users, current_active_user Inside FastAPI route @app.get(\"/secure-route\", dependencies=[Depends(current_active_user)]) def secure_route(): return {\"message\": \"You have access to this route\"} initial_users Initial Users Module for Tilly This module provides utilities for creating initial users in the Tilly application. It makes use of FastAPI Users and SQLAlchemy for user creation and database interaction. Main Components get_async_session_context: An asynchronous context manager for getting an SQLAlchemy async session. get_user_db_context: An asynchronous context manager for getting the SQLAlchemy user database instance. get_user_manager_context: An asynchronous context manager for getting the UserManager instance. create_user: An asynchronous function responsible for creating a new user. It takes in the email, password, and superuser status as arguments. It also handles UserAlreadyExists exceptions and logs relevant information. create_initial_users: An asynchronous function that iterates through a list of user details and creates them. It catches and handles IntegrityError exceptions. Logging User creation events, as well as exceptions, are logged using the loguru logger. Example from tilly.users.initial_users import create_initial_users await create_initial_users([ {'email': 'admin@example.com', 'password': 'password', 'is_superuser': True}, {'email': 'user@example.com', 'password': 'password'} ]) create_initial_users ( users : list = USERS ) -> None async Asynchronously creates initial users based on the provided list. Parameters: Name Type Description Default users list A list of dictionaries containing user details like email, password, and is_superuser. Defaults to a list from the configuration. USERS Returns: Name Type Description None None Returns None but logs the user creation events. Raises: Type Description IntegrityError Catches and ignores the IntegrityError to ensure the Example await create_initial_users([ {'email': 'admin@example.com', 'password': 'password', 'is_superuser': True}, {'email': 'user@example.com', 'password': 'password'} ]) Source code in ai/tilly/users/initial_users.py 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 async def create_initial_users ( users : list = USERS ) -> None : \"\"\" Asynchronously creates initial users based on the provided list. Args: users (list, optional): A list of dictionaries containing user details like email, password, and is_superuser. Defaults to a list from the configuration. Returns: None: Returns None but logs the user creation events. Exceptions: IntegrityError: Catches and ignores the IntegrityError to ensure the rest of the users are still created. Example: >>> await create_initial_users([ >>> {'email': 'admin@example.com', 'password': 'password', >>> 'is_superuser': True}, >>> {'email': 'user@example.com', 'password': 'password'} >>> ]) \"\"\" for user in users : try : user = create_user ( user . get ( \"email\" ), user . get ( \"password\" ), user . get ( \"is_superuser\" , False ), ) await user except IntegrityError : pass create_user ( email : str , password : str , is_superuser : bool = False ) -> None async Asynchronously creates a new user. Parameters: Name Type Description Default email str The email of the user. required password str The password of the user. required is_superuser bool A flag that denotes if the user is a superuser. Defaults to False. False Returns: Name Type Description None None Returns None but logs the user creation event. Raises: Type Description UserAlreadyExists Logs an info-level message stating that the user already exists. Example await create_user('example@example.com', 'example_password', is_superuser=True) Source code in ai/tilly/users/initial_users.py 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 async def create_user ( email : str , password : str , is_superuser : bool = False , ) -> None : \"\"\" Asynchronously creates a new user. Args: email (str): The email of the user. password (str): The password of the user. is_superuser (bool, optional): A flag that denotes if the user is a superuser. Defaults to False. Returns: None: Returns None but logs the user creation event. Exceptions: UserAlreadyExists: Logs an info-level message stating that the user already exists. Example: >>> await create_user('example@example.com', 'example_password', >>> is_superuser=True) \"\"\" try : async with get_async_session_context () as session : async with get_user_db_context ( session ) as user_db : async with get_user_manager_context ( user_db ) as user_manager : user = await user_manager . create ( UserCreate ( email = email , password = password , is_superuser = is_superuser , ) ) logger . info ( f \"User created { user } \" ) except UserAlreadyExists : logger . info ( f \"User { email } already exists\" ) schemas This script defines the schema classes for user-related operations. These schemas extend the base user schemas provided by the FastAPI Users library. These classes are utilized by other modules for data validation and serialization when handling user-related operations such as registration, login, and updating user details. The script contains the following classes UserRead: Schema for reading user details. UserCreate: Schema for creating a new user. UserUpdate: Schema for updating an existing user's details. UserCreate Bases: BaseUserCreate Schema for creating a new user. Extends the BaseUserCreate schema provided by FastAPI Users. Example create_user = UserCreate(email=\"example@example.com\", password=\"example_password\") Source code in ai/tilly/users/schemas.py 34 35 36 37 38 39 40 41 42 43 44 45 46 47 class UserCreate ( schemas . BaseUserCreate ): \"\"\" Schema for creating a new user. Extends the BaseUserCreate schema provided by FastAPI Users. Attributes: Inherits all attributes from schemas.BaseUserCreate. Example: >>> create_user = UserCreate(email=\"example@example.com\", >>> password=\"example_password\") \"\"\" pass UserRead Bases: BaseUser [ UUID ] Schema for reading user details. Extends the BaseUser schema provided by FastAPI Users. Example read_user = UserRead(email=\"example@example.com\", id=uuid.UUID(\"some-uuid\")) Source code in ai/tilly/users/schemas.py 18 19 20 21 22 23 24 25 26 27 28 29 30 31 class UserRead ( schemas . BaseUser [ uuid . UUID ]): \"\"\" Schema for reading user details. Extends the BaseUser schema provided by FastAPI Users. Attributes: Inherits all attributes from schemas.BaseUser. Example: >>> read_user = UserRead(email=\"example@example.com\", >>> id=uuid.UUID(\"some-uuid\")) \"\"\" pass UserUpdate Bases: BaseUserUpdate Schema for updating an existing user's details. Extends the BaseUserUpdate schema provided by FastAPI Users. Example update_user = UserUpdate(email=\"new_example@example.com\") Source code in ai/tilly/users/schemas.py 50 51 52 53 54 55 56 57 58 59 60 61 62 class UserUpdate ( schemas . BaseUserUpdate ): \"\"\" Schema for updating an existing user's details. Extends the BaseUserUpdate schema provided by FastAPI Users. Attributes: Inherits all attributes from schemas.BaseUserUpdate. Example: >>> update_user = UserUpdate(email=\"new_example@example.com\") \"\"\" pass","title":"Users"},{"location":"ai/reference/users/#users","text":"","title":"Users"},{"location":"ai/reference/users/#ai.tilly.users","text":"The users module handles all user-related functionality including authentication, user management, and initial user setup for the Tilly application. The module includes the following scripts auth.py : Contains utilities for user authentication, JWT strategy, and user management. Defines the UserManager class to handle custom behavior during user registration and implements FastAPI dependencies to get instances of user database and user manager. initial_users.py : A script that populates the database with initial users based on configuration. Handles exceptions for duplicate users and other integrity errors. schemas.py : Defines the schema classes that extend the FastAPI Users library schemas for reading, creating, and updating users. Each of these scripts plays a crucial role in managing user-related operations and ensures secure and consistent behavior throughout the application. Example Usage Import UserManager from auth to use in other modules for custom user behavior: from users.auth import UserManager Import schemas to validate user-related data: from users.schemas import UserCreate, UserRead Use create_initial_users function to populate initial users in the database: from users.initial_users import create_initial_users","title":"users"},{"location":"ai/reference/users/#ai.tilly.users.auth","text":"User Authentication Module for Tilly This module provides utilities for user authentication and session management using FastAPI and JWT (JSON Web Tokens). It uses SQLAlchemy to interact with the user database. Main Components get_user_db: An asynchronous function that yields the SQLAlchemyUserDatabase instance tied to the user model. UserManager: A class inheriting from BaseUserManager and UUIDIDMixin tha provides additional functionalities like handling events after user registration. It uses a secret token for both password reset and email verification. get_user_manager: An asynchronous function that yields an instance of UserManager. It depends on get_user_db. get_jwt_strategy: A function that returns an instance of JWTStrategy, with the secret and lifetime set. auth_backend: An instance of AuthenticationBackend, utilizing the JWT strategy and bearer token transport. fastapi_users: An instance of FastAPIUsers that is initialized with the UserManager and authentication backend. This is the core utility for user operations like login, registration, and more. current_active_user: A dependency that fetches the currently active user. Logging User-related events such as successful registration are logged using the loguru logger. Example from tilly.users.auth import fastapi_users, current_active_user","title":"auth"},{"location":"ai/reference/users/#ai.tilly.users.auth--inside-fastapi-route","text":"@app.get(\"/secure-route\", dependencies=[Depends(current_active_user)]) def secure_route(): return {\"message\": \"You have access to this route\"}","title":"Inside FastAPI route"},{"location":"ai/reference/users/#ai.tilly.users.initial_users","text":"Initial Users Module for Tilly This module provides utilities for creating initial users in the Tilly application. It makes use of FastAPI Users and SQLAlchemy for user creation and database interaction. Main Components get_async_session_context: An asynchronous context manager for getting an SQLAlchemy async session. get_user_db_context: An asynchronous context manager for getting the SQLAlchemy user database instance. get_user_manager_context: An asynchronous context manager for getting the UserManager instance. create_user: An asynchronous function responsible for creating a new user. It takes in the email, password, and superuser status as arguments. It also handles UserAlreadyExists exceptions and logs relevant information. create_initial_users: An asynchronous function that iterates through a list of user details and creates them. It catches and handles IntegrityError exceptions. Logging User creation events, as well as exceptions, are logged using the loguru logger. Example from tilly.users.initial_users import create_initial_users await create_initial_users([ {'email': 'admin@example.com', 'password': 'password', 'is_superuser': True}, {'email': 'user@example.com', 'password': 'password'} ])","title":"initial_users"},{"location":"ai/reference/users/#ai.tilly.users.initial_users.create_initial_users","text":"Asynchronously creates initial users based on the provided list. Parameters: Name Type Description Default users list A list of dictionaries containing user details like email, password, and is_superuser. Defaults to a list from the configuration. USERS Returns: Name Type Description None None Returns None but logs the user creation events. Raises: Type Description IntegrityError Catches and ignores the IntegrityError to ensure the Example await create_initial_users([ {'email': 'admin@example.com', 'password': 'password', 'is_superuser': True}, {'email': 'user@example.com', 'password': 'password'} ]) Source code in ai/tilly/users/initial_users.py 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 async def create_initial_users ( users : list = USERS ) -> None : \"\"\" Asynchronously creates initial users based on the provided list. Args: users (list, optional): A list of dictionaries containing user details like email, password, and is_superuser. Defaults to a list from the configuration. Returns: None: Returns None but logs the user creation events. Exceptions: IntegrityError: Catches and ignores the IntegrityError to ensure the rest of the users are still created. Example: >>> await create_initial_users([ >>> {'email': 'admin@example.com', 'password': 'password', >>> 'is_superuser': True}, >>> {'email': 'user@example.com', 'password': 'password'} >>> ]) \"\"\" for user in users : try : user = create_user ( user . get ( \"email\" ), user . get ( \"password\" ), user . get ( \"is_superuser\" , False ), ) await user except IntegrityError : pass","title":"create_initial_users()"},{"location":"ai/reference/users/#ai.tilly.users.initial_users.create_user","text":"Asynchronously creates a new user. Parameters: Name Type Description Default email str The email of the user. required password str The password of the user. required is_superuser bool A flag that denotes if the user is a superuser. Defaults to False. False Returns: Name Type Description None None Returns None but logs the user creation event. Raises: Type Description UserAlreadyExists Logs an info-level message stating that the user already exists. Example await create_user('example@example.com', 'example_password', is_superuser=True) Source code in ai/tilly/users/initial_users.py 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 async def create_user ( email : str , password : str , is_superuser : bool = False , ) -> None : \"\"\" Asynchronously creates a new user. Args: email (str): The email of the user. password (str): The password of the user. is_superuser (bool, optional): A flag that denotes if the user is a superuser. Defaults to False. Returns: None: Returns None but logs the user creation event. Exceptions: UserAlreadyExists: Logs an info-level message stating that the user already exists. Example: >>> await create_user('example@example.com', 'example_password', >>> is_superuser=True) \"\"\" try : async with get_async_session_context () as session : async with get_user_db_context ( session ) as user_db : async with get_user_manager_context ( user_db ) as user_manager : user = await user_manager . create ( UserCreate ( email = email , password = password , is_superuser = is_superuser , ) ) logger . info ( f \"User created { user } \" ) except UserAlreadyExists : logger . info ( f \"User { email } already exists\" )","title":"create_user()"},{"location":"ai/reference/users/#ai.tilly.users.schemas","text":"This script defines the schema classes for user-related operations. These schemas extend the base user schemas provided by the FastAPI Users library. These classes are utilized by other modules for data validation and serialization when handling user-related operations such as registration, login, and updating user details. The script contains the following classes UserRead: Schema for reading user details. UserCreate: Schema for creating a new user. UserUpdate: Schema for updating an existing user's details.","title":"schemas"},{"location":"ai/reference/users/#ai.tilly.users.schemas.UserCreate","text":"Bases: BaseUserCreate Schema for creating a new user. Extends the BaseUserCreate schema provided by FastAPI Users. Example create_user = UserCreate(email=\"example@example.com\", password=\"example_password\") Source code in ai/tilly/users/schemas.py 34 35 36 37 38 39 40 41 42 43 44 45 46 47 class UserCreate ( schemas . BaseUserCreate ): \"\"\" Schema for creating a new user. Extends the BaseUserCreate schema provided by FastAPI Users. Attributes: Inherits all attributes from schemas.BaseUserCreate. Example: >>> create_user = UserCreate(email=\"example@example.com\", >>> password=\"example_password\") \"\"\" pass","title":"UserCreate"},{"location":"ai/reference/users/#ai.tilly.users.schemas.UserRead","text":"Bases: BaseUser [ UUID ] Schema for reading user details. Extends the BaseUser schema provided by FastAPI Users. Example read_user = UserRead(email=\"example@example.com\", id=uuid.UUID(\"some-uuid\")) Source code in ai/tilly/users/schemas.py 18 19 20 21 22 23 24 25 26 27 28 29 30 31 class UserRead ( schemas . BaseUser [ uuid . UUID ]): \"\"\" Schema for reading user details. Extends the BaseUser schema provided by FastAPI Users. Attributes: Inherits all attributes from schemas.BaseUser. Example: >>> read_user = UserRead(email=\"example@example.com\", >>> id=uuid.UUID(\"some-uuid\")) \"\"\" pass","title":"UserRead"},{"location":"ai/reference/users/#ai.tilly.users.schemas.UserUpdate","text":"Bases: BaseUserUpdate Schema for updating an existing user's details. Extends the BaseUserUpdate schema provided by FastAPI Users. Example update_user = UserUpdate(email=\"new_example@example.com\") Source code in ai/tilly/users/schemas.py 50 51 52 53 54 55 56 57 58 59 60 61 62 class UserUpdate ( schemas . BaseUserUpdate ): \"\"\" Schema for updating an existing user's details. Extends the BaseUserUpdate schema provided by FastAPI Users. Attributes: Inherits all attributes from schemas.BaseUserUpdate. Example: >>> update_user = UserUpdate(email=\"new_example@example.com\") \"\"\" pass","title":"UserUpdate"},{"location":"analytics/","text":"Introduction The analytics part of the project has tried to innovate on how the available data can be used to show insights on energy usage as well as providing guidance in optimizing this usage. The project has in broad terms setup the following functionalities: Dataload via pipelines or APIs Storage in raw format in Azure Storage Accounts and processed data in Snowflake DB Transformations via Snowflake SQL Dashboard and insights via frontend in Power Bi Architecture The architecture has been built on a hybrid-approach by using an existing dataplatform provided by NTT DATA Business Solutions and furthermore developing specific functionalities such as transformations and analytics. The architecture is therefore split into 2 parts: * Existing dataplatform (Not open-soruced) * Specific functionalities (Open-source) In the below diagram an overview of the general architecture can be seen with the blue box telling how it is documented in this repo.","title":"Introduction"},{"location":"analytics/#introduction","text":"The analytics part of the project has tried to innovate on how the available data can be used to show insights on energy usage as well as providing guidance in optimizing this usage. The project has in broad terms setup the following functionalities: Dataload via pipelines or APIs Storage in raw format in Azure Storage Accounts and processed data in Snowflake DB Transformations via Snowflake SQL Dashboard and insights via frontend in Power Bi","title":"Introduction"},{"location":"analytics/#architecture","text":"The architecture has been built on a hybrid-approach by using an existing dataplatform provided by NTT DATA Business Solutions and furthermore developing specific functionalities such as transformations and analytics. The architecture is therefore split into 2 parts: * Existing dataplatform (Not open-soruced) * Specific functionalities (Open-source) In the below diagram an overview of the general architecture can be seen with the blue box telling how it is documented in this repo.","title":"Architecture"},{"location":"analytics/frontend/frontend/","text":"Configuration The frontend can be setup locally by downloading and using the template (.pbit) Setup Download the Power Bi Template file Import from template Insert Snowflake Server link Prerequistes Snowflake Server Role: GOVTECH_DB_OWNER Warehouse: GOVTECH_ANALYST_WH The tables as they are setup in transformations Alternative to Snowflake If you want to use another Database the snowflake, you need to change the data connections in Power Bi. Go into Advanced Editor in Transform Data and change to another Database. Example: let Source = Snowflake.Databases(Server,\"GOVTECH_ANALYST_WH\",[Role=\"GOVTECH_DB_OWNER\"]), GOVTECH_DB_Database = Source{[Name=\"GOVTECH_DB\",Kind=\"Database\"]}[Data], RAW_Schema = GOVTECH_DB_Database{[Name=\"RAW\",Kind=\"Schema\"]}[Data], #\"4_FEATURIZ_ENERGIDATA_Table\" = RAW_Schema{[Name=\"4_FEATURIZ_ENERGIDATA\",Kind=\"Table\"]}[Data], #\"Renamed Columns\" = Table.RenameColumns(#\"4_FEATURIZ_ENERGIDATA_Table\",{{\"VALUE::FLOAT\", \"VALUE\"}}) in #\"Renamed Columns\" Datamodel The datamodel for the Power Bi dashboards are shown in both a figure and a table. Relationship Id Left side Cardinality Right side 537 '4_FEATURIZ_DATOER'[TIMESTAMP] 1 --> M '4_FEATURIZ_ENERGIDATA'[TIMESTAMP] 538 '4_FEATURIZ_DATOER'[TIMESTAMP] 1 --> M '4_FEATURIZ_DMI'[TIMESTAMP] 539 '4_FEATURIZ_LOKALER'[KOMMUNE] M --> M '4_FEATURIZ_ENERGIDATA'[KOMMUNE] 540 '4_FEATURIZ_LOKALER'[KOMMUNE] M --> M '4_FEATURIZ_DMI'[KOMMUNE] 541 '4_FEATURIZ_LOKALER'[KOMMUNE_LOKALE] 1 --> M '4_FEATURIZ_BOOKINGS_TIME'[KOMMUNE_LOKALE] 542 '4_FEATURIZ_DATOER'[TIMESTAMP] 1 --> M '4_FEATURIZ_BOOKINGS_TIME'[TIMESTAMP] 543 '4_FEATURIZ_DATOER'[TIMESTAMP] 1 --> M '4_FEATURIZ_CTS_X_IOT'[TIMESTAMP] 544 '4_FEATURIZ_LOKALER'[KOMMUNE_LOKALE] 1 --> M '4_FEATURIZ_CTS_X_IOT'[KOMMUNE_LOKALE] 545 '4_FEATURIZ_LOKALER'[KOMMUNE_LOKALE] 1 --> M '4_FEATURIZ_DRIFTOPTIMERINGSMODEL'[KOMMUNE_LOKALE] 546 '4_FEATURIZ_DATOER'[TIMESTAMP] 1 --> M '4_FEATURIZ_DRIFTOPTIMERINGSMODEL'[DATETIME] 547 '4_FEATURIZ_PASSIVTIMER'[KOMMUNE_DATO_TIME] 1 --> M '4_FEATURIZ_ENERGIDATA'[KOMMUNE_DATO_TIME] 548 '1_RAW_KOMMUNER_SKOLER'[KOMMUNE] 1 --> M '4_FEATURIZ_LOKALER'[KOMMUNE] 549 '4_FEATURIZ_PASSIVTIMER'[KOMMUNE_DATO_TIME] 1 --> M '4_FEATURIZ_ENERGIDATA_OPTIMIZED'[KOMMUNE_DATO_TIME] 550 '4_FEATURIZ_LOKALER'[KOMMUNE] M --> M '4_FEATURIZ_ENERGIDATA_OPTIMIZED'[KOMMUNE] 551 '4_FEATURIZ_DATOER'[TIMESTAMP] 1 --> M '4_FEATURIZ_ENERGIDATA_OPTIMIZED'[TIMESTAMP] 552 '1_RAW_KOMMUNER_SKOLER'[KOMMUNE] 1 --> M '4_FEATURIZ_IDEALKURVE'[KOMMUNE] 553 '1_RAW_KOMMUNER_SKOLER'[KOMMUNE] 1 --> M '4_FEATURIZ_BENCHMARK_WEEKEND_MANUEL'[KOMMUNE] 554 '4_FEATURIZ_DATOER'[MONTHNO] M --> M 'Freml\u00f8bstemperaturer'[M\u00e5nednr] 555 '4_FEATURIZ_PASSIVTIMER'[KOMMUNE] M --> M '4_FEATURIZ_MINIMUM_EL'[KOMMUNE] Pages The built dashboards/pages can be seen in examples below. Overview Energy Utilization Year Vacations Building Utilization Ideal Curve Vacation Closing Benchmark with last year Benchmark with weekend Benchmark with target Passive Energy Usage Return Temperature Cooling Room Utilization Booking utilization General utilization","title":"Configuration"},{"location":"analytics/frontend/frontend/#configuration","text":"The frontend can be setup locally by downloading and using the template (.pbit)","title":"Configuration"},{"location":"analytics/frontend/frontend/#setup","text":"Download the Power Bi Template file Import from template Insert Snowflake Server link","title":"Setup"},{"location":"analytics/frontend/frontend/#prerequistes","text":"Snowflake Server Role: GOVTECH_DB_OWNER Warehouse: GOVTECH_ANALYST_WH The tables as they are setup in transformations","title":"Prerequistes"},{"location":"analytics/frontend/frontend/#alternative-to-snowflake","text":"If you want to use another Database the snowflake, you need to change the data connections in Power Bi. Go into Advanced Editor in Transform Data and change to another Database. Example: let Source = Snowflake.Databases(Server,\"GOVTECH_ANALYST_WH\",[Role=\"GOVTECH_DB_OWNER\"]), GOVTECH_DB_Database = Source{[Name=\"GOVTECH_DB\",Kind=\"Database\"]}[Data], RAW_Schema = GOVTECH_DB_Database{[Name=\"RAW\",Kind=\"Schema\"]}[Data], #\"4_FEATURIZ_ENERGIDATA_Table\" = RAW_Schema{[Name=\"4_FEATURIZ_ENERGIDATA\",Kind=\"Table\"]}[Data], #\"Renamed Columns\" = Table.RenameColumns(#\"4_FEATURIZ_ENERGIDATA_Table\",{{\"VALUE::FLOAT\", \"VALUE\"}}) in #\"Renamed Columns\"","title":"Alternative to Snowflake"},{"location":"analytics/frontend/frontend/#datamodel","text":"The datamodel for the Power Bi dashboards are shown in both a figure and a table. Relationship Id Left side Cardinality Right side 537 '4_FEATURIZ_DATOER'[TIMESTAMP] 1 --> M '4_FEATURIZ_ENERGIDATA'[TIMESTAMP] 538 '4_FEATURIZ_DATOER'[TIMESTAMP] 1 --> M '4_FEATURIZ_DMI'[TIMESTAMP] 539 '4_FEATURIZ_LOKALER'[KOMMUNE] M --> M '4_FEATURIZ_ENERGIDATA'[KOMMUNE] 540 '4_FEATURIZ_LOKALER'[KOMMUNE] M --> M '4_FEATURIZ_DMI'[KOMMUNE] 541 '4_FEATURIZ_LOKALER'[KOMMUNE_LOKALE] 1 --> M '4_FEATURIZ_BOOKINGS_TIME'[KOMMUNE_LOKALE] 542 '4_FEATURIZ_DATOER'[TIMESTAMP] 1 --> M '4_FEATURIZ_BOOKINGS_TIME'[TIMESTAMP] 543 '4_FEATURIZ_DATOER'[TIMESTAMP] 1 --> M '4_FEATURIZ_CTS_X_IOT'[TIMESTAMP] 544 '4_FEATURIZ_LOKALER'[KOMMUNE_LOKALE] 1 --> M '4_FEATURIZ_CTS_X_IOT'[KOMMUNE_LOKALE] 545 '4_FEATURIZ_LOKALER'[KOMMUNE_LOKALE] 1 --> M '4_FEATURIZ_DRIFTOPTIMERINGSMODEL'[KOMMUNE_LOKALE] 546 '4_FEATURIZ_DATOER'[TIMESTAMP] 1 --> M '4_FEATURIZ_DRIFTOPTIMERINGSMODEL'[DATETIME] 547 '4_FEATURIZ_PASSIVTIMER'[KOMMUNE_DATO_TIME] 1 --> M '4_FEATURIZ_ENERGIDATA'[KOMMUNE_DATO_TIME] 548 '1_RAW_KOMMUNER_SKOLER'[KOMMUNE] 1 --> M '4_FEATURIZ_LOKALER'[KOMMUNE] 549 '4_FEATURIZ_PASSIVTIMER'[KOMMUNE_DATO_TIME] 1 --> M '4_FEATURIZ_ENERGIDATA_OPTIMIZED'[KOMMUNE_DATO_TIME] 550 '4_FEATURIZ_LOKALER'[KOMMUNE] M --> M '4_FEATURIZ_ENERGIDATA_OPTIMIZED'[KOMMUNE] 551 '4_FEATURIZ_DATOER'[TIMESTAMP] 1 --> M '4_FEATURIZ_ENERGIDATA_OPTIMIZED'[TIMESTAMP] 552 '1_RAW_KOMMUNER_SKOLER'[KOMMUNE] 1 --> M '4_FEATURIZ_IDEALKURVE'[KOMMUNE] 553 '1_RAW_KOMMUNER_SKOLER'[KOMMUNE] 1 --> M '4_FEATURIZ_BENCHMARK_WEEKEND_MANUEL'[KOMMUNE] 554 '4_FEATURIZ_DATOER'[MONTHNO] M --> M 'Freml\u00f8bstemperaturer'[M\u00e5nednr] 555 '4_FEATURIZ_PASSIVTIMER'[KOMMUNE] M --> M '4_FEATURIZ_MINIMUM_EL'[KOMMUNE]","title":"Datamodel"},{"location":"analytics/frontend/frontend/#pages","text":"The built dashboards/pages can be seen in examples below.","title":"Pages"},{"location":"analytics/frontend/frontend/#overview","text":"","title":"Overview"},{"location":"analytics/frontend/frontend/#energy-utilization","text":"","title":"Energy Utilization"},{"location":"analytics/frontend/frontend/#year","text":"","title":"Year"},{"location":"analytics/frontend/frontend/#vacations","text":"","title":"Vacations"},{"location":"analytics/frontend/frontend/#building-utilization","text":"","title":"Building Utilization"},{"location":"analytics/frontend/frontend/#ideal-curve","text":"","title":"Ideal Curve"},{"location":"analytics/frontend/frontend/#vacation-closing","text":"","title":"Vacation Closing"},{"location":"analytics/frontend/frontend/#benchmark-with-last-year","text":"","title":"Benchmark with last year"},{"location":"analytics/frontend/frontend/#benchmark-with-weekend","text":"","title":"Benchmark with weekend"},{"location":"analytics/frontend/frontend/#benchmark-with-target","text":"","title":"Benchmark with target"},{"location":"analytics/frontend/frontend/#passive-energy-usage","text":"","title":"Passive Energy Usage"},{"location":"analytics/frontend/frontend/#return-temperature","text":"","title":"Return Temperature"},{"location":"analytics/frontend/frontend/#cooling","text":"","title":"Cooling"},{"location":"analytics/frontend/frontend/#room-utilization","text":"","title":"Room Utilization"},{"location":"analytics/frontend/frontend/#booking-utilization","text":"","title":"Booking utilization"},{"location":"analytics/frontend/frontend/#general-utilization","text":"","title":"General utilization"},{"location":"analytics/pipelines/pipelines/","text":"Azure Data Factory The pipelines for fetching and storing data as well as orchestrating the load and persisting of both raw data as well as featurized tabels used for reportin are all built in Azure Data Factory. In the folders the individual pipelines for each datasource are put. The master pipeline for orchistrating and updating data are organized in the Master-folder. Setup If setup is wanted on a seperate instace of ADF it is needed to: Download the folder of the needed pipeline Zip it Create new pipeline --> Import from template Attach the needed Linked Services as they are described The UI for using a template: Output / Storage All the pipelines are sending their raw files in the Copy activites to a set Storage Account. All updates of raw and featurized tables are setup in a Snowflake Instance. This can be switched to any Database that are ADF-compatiable. Security All API-keys, credentials etc. are stored in an Azure Key Vault.","title":"Azure Data Factory"},{"location":"analytics/pipelines/pipelines/#azure-data-factory","text":"The pipelines for fetching and storing data as well as orchestrating the load and persisting of both raw data as well as featurized tabels used for reportin are all built in Azure Data Factory. In the folders the individual pipelines for each datasource are put. The master pipeline for orchistrating and updating data are organized in the Master-folder.","title":"Azure Data Factory"},{"location":"analytics/pipelines/pipelines/#setup","text":"If setup is wanted on a seperate instace of ADF it is needed to: Download the folder of the needed pipeline Zip it Create new pipeline --> Import from template Attach the needed Linked Services as they are described The UI for using a template:","title":"Setup"},{"location":"analytics/pipelines/pipelines/#output-storage","text":"All the pipelines are sending their raw files in the Copy activites to a set Storage Account. All updates of raw and featurized tables are setup in a Snowflake Instance. This can be switched to any Database that are ADF-compatiable.","title":"Output / Storage"},{"location":"analytics/pipelines/pipelines/#security","text":"All API-keys, credentials etc. are stored in an Azure Key Vault.","title":"Security"},{"location":"analytics/pipelines/bookings/bookings_pipeline/","text":"Pipeline The following pipeline have been setup in Azure Data Factory: Config The bookings pipeline needs 3 linked services setup. An example with Favrskov's pipeline is shown below: \"requires\" : { \"linkedservices\" : { \"LS_BLOB\" : { \"supportTypes\" : [ \"AzureBlobStorage\" ] }, \"LS_SNOWFLAKE\" : { \"supportTypes\" : [ \"Snowflake\" ] }, \"LS_FAVRSKOV_BOOKING_API\" : { \"supportTypes\" : [ \"RestService\" ] } } }","title":"Pipeline"},{"location":"analytics/pipelines/bookings/bookings_pipeline/#pipeline","text":"The following pipeline have been setup in Azure Data Factory:","title":"Pipeline"},{"location":"analytics/pipelines/bookings/bookings_pipeline/#config","text":"The bookings pipeline needs 3 linked services setup. An example with Favrskov's pipeline is shown below: \"requires\" : { \"linkedservices\" : { \"LS_BLOB\" : { \"supportTypes\" : [ \"AzureBlobStorage\" ] }, \"LS_SNOWFLAKE\" : { \"supportTypes\" : [ \"Snowflake\" ] }, \"LS_FAVRSKOV_BOOKING_API\" : { \"supportTypes\" : [ \"RestService\" ] } } }","title":"Config"},{"location":"analytics/pipelines/driftoptimeringsmodel/driftoptimering_pipeline/","text":"Pipeline The AI-model is activated by two different endpoints. The first is the training, that is orchestrated from this pipeline. This pipelines is therefore responsible for training the AI-model \"Driftoptimeringsmodel\" once a month. The prediction are done during runtime in the master pipeline. Config The pipelines needs an endpoint for the model as well as connection to a database/snowflake: \"requires\" : { \"linkedservices\" : { \"LS_Driftoptimeringsmodel\" : { \"supportTypes\" : [ \"HttpServer\" ] }, \"LS_SNOWFLAKE\" : { \"supportTypes\" : [ \"Snowflake\" ] } } }","title":"Pipeline"},{"location":"analytics/pipelines/driftoptimeringsmodel/driftoptimering_pipeline/#pipeline","text":"The AI-model is activated by two different endpoints. The first is the training, that is orchestrated from this pipeline. This pipelines is therefore responsible for training the AI-model \"Driftoptimeringsmodel\" once a month. The prediction are done during runtime in the master pipeline.","title":"Pipeline"},{"location":"analytics/pipelines/driftoptimeringsmodel/driftoptimering_pipeline/#config","text":"The pipelines needs an endpoint for the model as well as connection to a database/snowflake: \"requires\" : { \"linkedservices\" : { \"LS_Driftoptimeringsmodel\" : { \"supportTypes\" : [ \"HttpServer\" ] }, \"LS_SNOWFLAKE\" : { \"supportTypes\" : [ \"Snowflake\" ] } } }","title":"Config"},{"location":"analytics/pipelines/energy/energy_pipeline/","text":"Pipeline This pipeline is responsible for gathering energy consumption. Firstly it fetches the credentials/config for getting an access token for the API. The token is then fetched and stored in a variable. The wanted tags are then fetched from a config and looped through. In the loop the last load date from the databased is looked up and the API is then called to get and save the newest data. Finally the raw table in the database is triggered to update. For Aarhus the return temperatures and Energy consumption are on two different endpoints: Config The energy pipeline needs 3 linked services setup. An example with Favrskov's pipeline is shown below: \"requires\" : { \"linkedservices\" : { \"LS_BLOB\" : { \"supportTypes\" : [ \"AzureBlobStorage\" ] }, \"LS_SNOWFLAKE\" : { \"supportTypes\" : [ \"Snowflake\" ] }, \"LS_ENERGI_FAVRSKOV\" : { \"supportTypes\" : [ \"RestService\" ] } } }","title":"Pipeline"},{"location":"analytics/pipelines/energy/energy_pipeline/#pipeline","text":"This pipeline is responsible for gathering energy consumption. Firstly it fetches the credentials/config for getting an access token for the API. The token is then fetched and stored in a variable. The wanted tags are then fetched from a config and looped through. In the loop the last load date from the databased is looked up and the API is then called to get and save the newest data. Finally the raw table in the database is triggered to update. For Aarhus the return temperatures and Energy consumption are on two different endpoints:","title":"Pipeline"},{"location":"analytics/pipelines/energy/energy_pipeline/#config","text":"The energy pipeline needs 3 linked services setup. An example with Favrskov's pipeline is shown below: \"requires\" : { \"linkedservices\" : { \"LS_BLOB\" : { \"supportTypes\" : [ \"AzureBlobStorage\" ] }, \"LS_SNOWFLAKE\" : { \"supportTypes\" : [ \"Snowflake\" ] }, \"LS_ENERGI_FAVRSKOV\" : { \"supportTypes\" : [ \"RestService\" ] } } }","title":"Config"},{"location":"analytics/pipelines/iot/iot_pipeline/","text":"Pipeline This pipeline fetches IoT Data from Syddjurs. It firstly gathers the config for the credentials to fetch an access token. After the token is obtained it looks up the last load date of IoT Data. It the proceeds to call the API for IoT Data from the last load date until today. Finally it updates the raw table in the database Config The IoT pipeline needs 3 linked services setup. An example with Favrskov's pipeline is shown below: \"requires\" : { \"linkedservices\" : { \"LS_SNOWFLAKE\" : { \"supportTypes\" : [ \"Snowflake\" ] }, \"LS_IOT_SYDDJURS\" : { \"supportTypes\" : [ \"RestService\" ] }, \"LS_BLOB\" : { \"supportTypes\" : [ \"AzureBlobStorage\" ] } } }","title":"Pipeline"},{"location":"analytics/pipelines/iot/iot_pipeline/#pipeline","text":"This pipeline fetches IoT Data from Syddjurs. It firstly gathers the config for the credentials to fetch an access token. After the token is obtained it looks up the last load date of IoT Data. It the proceeds to call the API for IoT Data from the last load date until today. Finally it updates the raw table in the database","title":"Pipeline"},{"location":"analytics/pipelines/iot/iot_pipeline/#config","text":"The IoT pipeline needs 3 linked services setup. An example with Favrskov's pipeline is shown below: \"requires\" : { \"linkedservices\" : { \"LS_SNOWFLAKE\" : { \"supportTypes\" : [ \"Snowflake\" ] }, \"LS_IOT_SYDDJURS\" : { \"supportTypes\" : [ \"RestService\" ] }, \"LS_BLOB\" : { \"supportTypes\" : [ \"AzureBlobStorage\" ] } } }","title":"Config"},{"location":"analytics/pipelines/master/master_pipeline/","text":"Pipeline This pipeline is the main pipeline and orchestrator for the collection and storing of new data. It firstly updates the date table in datebase to always contain dates until today. Then it calls all the individual pipelines for fetching data. It then runs a number of stored procedures to update the persisted tables that are being used for e.g., BI/Frontend. Finally it runs the AI-model for predicting room usage before also updating that persisted table of output from the model. Config The pipeline requires all the needed configuration of linked services: \"requires\" : { \"linkedservices\" : { \"LS_BLOB\" : { \"supportTypes\" : [ \"AzureBlobStorage\" ] }, \"LS_SNOWFLAKE\" : { \"supportTypes\" : [ \"Snowflake\" ] }, \"LS_Driftoptimeringsmodel\" : { \"supportTypes\" : [ \"HttpServer\" ] }, \"LS_BOOKINGS_SYDDJURS\" : { \"supportTypes\" : [ \"RestService\" ] }, \"LS_ENERGI_AARHUS\" : { \"supportTypes\" : [ \"RestService\" ] }, \"LS_IOT_SYDDJURS\" : { \"supportTypes\" : [ \"RestService\" ] }, \"LS_FAVRSKOV_BOOKING_API\" : { \"supportTypes\" : [ \"RestService\" ] }, \"LS_ENERGI_FAVRSKOV2\" : { \"supportTypes\" : [ \"RestService\" ] }, \"LS_DMI\" : { \"supportTypes\" : [ \"RestService\" ] } } }","title":"Pipeline"},{"location":"analytics/pipelines/master/master_pipeline/#pipeline","text":"This pipeline is the main pipeline and orchestrator for the collection and storing of new data. It firstly updates the date table in datebase to always contain dates until today. Then it calls all the individual pipelines for fetching data. It then runs a number of stored procedures to update the persisted tables that are being used for e.g., BI/Frontend. Finally it runs the AI-model for predicting room usage before also updating that persisted table of output from the model.","title":"Pipeline"},{"location":"analytics/pipelines/master/master_pipeline/#config","text":"The pipeline requires all the needed configuration of linked services: \"requires\" : { \"linkedservices\" : { \"LS_BLOB\" : { \"supportTypes\" : [ \"AzureBlobStorage\" ] }, \"LS_SNOWFLAKE\" : { \"supportTypes\" : [ \"Snowflake\" ] }, \"LS_Driftoptimeringsmodel\" : { \"supportTypes\" : [ \"HttpServer\" ] }, \"LS_BOOKINGS_SYDDJURS\" : { \"supportTypes\" : [ \"RestService\" ] }, \"LS_ENERGI_AARHUS\" : { \"supportTypes\" : [ \"RestService\" ] }, \"LS_IOT_SYDDJURS\" : { \"supportTypes\" : [ \"RestService\" ] }, \"LS_FAVRSKOV_BOOKING_API\" : { \"supportTypes\" : [ \"RestService\" ] }, \"LS_ENERGI_FAVRSKOV2\" : { \"supportTypes\" : [ \"RestService\" ] }, \"LS_DMI\" : { \"supportTypes\" : [ \"RestService\" ] } } }","title":"Config"},{"location":"analytics/pipelines/weather/weather_pipeline/","text":"Pipeline This pipeline fetches the wanted stations from the municipalities from the config-file and saves them in an array. It also fetches the API-key from the Azure Keyvault. It then loops through all stations, gets the last load date and executes the pipelines fetching and saving the observations. Finally, it updates the raw table in the database by a stored procedure. The loop in \"PL_DMI_MASTER\" starts a sub-pipeline \"PL_DMI\" that fetched the wanted parameters from a config a fetches the latest data API: https://confluence.govcloud.dk/display/FDAPI/Meteorological+Observation+Data Master Sub-pipeline Config \"requires\" : { \"linkedservices\" : { \"LS_BLOB\" : { \"supportTypes\" : [ \"AzureBlobStorage\" ] }, \"LS_SNOWFLAKE\" : { \"supportTypes\" : [ \"Snowflake\" ] }, \"LS_DMI\" : { \"supportTypes\" : [ \"RestService\" ] } } }","title":"Pipeline"},{"location":"analytics/pipelines/weather/weather_pipeline/#pipeline","text":"This pipeline fetches the wanted stations from the municipalities from the config-file and saves them in an array. It also fetches the API-key from the Azure Keyvault. It then loops through all stations, gets the last load date and executes the pipelines fetching and saving the observations. Finally, it updates the raw table in the database by a stored procedure. The loop in \"PL_DMI_MASTER\" starts a sub-pipeline \"PL_DMI\" that fetched the wanted parameters from a config a fetches the latest data API: https://confluence.govcloud.dk/display/FDAPI/Meteorological+Observation+Data","title":"Pipeline"},{"location":"analytics/pipelines/weather/weather_pipeline/#master","text":"","title":"Master"},{"location":"analytics/pipelines/weather/weather_pipeline/#sub-pipeline","text":"","title":"Sub-pipeline"},{"location":"analytics/pipelines/weather/weather_pipeline/#config","text":"\"requires\" : { \"linkedservices\" : { \"LS_BLOB\" : { \"supportTypes\" : [ \"AzureBlobStorage\" ] }, \"LS_SNOWFLAKE\" : { \"supportTypes\" : [ \"Snowflake\" ] }, \"LS_DMI\" : { \"supportTypes\" : [ \"RestService\" ] } } }","title":"Config"},{"location":"analytics/storage/overview/","text":"Overview There have been used 3 main components to store informations, data and files. Storage Account All raw files that is being fetched from API's and has been collected as historical data are stored in an Azure Storage Account. In this storage account there has been setup a folder structure dividing files into their types and which municipality they are belonging to. Furthermore the storage account also contains the config-files for the pipelines. An example/template of the Azure Ressource can be seen in the template json-file. Snowflake The main datastorage for both data in raw tables and also processed data is a Snowflake database. The snowflake database has been setup and hosted through the existing dataplatform. Therefore it would be necessary to setup a new instance to do run the transformations and store data on a municipality specific environment. Azure Key Vault An Azure Key Vault has been used to store all credentials for e.g. API's securely. An example/template of the Azure Ressource can be seen in the template json-file.","title":"Overview"},{"location":"analytics/storage/overview/#overview","text":"There have been used 3 main components to store informations, data and files.","title":"Overview"},{"location":"analytics/storage/overview/#storage-account","text":"All raw files that is being fetched from API's and has been collected as historical data are stored in an Azure Storage Account. In this storage account there has been setup a folder structure dividing files into their types and which municipality they are belonging to. Furthermore the storage account also contains the config-files for the pipelines. An example/template of the Azure Ressource can be seen in the template json-file.","title":"Storage Account"},{"location":"analytics/storage/overview/#snowflake","text":"The main datastorage for both data in raw tables and also processed data is a Snowflake database. The snowflake database has been setup and hosted through the existing dataplatform. Therefore it would be necessary to setup a new instance to do run the transformations and store data on a municipality specific environment.","title":"Snowflake"},{"location":"analytics/storage/overview/#azure-key-vault","text":"An Azure Key Vault has been used to store all credentials for e.g. API's securely. An example/template of the Azure Ressource can be seen in the template json-file.","title":"Azure Key Vault"},{"location":"analytics/transformations/transformations/","text":"Tranformations The dataload and transformation layer are built on layer-diveded principle. All data sources are as a standard handled in 4 layers: 1_RAW 2_STANDARD 3_CLEANSED 4_FEATURIZ The project have both loaded and transformed data from individual municipalities as well as joined the sources into standardized tabels for cohorent usage and scaleability. Data load The data load is done by integrating raw tables in Snowflake with the Storage Account setup in Azure. This has been done by using stages that fetches the file from the individual folders in the storage account. Specific & General transformations The specific transformations for the local datasources of each municipality is divided into folders. These transformations contains the ETL-scripts (with inline-comments) used for taking the sources from anywhere between level 1 - 3 of the database layers. From there the transformations seeks to transform the individual municipalities data into a unified schema/table. These scripts are both commented and described in the general folder. In there one may also find the final suggested schemas for each data source.","title":"Tranformations"},{"location":"analytics/transformations/transformations/#tranformations","text":"The dataload and transformation layer are built on layer-diveded principle. All data sources are as a standard handled in 4 layers: 1_RAW 2_STANDARD 3_CLEANSED 4_FEATURIZ The project have both loaded and transformed data from individual municipalities as well as joined the sources into standardized tabels for cohorent usage and scaleability.","title":"Tranformations"},{"location":"analytics/transformations/transformations/#data-load","text":"The data load is done by integrating raw tables in Snowflake with the Storage Account setup in Azure. This has been done by using stages that fetches the file from the individual folders in the storage account.","title":"Data load"},{"location":"analytics/transformations/transformations/#specific-general-transformations","text":"The specific transformations for the local datasources of each municipality is divided into folders. These transformations contains the ETL-scripts (with inline-comments) used for taking the sources from anywhere between level 1 - 3 of the database layers. From there the transformations seeks to transform the individual municipalities data into a unified schema/table. These scripts are both commented and described in the general folder. In there one may also find the final suggested schemas for each data source.","title":"Specific &amp; General transformations"},{"location":"analytics/transformations/aarhus/aarhus_transformations/","text":"Sample of outputs Metadata RUM_ID AEC_OBJECTID AREAL AREAL_FORMATTED BREDDE BREDDE_FORMATTED BYGNINGS_ID BYGNINGS_ID_FORMATTED ETAGE_ID ETAGE_ID_FORMATTED LAENGDE LAENGDE_FORMATTED OMKREDS OMKREDS_FORMATTED RUMFUNKTION RUMFUNKTION_FORMATTED RUMHOJDE RUMHOJDE_FORMATTED RUMKODE RUMKODE_FORMATTED SKOLE_ID SKOLE_ID_FORMATTED SKOLE_NAVN SKOLE_NAVN_FORMATTED VAEGAREAL VAEGAREAL_FORMATTED 03.S.04 {1F661AE1-EFA1-4E0F-838B-3A89D219C798}BB1_A88 55,19 55.2 m\u00b2 7930 7930 3 3 S S 6960 6960 29780 29780 NORMALKLASSE NORMALKLASSE 2750 2750 150 150 42 42 Strandskolen Strandskolen 81,895 81.9 m\u00b2 03.S.03 {1F661AE1-EFA1-4E0F-838B-3A89D219C798}BB1_A89 55,83 55.8 m\u00b2 7930 7930 3 3 S S 7040 7040 29940 29940 NORMALKLASSE NORMALKLASSE 2750 2750 150 150 42 42 Strandskolen Strandskolen 82,335 82.3 m\u00b2 Bookings DATO UGEDAG START_TID SLUT_TID EMNE RESSOURCE FORENINGSNAVN LOKATION 03-01-2022 Mandag 17:00:00 18:30:00 S\u00e6son booking Gymnastiksal piger 1 Strandskolen 03-01-2022 Mandag 19:00:00 20:30:00 VRI Gymnastik Gymnastiksal piger 1 Strandskolen Energy KEY DATO MEASURETYPE VALUE ENHED TIME 3261220203 2022-01-01 cooling 34.4 C 23:00:00 3261220204 2022-01-01 cooling 27.89 C 23:00:00 IoT LOKALEID SENSORNAVN DATE TIME CO2 SOUND LIGHT IAQ TEMPERATURE HUMIDITY 02.S.08 0x8a10072b6e42a9e3 2021-10-19 11:29:20 414 38 16 32.67 02.S.08 0x8a10072b6e42a9e3 2021-10-19 11:33:20 559 38 19 32.7 16.8 55.9","title":"Sample of outputs"},{"location":"analytics/transformations/aarhus/aarhus_transformations/#sample-of-outputs","text":"","title":"Sample of outputs"},{"location":"analytics/transformations/aarhus/aarhus_transformations/#metadata","text":"RUM_ID AEC_OBJECTID AREAL AREAL_FORMATTED BREDDE BREDDE_FORMATTED BYGNINGS_ID BYGNINGS_ID_FORMATTED ETAGE_ID ETAGE_ID_FORMATTED LAENGDE LAENGDE_FORMATTED OMKREDS OMKREDS_FORMATTED RUMFUNKTION RUMFUNKTION_FORMATTED RUMHOJDE RUMHOJDE_FORMATTED RUMKODE RUMKODE_FORMATTED SKOLE_ID SKOLE_ID_FORMATTED SKOLE_NAVN SKOLE_NAVN_FORMATTED VAEGAREAL VAEGAREAL_FORMATTED 03.S.04 {1F661AE1-EFA1-4E0F-838B-3A89D219C798}BB1_A88 55,19 55.2 m\u00b2 7930 7930 3 3 S S 6960 6960 29780 29780 NORMALKLASSE NORMALKLASSE 2750 2750 150 150 42 42 Strandskolen Strandskolen 81,895 81.9 m\u00b2 03.S.03 {1F661AE1-EFA1-4E0F-838B-3A89D219C798}BB1_A89 55,83 55.8 m\u00b2 7930 7930 3 3 S S 7040 7040 29940 29940 NORMALKLASSE NORMALKLASSE 2750 2750 150 150 42 42 Strandskolen Strandskolen 82,335 82.3 m\u00b2","title":"Metadata"},{"location":"analytics/transformations/aarhus/aarhus_transformations/#bookings","text":"DATO UGEDAG START_TID SLUT_TID EMNE RESSOURCE FORENINGSNAVN LOKATION 03-01-2022 Mandag 17:00:00 18:30:00 S\u00e6son booking Gymnastiksal piger 1 Strandskolen 03-01-2022 Mandag 19:00:00 20:30:00 VRI Gymnastik Gymnastiksal piger 1 Strandskolen","title":"Bookings"},{"location":"analytics/transformations/aarhus/aarhus_transformations/#energy","text":"KEY DATO MEASURETYPE VALUE ENHED TIME 3261220203 2022-01-01 cooling 34.4 C 23:00:00 3261220204 2022-01-01 cooling 27.89 C 23:00:00","title":"Energy"},{"location":"analytics/transformations/aarhus/aarhus_transformations/#iot","text":"LOKALEID SENSORNAVN DATE TIME CO2 SOUND LIGHT IAQ TEMPERATURE HUMIDITY 02.S.08 0x8a10072b6e42a9e3 2021-10-19 11:29:20 414 38 16 32.67 02.S.08 0x8a10072b6e42a9e3 2021-10-19 11:33:20 559 38 19 32.7 16.8 55.9","title":"IoT"},{"location":"analytics/transformations/favrskov/favrskov_transformations/","text":"Sample of outputs Metadata LOKALENR FLOJ OVERSKRIFT TYPE NOTE AREAL A.09 A Biologi Faglokale 63,9 A.10 A Natur/Teknologi Faglokale 64,1 Bookings DATO FRAKL TILKL LOKALE LOKALENR OVERSKRIFT BESKRIVELSE LEJER GRUPPE BOOKINGID BOOKINGTYPE 2020-01-01 14:00:00 19:30:00 R\u00d8N-H.06 Musik syd 2575 Favrskov Musikskole R\u00f8nb\u00e6kskolen 5952 Booking 2020-01-01 14:00:00 15:15:00 R\u00d8N-H.05 Musik\u00f8verum Vest 2585 Favrskov Musikskole R\u00f8nb\u00e6kskolen 5954 Booking Energy TAGID NAME VALUE DATE TIME 74 2023-10-05 12-13 47.4 2023-10-05 12:00:00 74 2023-10-05 12-13 47.4 2023-10-05 12:00:00 IoT NAVN SENSORNAVN DATE TIME CO2 SOUND LIGHT IAQ TEMPERATURE HUMIDITY B.56 0x4c223205c895a458 2023-09-27 17:19:03 457 38 0 0.07 24.6 57 G.03 0x83784b9635559921 2023-09-27 17:18:58 470 37 0 0.03 23.4 62.9 CTS TIMESTAMP CO2 MOTORVENTIL RUM_TILSTAND MOTORSPJAELD_INDBLAESNING MOTORSPJAELD_UDBLAESNING TEMPERATUR VAV_SPJAELD VENTILATIONSKANAL_1 VENTILATIONSKANAL_2 LOKALE DATO TIME 05-01-2023 03:45 473 0 Nat 6 6 18,1 30 D.06 5. januar 2023 03:45:00 05-01-2023 04:15 469 0 Nat 6 6 18,1 30 D.06 5. januar 2023 04:15:00","title":"Sample of outputs"},{"location":"analytics/transformations/favrskov/favrskov_transformations/#sample-of-outputs","text":"","title":"Sample of outputs"},{"location":"analytics/transformations/favrskov/favrskov_transformations/#metadata","text":"LOKALENR FLOJ OVERSKRIFT TYPE NOTE AREAL A.09 A Biologi Faglokale 63,9 A.10 A Natur/Teknologi Faglokale 64,1","title":"Metadata"},{"location":"analytics/transformations/favrskov/favrskov_transformations/#bookings","text":"DATO FRAKL TILKL LOKALE LOKALENR OVERSKRIFT BESKRIVELSE LEJER GRUPPE BOOKINGID BOOKINGTYPE 2020-01-01 14:00:00 19:30:00 R\u00d8N-H.06 Musik syd 2575 Favrskov Musikskole R\u00f8nb\u00e6kskolen 5952 Booking 2020-01-01 14:00:00 15:15:00 R\u00d8N-H.05 Musik\u00f8verum Vest 2585 Favrskov Musikskole R\u00f8nb\u00e6kskolen 5954 Booking","title":"Bookings"},{"location":"analytics/transformations/favrskov/favrskov_transformations/#energy","text":"TAGID NAME VALUE DATE TIME 74 2023-10-05 12-13 47.4 2023-10-05 12:00:00 74 2023-10-05 12-13 47.4 2023-10-05 12:00:00","title":"Energy"},{"location":"analytics/transformations/favrskov/favrskov_transformations/#iot","text":"NAVN SENSORNAVN DATE TIME CO2 SOUND LIGHT IAQ TEMPERATURE HUMIDITY B.56 0x4c223205c895a458 2023-09-27 17:19:03 457 38 0 0.07 24.6 57 G.03 0x83784b9635559921 2023-09-27 17:18:58 470 37 0 0.03 23.4 62.9","title":"IoT"},{"location":"analytics/transformations/favrskov/favrskov_transformations/#cts","text":"TIMESTAMP CO2 MOTORVENTIL RUM_TILSTAND MOTORSPJAELD_INDBLAESNING MOTORSPJAELD_UDBLAESNING TEMPERATUR VAV_SPJAELD VENTILATIONSKANAL_1 VENTILATIONSKANAL_2 LOKALE DATO TIME 05-01-2023 03:45 473 0 Nat 6 6 18,1 30 D.06 5. januar 2023 03:45:00 05-01-2023 04:15 469 0 Nat 6 6 18,1 30 D.06 5. januar 2023 04:15:00","title":"CTS"},{"location":"analytics/transformations/general/Schemas/","text":"Base table Schemas The below schemas are for tables that have been used both directly in the visual insights and analytics but also in transformations into derived tables. These tables could be the direct input for how the future dataformats might be. Municipalities Table Column 1_RAW_KOMMUNER_SKOLER KOMMUNE 1_RAW_KOMMUNER_SKOLER SKOLE 1_RAW_KOMMUNER_SKOLER DRIFTSAREAL Rooms Table Column 4_FEATURIZ_LOKALER ID 4_FEATURIZ_LOKALER AREAL 4_FEATURIZ_LOKALER AREAL_FORMAT 4_FEATURIZ_LOKALER TYPE_RUM 4_FEATURIZ_LOKALER FLOJ 4_FEATURIZ_LOKALER KOMMUNE 4_FEATURIZ_LOKALER SKOLE 4_FEATURIZ_LOKALER KOMMUNE_LOKALE Bookings Table Column 4_FEATURIZ_BOOKINGS ID 4_FEATURIZ_BOOKINGS DATO 4_FEATURIZ_BOOKINGS START_TID 4_FEATURIZ_BOOKINGS SLUT_TID 4_FEATURIZ_BOOKINGS TIMER 4_FEATURIZ_BOOKINGS TIMER_FORMAT 4_FEATURIZ_BOOKINGS TYPE 4_FEATURIZ_BOOKINGS LEJER 4_FEATURIZ_BOOKINGS KOMMUNE_DATO 4_FEATURIZ_BOOKINGS KOMMUNE_DATO_LOKALE 4_FEATURIZ_BOOKINGS KOMMUNE 4_FEATURIZ_BOOKINGS SKOLE Energy Table Column 4_FEATURIZ_ENERGIDATA MAALER_ID 4_FEATURIZ_ENERGIDATA DATO 4_FEATURIZ_ENERGIDATA MEASURE_TYPE 4_FEATURIZ_ENERGIDATA VALUE 4_FEATURIZ_ENERGIDATA ENHED 4_FEATURIZ_ENERGIDATA TIME 4_FEATURIZ_ENERGIDATA TIMESTAMP 4_FEATURIZ_ENERGIDATA KOMMUNE_DATO 4_FEATURIZ_ENERGIDATA KOMMUNE_DATO_TIME 4_FEATURIZ_ENERGIDATA KOMMUNE 4_FEATURIZ_ENERGIDATA SKOLE IoT & CTS Table Column 4_FEATURIZ_CTS_X_IOT DATE 4_FEATURIZ_CTS_X_IOT TIME 4_FEATURIZ_CTS_X_IOT TIMESTAMP 4_FEATURIZ_CTS_X_IOT TIME_GROUP 4_FEATURIZ_CTS_X_IOT CO2 4_FEATURIZ_CTS_X_IOT SOUND 4_FEATURIZ_CTS_X_IOT LIGHT 4_FEATURIZ_CTS_X_IOT IAQ 4_FEATURIZ_CTS_X_IOT TEMPERATURE 4_FEATURIZ_CTS_X_IOT HUMIDITY 4_FEATURIZ_CTS_X_IOT MOTION 4_FEATURIZ_CTS_X_IOT ID 4_FEATURIZ_CTS_X_IOT KOMMUNE 4_FEATURIZ_CTS_X_IOT SKOLE 4_FEATURIZ_CTS_X_IOT KOMMUNE_LOKALE Ideal Curve Table Column 4_FEATURIZ_IDEALKURVE GRADER 4_FEATURIZ_IDEALKURVE HAELDNING_BYGNING 4_FEATURIZ_IDEALKURVE SKAERING_BYGNING 4_FEATURIZ_IDEALKURVE IDEALKURVE 4_FEATURIZ_IDEALKURVE KOMMUNE Manuel benchmark values (Vacation Closing) Table Column 4_FEATURIZ_BENCHMARK_WEEKEND_MANUEL KOMMUNE 4_FEATURIZ_BENCHMARK_WEEKEND_MANUEL BENCHMARK_WEEKEND Derived / Featurized tables These schemas for tables are the ones that have been created by using base tables and raw data. They are the ones used mainly for specific analytics and insights Dates Table Column 4_FEATURIZ_DATOER TIMESTAMP 4_FEATURIZ_DATOER DATE 4_FEATURIZ_DATOER TIME 4_FEATURIZ_DATOER WEEKNO 4_FEATURIZ_DATOER DAYNAME 4_FEATURIZ_DATOER WEEKDAYNUM 4_FEATURIZ_DATOER YEAR 4_FEATURIZ_DATOER MONTH 4_FEATURIZ_DATOER MONTHNAME 4_FEATURIZ_DATOER MONTHNO 4_FEATURIZ_DATOER TIDSPUNKT_TYPE 4_FEATURIZ_DATOER NAVN 4_FEATURIZ_DATOER TYPE 4_FEATURIZ_DATOER DAG_I_FERIEN 4_FEATURIZ_DATOER LAST_YEAR_OR_THIS_YEAR 4_FEATURIZ_DATOER IN_THE_LAST_365_DAYS 4_FEATURIZ_DATOER LAST_VACATION 4_FEATURIZ_DATOER DAY_OF_YEAR Bookings Table Column 4_FEATURIZ_BOOKINGS_TIME DATO 4_FEATURIZ_BOOKINGS_TIME TIME 4_FEATURIZ_BOOKINGS_TIME TIMESTAMP 4_FEATURIZ_BOOKINGS_TIME KOMMUNE 4_FEATURIZ_BOOKINGS_TIME ID 4_FEATURIZ_BOOKINGS_TIME KOMMUNE_LOKALE 4_FEATURIZ_BOOKINGS_TIME KOMMUNE_DATO_TIME 4_FEATURIZ_BOOKINGS_TIME BOOKET 4_FEATURIZ_BOOKINGS_TIME STARTTID 4_FEATURIZ_BOOKINGS_TIME SLUTTID 4_FEATURIZ_BOOKINGS_TIME LEJERID 4_FEATURIZ_BOOKINGS_TIME TYPEBOOKING 4_FEATURIZ_BOOKINGS_TIME BOOKET_TID Energy Table Column 4_FEATURIZ_ENERGIDATA_OPTIMIZED MEASURE_TYPE 4_FEATURIZ_ENERGIDATA_OPTIMIZED KOMMUNE 4_FEATURIZ_ENERGIDATA_OPTIMIZED DATO 4_FEATURIZ_ENERGIDATA_OPTIMIZED TIME 4_FEATURIZ_ENERGIDATA_OPTIMIZED VALUE 4_FEATURIZ_ENERGIDATA_OPTIMIZED ENHED 4_FEATURIZ_ENERGIDATA_OPTIMIZED TIMESTAMP 4_FEATURIZ_ENERGIDATA_OPTIMIZED KOMMUNE_DATO_TIME Weather Table Column 4_FEATURIZ_DMI STATIONID 4_FEATURIZ_DMI PARAMETER 4_FEATURIZ_DMI PARAMETER_DANSK 4_FEATURIZ_DMI VALUE 4_FEATURIZ_DMI VALUE_ROUND 4_FEATURIZ_DMI TIMESTAMP 4_FEATURIZ_DMI DATE 4_FEATURIZ_DMI KOMMUNE Driftoptimeringsmodel Table Column 4_FEATURIZ_DRIFTOPTIMERINGSMODEL DATE 4_FEATURIZ_DRIFTOPTIMERINGSMODEL TIME 4_FEATURIZ_DRIFTOPTIMERINGSMODEL DATETIME 4_FEATURIZ_DRIFTOPTIMERINGSMODEL ID 4_FEATURIZ_DRIFTOPTIMERINGSMODEL KOMMUNE 4_FEATURIZ_DRIFTOPTIMERINGSMODEL BRUGTE_KVARTER 4_FEATURIZ_DRIFTOPTIMERINGSMODEL IN_USE 4_FEATURIZ_DRIFTOPTIMERINGSMODEL KOMMUNE_LOKALE Table Column 4_FEATURIZ_DRIFTOPTIMERING_TRAINING ID 4_FEATURIZ_DRIFTOPTIMERING_TRAINING KOMMUNE 4_FEATURIZ_DRIFTOPTIMERING_TRAINING SKOLE 4_FEATURIZ_DRIFTOPTIMERING_TRAINING DATE 4_FEATURIZ_DRIFTOPTIMERING_TRAINING TIME 4_FEATURIZ_DRIFTOPTIMERING_TRAINING DAYNAME 4_FEATURIZ_DRIFTOPTIMERING_TRAINING TIDSPUNKT_TYPE 4_FEATURIZ_DRIFTOPTIMERING_TRAINING TYPE 4_FEATURIZ_DRIFTOPTIMERING_TRAINING NAVN 4_FEATURIZ_DRIFTOPTIMERING_TRAINING CO2 4_FEATURIZ_DRIFTOPTIMERING_TRAINING TEMP 4_FEATURIZ_DRIFTOPTIMERING_TRAINING MOTION 4_FEATURIZ_DRIFTOPTIMERING_TRAINING IAQ 4_FEATURIZ_DRIFTOPTIMERING_TRAINING BOOKET 4_FEATURIZ_DRIFTOPTIMERING_TRAINING SKEMALAGT Table Column 4_FEATURIZ_DRIFTOPTIMERING_PREDICT ID 4_FEATURIZ_DRIFTOPTIMERING_PREDICT KOMMUNE 4_FEATURIZ_DRIFTOPTIMERING_PREDICT SKOLE 4_FEATURIZ_DRIFTOPTIMERING_PREDICT DATE 4_FEATURIZ_DRIFTOPTIMERING_PREDICT TIME 4_FEATURIZ_DRIFTOPTIMERING_PREDICT DAYNAME 4_FEATURIZ_DRIFTOPTIMERING_PREDICT TIDSPUNKT_TYPE 4_FEATURIZ_DRIFTOPTIMERING_PREDICT TYPE 4_FEATURIZ_DRIFTOPTIMERING_PREDICT NAVN 4_FEATURIZ_DRIFTOPTIMERING_PREDICT CO2 4_FEATURIZ_DRIFTOPTIMERING_PREDICT TEMP 4_FEATURIZ_DRIFTOPTIMERING_PREDICT MOTION 4_FEATURIZ_DRIFTOPTIMERING_PREDICT IAQ 4_FEATURIZ_DRIFTOPTIMERING_PREDICT BOOKET 4_FEATURIZ_DRIFTOPTIMERING_PREDICT SKEMALAGT Table Column 4_FEATURIZ_DRIFTOPTIMERING_PREDICT ID 4_FEATURIZ_DRIFTOPTIMERING_PREDICT KOMMUNE 4_FEATURIZ_DRIFTOPTIMERING_PREDICT SKOLE 4_FEATURIZ_DRIFTOPTIMERING_PREDICT DATE 4_FEATURIZ_DRIFTOPTIMERING_PREDICT TIME 4_FEATURIZ_DRIFTOPTIMERING_PREDICT DAYNAME 4_FEATURIZ_DRIFTOPTIMERING_PREDICT TIDSPUNKT_TYPE 4_FEATURIZ_DRIFTOPTIMERING_PREDICT TYPE 4_FEATURIZ_DRIFTOPTIMERING_PREDICT NAVN 4_FEATURIZ_DRIFTOPTIMERING_PREDICT CO2 4_FEATURIZ_DRIFTOPTIMERING_PREDICT TEMP 4_FEATURIZ_DRIFTOPTIMERING_PREDICT MOTION 4_FEATURIZ_DRIFTOPTIMERING_PREDICT IAQ 4_FEATURIZ_DRIFTOPTIMERING_PREDICT BOOKET 4_FEATURIZ_DRIFTOPTIMERING_PREDICT SKEMALAGT Passive Hours Table Column 4_FEATURIZ_PASSIVTIMER TIMESTAMP 4_FEATURIZ_PASSIVTIMER DATE 4_FEATURIZ_PASSIVTIMER TIME 4_FEATURIZ_PASSIVTIMER YEAR 4_FEATURIZ_PASSIVTIMER MONTH 4_FEATURIZ_PASSIVTIMER MONTHNAME 4_FEATURIZ_PASSIVTIMER MONTHNO 4_FEATURIZ_PASSIVTIMER KOMMUNE 4_FEATURIZ_PASSIVTIMER PASSIVTIME 4_FEATURIZ_PASSIVTIMER KOMMUNE_DATO_TIME Table Column 4_FEATURIZ_MINIMUM_EL KOMMUNE 4_FEATURIZ_MINIMUM_EL MINIMUM_EL 4_FEATURIZ_MINIMUM_EL MANUEL_MINIMUM","title":"Base table Schemas"},{"location":"analytics/transformations/general/Schemas/#base-table-schemas","text":"The below schemas are for tables that have been used both directly in the visual insights and analytics but also in transformations into derived tables. These tables could be the direct input for how the future dataformats might be.","title":"Base table Schemas"},{"location":"analytics/transformations/general/Schemas/#municipalities","text":"Table Column 1_RAW_KOMMUNER_SKOLER KOMMUNE 1_RAW_KOMMUNER_SKOLER SKOLE 1_RAW_KOMMUNER_SKOLER DRIFTSAREAL","title":"Municipalities"},{"location":"analytics/transformations/general/Schemas/#rooms","text":"Table Column 4_FEATURIZ_LOKALER ID 4_FEATURIZ_LOKALER AREAL 4_FEATURIZ_LOKALER AREAL_FORMAT 4_FEATURIZ_LOKALER TYPE_RUM 4_FEATURIZ_LOKALER FLOJ 4_FEATURIZ_LOKALER KOMMUNE 4_FEATURIZ_LOKALER SKOLE 4_FEATURIZ_LOKALER KOMMUNE_LOKALE","title":"Rooms"},{"location":"analytics/transformations/general/Schemas/#bookings","text":"Table Column 4_FEATURIZ_BOOKINGS ID 4_FEATURIZ_BOOKINGS DATO 4_FEATURIZ_BOOKINGS START_TID 4_FEATURIZ_BOOKINGS SLUT_TID 4_FEATURIZ_BOOKINGS TIMER 4_FEATURIZ_BOOKINGS TIMER_FORMAT 4_FEATURIZ_BOOKINGS TYPE 4_FEATURIZ_BOOKINGS LEJER 4_FEATURIZ_BOOKINGS KOMMUNE_DATO 4_FEATURIZ_BOOKINGS KOMMUNE_DATO_LOKALE 4_FEATURIZ_BOOKINGS KOMMUNE 4_FEATURIZ_BOOKINGS SKOLE","title":"Bookings"},{"location":"analytics/transformations/general/Schemas/#energy","text":"Table Column 4_FEATURIZ_ENERGIDATA MAALER_ID 4_FEATURIZ_ENERGIDATA DATO 4_FEATURIZ_ENERGIDATA MEASURE_TYPE 4_FEATURIZ_ENERGIDATA VALUE 4_FEATURIZ_ENERGIDATA ENHED 4_FEATURIZ_ENERGIDATA TIME 4_FEATURIZ_ENERGIDATA TIMESTAMP 4_FEATURIZ_ENERGIDATA KOMMUNE_DATO 4_FEATURIZ_ENERGIDATA KOMMUNE_DATO_TIME 4_FEATURIZ_ENERGIDATA KOMMUNE 4_FEATURIZ_ENERGIDATA SKOLE","title":"Energy"},{"location":"analytics/transformations/general/Schemas/#iot-cts","text":"Table Column 4_FEATURIZ_CTS_X_IOT DATE 4_FEATURIZ_CTS_X_IOT TIME 4_FEATURIZ_CTS_X_IOT TIMESTAMP 4_FEATURIZ_CTS_X_IOT TIME_GROUP 4_FEATURIZ_CTS_X_IOT CO2 4_FEATURIZ_CTS_X_IOT SOUND 4_FEATURIZ_CTS_X_IOT LIGHT 4_FEATURIZ_CTS_X_IOT IAQ 4_FEATURIZ_CTS_X_IOT TEMPERATURE 4_FEATURIZ_CTS_X_IOT HUMIDITY 4_FEATURIZ_CTS_X_IOT MOTION 4_FEATURIZ_CTS_X_IOT ID 4_FEATURIZ_CTS_X_IOT KOMMUNE 4_FEATURIZ_CTS_X_IOT SKOLE 4_FEATURIZ_CTS_X_IOT KOMMUNE_LOKALE","title":"IoT &amp; CTS"},{"location":"analytics/transformations/general/Schemas/#ideal-curve","text":"Table Column 4_FEATURIZ_IDEALKURVE GRADER 4_FEATURIZ_IDEALKURVE HAELDNING_BYGNING 4_FEATURIZ_IDEALKURVE SKAERING_BYGNING 4_FEATURIZ_IDEALKURVE IDEALKURVE 4_FEATURIZ_IDEALKURVE KOMMUNE","title":"Ideal Curve"},{"location":"analytics/transformations/general/Schemas/#manuel-benchmark-values-vacation-closing","text":"Table Column 4_FEATURIZ_BENCHMARK_WEEKEND_MANUEL KOMMUNE 4_FEATURIZ_BENCHMARK_WEEKEND_MANUEL BENCHMARK_WEEKEND","title":"Manuel benchmark values (Vacation Closing)"},{"location":"analytics/transformations/general/Schemas/#derived-featurized-tables","text":"These schemas for tables are the ones that have been created by using base tables and raw data. They are the ones used mainly for specific analytics and insights","title":"Derived / Featurized tables"},{"location":"analytics/transformations/general/Schemas/#dates","text":"Table Column 4_FEATURIZ_DATOER TIMESTAMP 4_FEATURIZ_DATOER DATE 4_FEATURIZ_DATOER TIME 4_FEATURIZ_DATOER WEEKNO 4_FEATURIZ_DATOER DAYNAME 4_FEATURIZ_DATOER WEEKDAYNUM 4_FEATURIZ_DATOER YEAR 4_FEATURIZ_DATOER MONTH 4_FEATURIZ_DATOER MONTHNAME 4_FEATURIZ_DATOER MONTHNO 4_FEATURIZ_DATOER TIDSPUNKT_TYPE 4_FEATURIZ_DATOER NAVN 4_FEATURIZ_DATOER TYPE 4_FEATURIZ_DATOER DAG_I_FERIEN 4_FEATURIZ_DATOER LAST_YEAR_OR_THIS_YEAR 4_FEATURIZ_DATOER IN_THE_LAST_365_DAYS 4_FEATURIZ_DATOER LAST_VACATION 4_FEATURIZ_DATOER DAY_OF_YEAR","title":"Dates"},{"location":"analytics/transformations/general/Schemas/#bookings_1","text":"Table Column 4_FEATURIZ_BOOKINGS_TIME DATO 4_FEATURIZ_BOOKINGS_TIME TIME 4_FEATURIZ_BOOKINGS_TIME TIMESTAMP 4_FEATURIZ_BOOKINGS_TIME KOMMUNE 4_FEATURIZ_BOOKINGS_TIME ID 4_FEATURIZ_BOOKINGS_TIME KOMMUNE_LOKALE 4_FEATURIZ_BOOKINGS_TIME KOMMUNE_DATO_TIME 4_FEATURIZ_BOOKINGS_TIME BOOKET 4_FEATURIZ_BOOKINGS_TIME STARTTID 4_FEATURIZ_BOOKINGS_TIME SLUTTID 4_FEATURIZ_BOOKINGS_TIME LEJERID 4_FEATURIZ_BOOKINGS_TIME TYPEBOOKING 4_FEATURIZ_BOOKINGS_TIME BOOKET_TID","title":"Bookings"},{"location":"analytics/transformations/general/Schemas/#energy_1","text":"Table Column 4_FEATURIZ_ENERGIDATA_OPTIMIZED MEASURE_TYPE 4_FEATURIZ_ENERGIDATA_OPTIMIZED KOMMUNE 4_FEATURIZ_ENERGIDATA_OPTIMIZED DATO 4_FEATURIZ_ENERGIDATA_OPTIMIZED TIME 4_FEATURIZ_ENERGIDATA_OPTIMIZED VALUE 4_FEATURIZ_ENERGIDATA_OPTIMIZED ENHED 4_FEATURIZ_ENERGIDATA_OPTIMIZED TIMESTAMP 4_FEATURIZ_ENERGIDATA_OPTIMIZED KOMMUNE_DATO_TIME","title":"Energy"},{"location":"analytics/transformations/general/Schemas/#weather","text":"Table Column 4_FEATURIZ_DMI STATIONID 4_FEATURIZ_DMI PARAMETER 4_FEATURIZ_DMI PARAMETER_DANSK 4_FEATURIZ_DMI VALUE 4_FEATURIZ_DMI VALUE_ROUND 4_FEATURIZ_DMI TIMESTAMP 4_FEATURIZ_DMI DATE 4_FEATURIZ_DMI KOMMUNE","title":"Weather"},{"location":"analytics/transformations/general/Schemas/#driftoptimeringsmodel","text":"Table Column 4_FEATURIZ_DRIFTOPTIMERINGSMODEL DATE 4_FEATURIZ_DRIFTOPTIMERINGSMODEL TIME 4_FEATURIZ_DRIFTOPTIMERINGSMODEL DATETIME 4_FEATURIZ_DRIFTOPTIMERINGSMODEL ID 4_FEATURIZ_DRIFTOPTIMERINGSMODEL KOMMUNE 4_FEATURIZ_DRIFTOPTIMERINGSMODEL BRUGTE_KVARTER 4_FEATURIZ_DRIFTOPTIMERINGSMODEL IN_USE 4_FEATURIZ_DRIFTOPTIMERINGSMODEL KOMMUNE_LOKALE Table Column 4_FEATURIZ_DRIFTOPTIMERING_TRAINING ID 4_FEATURIZ_DRIFTOPTIMERING_TRAINING KOMMUNE 4_FEATURIZ_DRIFTOPTIMERING_TRAINING SKOLE 4_FEATURIZ_DRIFTOPTIMERING_TRAINING DATE 4_FEATURIZ_DRIFTOPTIMERING_TRAINING TIME 4_FEATURIZ_DRIFTOPTIMERING_TRAINING DAYNAME 4_FEATURIZ_DRIFTOPTIMERING_TRAINING TIDSPUNKT_TYPE 4_FEATURIZ_DRIFTOPTIMERING_TRAINING TYPE 4_FEATURIZ_DRIFTOPTIMERING_TRAINING NAVN 4_FEATURIZ_DRIFTOPTIMERING_TRAINING CO2 4_FEATURIZ_DRIFTOPTIMERING_TRAINING TEMP 4_FEATURIZ_DRIFTOPTIMERING_TRAINING MOTION 4_FEATURIZ_DRIFTOPTIMERING_TRAINING IAQ 4_FEATURIZ_DRIFTOPTIMERING_TRAINING BOOKET 4_FEATURIZ_DRIFTOPTIMERING_TRAINING SKEMALAGT Table Column 4_FEATURIZ_DRIFTOPTIMERING_PREDICT ID 4_FEATURIZ_DRIFTOPTIMERING_PREDICT KOMMUNE 4_FEATURIZ_DRIFTOPTIMERING_PREDICT SKOLE 4_FEATURIZ_DRIFTOPTIMERING_PREDICT DATE 4_FEATURIZ_DRIFTOPTIMERING_PREDICT TIME 4_FEATURIZ_DRIFTOPTIMERING_PREDICT DAYNAME 4_FEATURIZ_DRIFTOPTIMERING_PREDICT TIDSPUNKT_TYPE 4_FEATURIZ_DRIFTOPTIMERING_PREDICT TYPE 4_FEATURIZ_DRIFTOPTIMERING_PREDICT NAVN 4_FEATURIZ_DRIFTOPTIMERING_PREDICT CO2 4_FEATURIZ_DRIFTOPTIMERING_PREDICT TEMP 4_FEATURIZ_DRIFTOPTIMERING_PREDICT MOTION 4_FEATURIZ_DRIFTOPTIMERING_PREDICT IAQ 4_FEATURIZ_DRIFTOPTIMERING_PREDICT BOOKET 4_FEATURIZ_DRIFTOPTIMERING_PREDICT SKEMALAGT Table Column 4_FEATURIZ_DRIFTOPTIMERING_PREDICT ID 4_FEATURIZ_DRIFTOPTIMERING_PREDICT KOMMUNE 4_FEATURIZ_DRIFTOPTIMERING_PREDICT SKOLE 4_FEATURIZ_DRIFTOPTIMERING_PREDICT DATE 4_FEATURIZ_DRIFTOPTIMERING_PREDICT TIME 4_FEATURIZ_DRIFTOPTIMERING_PREDICT DAYNAME 4_FEATURIZ_DRIFTOPTIMERING_PREDICT TIDSPUNKT_TYPE 4_FEATURIZ_DRIFTOPTIMERING_PREDICT TYPE 4_FEATURIZ_DRIFTOPTIMERING_PREDICT NAVN 4_FEATURIZ_DRIFTOPTIMERING_PREDICT CO2 4_FEATURIZ_DRIFTOPTIMERING_PREDICT TEMP 4_FEATURIZ_DRIFTOPTIMERING_PREDICT MOTION 4_FEATURIZ_DRIFTOPTIMERING_PREDICT IAQ 4_FEATURIZ_DRIFTOPTIMERING_PREDICT BOOKET 4_FEATURIZ_DRIFTOPTIMERING_PREDICT SKEMALAGT","title":"Driftoptimeringsmodel"},{"location":"analytics/transformations/general/Schemas/#passive-hours","text":"Table Column 4_FEATURIZ_PASSIVTIMER TIMESTAMP 4_FEATURIZ_PASSIVTIMER DATE 4_FEATURIZ_PASSIVTIMER TIME 4_FEATURIZ_PASSIVTIMER YEAR 4_FEATURIZ_PASSIVTIMER MONTH 4_FEATURIZ_PASSIVTIMER MONTHNAME 4_FEATURIZ_PASSIVTIMER MONTHNO 4_FEATURIZ_PASSIVTIMER KOMMUNE 4_FEATURIZ_PASSIVTIMER PASSIVTIME 4_FEATURIZ_PASSIVTIMER KOMMUNE_DATO_TIME Table Column 4_FEATURIZ_MINIMUM_EL KOMMUNE 4_FEATURIZ_MINIMUM_EL MINIMUM_EL 4_FEATURIZ_MINIMUM_EL MANUEL_MINIMUM","title":"Passive Hours"},{"location":"analytics/transformations/general/bookings/","text":"create or replace view \"3_CLEANSED_BOOKINGS\" as -- Joining the 3 municipalities booking-data in a unified format select b . RUM_ID as ID , DATO :: string as DATO , START_TID as START_TID , SLUT_TID as SLUT_TID , to_char ( timestampdiff ( MINUTE , START_TID , SLUT_TID ) / 60 ) as TIMER , REPLACE ( TIMER , ',' , '.' ) as TIMER_FORMAT , EMNE as TYPE , FORENINGSNAVN as LEJER , concat ( dato :: string , '-Aarhus' ) as KOMMUNE_DATO , concat ( dato :: string , '-' , ID , '-Aarhus' ) as KOMMUNE_DATO_LOKALE , 'Aarhus' as KOMMUNE , 'Strandskolen' as SKOLE from \"2_STANDARD_AARHUS_BOOKINGS_HISTORY\" , \"1_RAW_AARHUS_LOKALE_STAMDATA_BOOKBAR\" b where RESSOURCE = RUM_NAVN -- Aarhus union all select SUBSTR ( LOKALE , 5 , 4 ) as ID , DATE ( DATO ):: string as DATO , FRAKL as START_TID , TILKL as SLUT_TID , to_char ( timestampdiff ( MINUTE , START_TID , SLUT_TID ) / 60 ) as TIMER , REPLACE ( TIMER , ',' , '.' ) as TIMER_FORMAT , BOOKINGTYPE as TYPE , LEJER as LEJER , concat ( right ( dato :: string , 5 ), '-' , left ( dato :: string , 4 ), '-Favrskov' ) as KOMMUNE_DATO , concat ( right ( dato :: string , 2 ), '-' , substr ( dato :: string , 6 , 2 ), '-' , left ( dato :: string , 4 ), '-' , ID , '-Favrskov' ) as KOMMUNE_DATO_LOKALE , 'Favrskov' as KOMMUNE , 'R\u00f8nb\u00e6kskolen' as SKOLE from \"3_CLEANSED_FAVRSKOV_BOOKINGS\" --Favrskov union all select RIGHT ( LOKALENUMMER , length ( LOKALENUMMER ) - 4 ) as ID , DATE ( BOOKING_START ):: string as DATO , BOOKING_START as START_TID , BOOKING_END as SLUT_TID , to_char ( timestampdiff ( MINUTE , START_TID , SLUT_TID ) / 60 ) as TIMER , REPLACE ( TIMER , ',' , '.' ) as TIMER_FORMAT , BOOKING_TYPE as TYPE , ORGANIZATION_NAME as LEJER , concat ( right ( dato :: string , 5 ), '-' , left ( dato :: string , 4 ), '-Syddjurs' ) as KOMMUNE_DATO , concat ( right ( dato :: string , 2 ), '-' , substr ( dato :: string , 6 , 2 ), '-' , left ( dato :: string , 4 ), '-' , ID , '-Syddjurs' ) as KOMMUNE_DATO_LOKALE , 'Syddjurs' as KOMMUNE , 'Thorsager Skole' as SKOLE from \"3_CLEANSED_SYDDJURS_BOOKINGS\" , \"2_STANDARD_SYDDJURS_LOKALE_STAMDATA_AREAL\" where RESOURCE_ID :: string = CONVENTUSRESSOURCEID ; --Syddjurs create or replace table \"4_FEATURIZ_BOOKINGS\" as -- Persisting of data in a table select distinct * from \"3_CLEANSED_BOOKINGS\" ; create or replace view \"3_CLEANSED_BOOKINGS_TIME\" as -- Bookings on an hourly-flattened level SELECT d . date as dato , d . TIME , concat ( d . date :: date , ' ' , d . time ):: datetime as timestamp , r . KOMMUNE , r . ID , concat ( r . kommune , '-' , r . id ) as kommune_lokale , CONCAT ( d . date :: STRING , '-' , d . TIME , '-' , r . KOMMUNE ) AS KOMMUNE_DATO_TIME , IFF ( d . time >= b . start_tid , IFF ( d . time <= b . slut_tid , 1 , 0 ), 0 ) AS booket , IFF ( booket = 1 , start_tid , NULL ) AS starttid , IFF ( booket = 1 , slut_tid , NULL ) AS sluttid , IFF ( booket = 1 , lejer , NULL ) AS lejerid , IFF ( booket = 1 , b . type , NULL ) AS typebooking , CAST ( IFF ( booket = 1 , timer_format , NULL ) AS FLOAT ) AS booket_tid FROM \"4_FEATURIZ_DATOER\" d CROSS JOIN \"4_FEATURIZ_LOKALER\" r LEFT JOIN \"4_FEATURIZ_BOOKINGS\" b ON d . date :: date = iff ( b . kommune = 'Aarhus' , try_to_date ( b . dato , 'dd-mm-yyyy' ), try_to_date ( b . dato , 'yyyy-mm-dd' )) and r . id = b . id and r . kommune = b . kommune WHERE booket = 1 GROUP BY d . date , TIME , slut_tid , start_tid , timer_format , r . KOMMUNE , r . ID , lejer , b . type ORDER BY date :: date , time , kommune , id ; create or replace table \"4_FEATURIZ_BOOKINGS_TIME\" -- Persisting of data in a table as select * from \"3_CLEANSED_BOOKINGS_TIME\" ;","title":"Bookings"},{"location":"analytics/transformations/general/dates/","text":"create or replace table \"1_RAW_KALENDER\" ( DATE date , NAVN varchar , TYPE varchar , WeekNo int , Dato varchar ); -- raw table containing vacations create or replace view \"2_STANDARD_TIMESTAMPS\" as -- generate timestamps from the 2022 to 2023 and define the types of days and timestamps select dateadd ( hour , row_number () over ( order by null ), '2022-01-01 00:00' ) as timestamp , timestamp :: date as date , timestamp :: time as time , decode ( DAYNAME ( date ), 'Mon' , 'Mandag' , 'Tue' , 'Tirsdag' , 'Wed' , 'Onsdag' , 'Thu' , 'Torsdag' , 'Fri' , 'Fredag' , 'Sat' , 'Loerdag' , 'Sun' , 'Soendag' ) as dayname , DAYOFWEEKISO ( date ) as weekdaynum , weekofyear ( timestamp ) as weekno , iff ( time >= '16:00' , 'Fritid' , iff ( weekdaynum > 5 , 'Fritid' , 'Skole' )) as TIDSPUNKT_TYPE from table ( generator ( rowcount => 17496 )) g ; create or replace view \"3_CLEANSED_DATOER\" as -- joining timestamps with the calendar for defining information for every hour since 2022 select k1 . timestamp , k1 . date :: date as date , k1 . TIME , k1 . weekno , k1 . dayname , k1 . weekdaynum , year ( k1 . date :: date ):: string as year , month ( k1 . date :: date ):: string as month , monthname ( k1 . date :: date ) as monthname , month ( k1 . date :: date ):: int as monthno , iff ( k1 . time <= '07:00' OR k1 . time >= '16:00' , 'Fritid' , iff ( k1 . weekdaynum > 5 OR k2 . type in ( 'Helligdag' , 'Skole-ferie' ), 'Fritid' , 'Skole' )) as TIDSPUNKT_TYPE , k2 . navn , k2 . type , DATEDIFF ( DAY ,( -- calculating the number of day in the vacation the current date is select min ( s . date ) from \"1_RAW_KALENDER\" s where s . type = k2 . type and s . navn = k2 . navn and year ( s . date ) = year ( k2 . date ) ), k1 . date :: date ):: int as dag_i_ferien , iff ( year ( k1 . date :: date ) = year ( current_date ), 'TY' , iff ( year ( k1 . date :: date ) = year ( current_date ) - 1 , 'LY' , '' )) as last_year_or_this_year , iff ( datediff ( DAY , k1 . date :: date , current_date ) <= 365 , 'TY' , iff ( datediff ( DAY , k1 . date :: date , current_date ) > 365 , 'LY' , '' )) as in_the_last_365_days , iff ( navn = ( SELECT MAX ( NAVN ) AS max_name FROM \"1_RAW_KALENDER\" WHERE \"DATE\" = ( SELECT MAX ( \"DATE\" ) FROM \"1_RAW_KALENDER\" WHERE \"DATE\" <= CURRENT_DATE AND \"TYPE\" = 'Skole-ferie' )), 1 , 0 ) as last_vacation , dayofyear ( k1 . date :: date ) as day_of_year from \"2_STANDARD_TIMESTAMPS\" k1 , \"1_RAW_KALENDER\" k2 where k1 . date = k2 . date and k1 . date <= current_date -- only show dates on or before today order by k1 . date desc ; create or replace table \"4_FEATURIZ_DATOER\" as -- update persisted table select * from \"3_CLEANSED_DATOER\" ;","title":"Dates"},{"location":"analytics/transformations/general/driftoptimeringsmodel/","text":"create or replace table \"1_RAW_DRIFTOPTIMERINGSMODEL\" ( DATE varchar , TIME VARCHAR , DATETIME varchar , ID varchar , KOMMUNE VARCHAR , IN_USE INT , ANOMALY_SCORE FLOAT ); create or replace view \"2_STANDARD_DRIFTOTIMERINGSMODEL\" as select date :: date as date , date_trunc ( HOUR , timestamp_from_parts ( date , time ):: timestamp ):: time as time , date_trunc ( HOUR , timestamp_from_parts ( date , time ):: timestamp ) as datetime , id , kommune , IN_USE as IF_ANOMALY , anomaly_score as IF_anomaly_score , concat ( kommune , '-' , ID ) as KOMMUNE_LOKALE from \"1_RAW_DRIFTOPTIMERINGSMODEL\" ; create or replace view \"3_CLEANSED_DRIFTOPTIMERINGSMODEL\" as select date , time , datetime , id , kommune , sum ( if_anomaly ) as brugte_kvarter , iff ( brugte_kvarter > 0 , 1 , brugte_kvarter ) as in_use , kommune_lokale from \"2_STANDARD_DRIFTOTIMERINGSMODEL\" where if_anomaly is not null group by date , time , datetime , id , kommune , kommune_lokale ; create or replace table \"4_FEATURIZ_DRIFTOPTIMERINGSMODEL\" as select distinct * from \"3_CLEANSED_DRIFTOPTIMERINGSMODEL\" ; ---Bookings x Drift--- create or replace view \"3_CLEANSED_BOOKINGS_X_DRIFTOPTIMERINGSMODEL\" as select b . * , d . in_use , iff ( b . booket = 1 and d . in_use = 1 , 1 , 0 ) as brugt_booking from \"4_FEATURIZ_DRIFTOPTIMERINGSMODEL\" d join \"4_FEATURIZ_BOOKINGS_TIME\" b on d . kommune_lokale = b . kommune_lokale and d . datetime = b . timestamp ; create or replace table \"4_FEATURIZ_BOOKINGS_X_DRIFTOPTIMERINGSMODEL\" as select * from \"3_CLEANSED_BOOKINGS_X_DRIFTOPTIMERINGSMODEL\" ; ---DATA--- create or replace view \"2_STANDARD_DRIFTOPTIMERING_DATA\" as select distinct r . ID , r . KOMMUNE , r . SKOLE , d . DATE , d . TIME , d . DAYNAME , d . TIDSPUNKT_TYPE , k . TYPE , k . NAVN , avg ( i . CO2 ) as co2 , avg ( i . TEMP ) as temp , sum ( i . MOTION ) as motion , avg ( i . IAQ ) as IAQ , min ( b . booket ) as booket , iff ( s . starttid is not null and s . sluttid is not null , 1 , null ) as skemalagt FROM \"4_FEATURIZ_LOKALER\" r CROSS JOIN \"KALENDER_KVARTER\" d LEFT JOIN \"1_RAW_KALENDER\" k on d . date = k . date LEFT JOIN \"CTS_X_IOT_KVARTER\" i on r . id = i . id and r . kommune = i . kommune and r . skole = i . skole and d . date = i . date and d . time = i . timestamp :: time LEFT JOIN \"BOOKINGER_KVARTER\" b on r . id = b . id and r . kommune = b . kommune and r . skole = b . skole and d . date = b . dato and d . time = b . time LEFT JOIN \"2_STANDARD_SKEMAER\" s on r . id = s . rumid and r . kommune = s . kommune and r . skole = s . skole and d . dayname = s . dag and d . time >= s . starttid and d . time < s . sluttid and d . date > '2022-08-07' and d . date < '2023-06-24' and k . TYPE = 'Normal Dag' where co2 is not null and (( r . KOMMUNE = 'Aarhus' and ( d . DATE < '2023-05-15' or d . DATE > '2023-09-21' )) or ( r . KOMMUNE = 'Favrskov' and ( d . DATE < '2023-05-15' or d . DATE > '2023-09-21' )) or r . kommune = 'Syddjurs' ) and d . date > '2022-09-01' GROUP BY r . id , r . kommune , r . skole , d . date , d . time , d . dayname , d . tidspunkt_type , s . starttid , s . sluttid , k . type , k . navn ; --Skemaer create or replace view \"2_STANDARD_SKEMAER\" as select n . rumid , dag , starttid , sluttid , 'Aarhus' as KOMMUNE , 'Strandskolen' as SKOLE from \"1_RAW_AARHUS_SKEMA\" s INNER join \"1_RAW_AARHUS_LOKALER_MED_NAVNE\" n on n . navn = s . rumid union all select * , 'Syddjurs' as KOMMUNE , 'Thorsager Skole' as SKOLE from \"1_RAW_SYDDJURS_SKEMA\" union all select * , 'Favrskov' as KOMMUNE , 'R\u00f8nb\u00e6kskolen' as SKOLE from \"1_RAW_FAVRSKOV_SKEMA\" ; --Kalender create view \"KALENDER_KVARTER\" as select dateadd ( minute , row_number () over ( order by null ) * 15 , '2022-01-01 00:00' ) as timestamp , timestamp :: date as date , timestamp :: time as time , decode ( DAYNAME ( date ), 'Mon' , 'Mandag' , 'Tue' , 'Tirsdag' , 'Wed' , 'Onsdag' , 'Thu' , 'Torsdag' , 'Fri' , 'Fredag' , 'Sat' , 'Loerdag' , 'Sun' , 'Soendag' ) as dayname , DAYOFWEEKISO ( date ) as weekdaynum , weekofyear ( timestamp ) as weekno , iff ( time >= '16:00' , 'Fritid' , iff ( weekdaynum > 5 , 'Fritid' , 'Skole' )) as TIDSPUNKT_TYPE from table ( generator ( rowcount => 70080 )) g ; --Bookinger create or replace view \"BOOKINGER_KVARTER\" as SELECT d . date as dato , d . TIME , concat ( d . date :: date , ' ' , d . time ):: datetime as timestamp , r . KOMMUNE , r . SKOLE , r . ID , concat ( r . kommune , '-' , r . id ) as kommune_lokale , CONCAT ( d . date :: STRING , '-' , d . TIME , '-' , r . KOMMUNE ) AS KOMMUNE_DATO_TIME , IFF ( d . time >= b . start_tid , IFF ( d . time <= b . slut_tid , 1 , 0 ), 0 ) AS booket , IFF ( booket = 1 , start_tid , NULL ) AS starttid , IFF ( booket = 1 , slut_tid , NULL ) AS sluttid , IFF ( booket = 1 , lejer , NULL ) AS lejerid , IFF ( booket = 1 , b . type , NULL ) AS typebooking , CAST ( IFF ( booket = 1 , timer_format , NULL ) AS FLOAT ) AS booket_tid FROM \"KALENDER_KVARTER\" d CROSS JOIN \"4_FEATURIZ_LOKALER\" r LEFT JOIN \"4_FEATURIZ_BOOKINGS\" b ON d . date :: date = iff ( b . kommune = 'Aarhus' , try_to_date ( b . dato , 'dd-mm-yyyy' ), try_to_date ( b . dato , 'yyyy-mm-dd' )) and r . id = b . id and r . kommune = b . kommune and r . skole = b . skole WHERE booket = 1 GROUP BY d . date , TIME , slut_tid , start_tid , timer_format , r . KOMMUNE , r . ID , r . skole , lejer , b . type ORDER BY date :: date , time , kommune , id ; --IoT/CTS-data create or replace view \"CTS_X_IOT_KVARTER\" as with iot_aarhus as ( select distinct s . date , time_slice ( TIMESTAMP_NTZ_FROM_PARTS ( s . date , TIME ), 15 , 'MINUTE' ):: time as time , time_slice ( TIMESTAMP_NTZ_FROM_PARTS ( s . date , TIME ), 15 , 'MINUTE' ) as timestamp , avg ( CO2 ) as CO2 , avg ( SOUND ) as SOUND , avg ( LIGHT ) as LIGHT , avg ( IAQ ) as IAQ , avg ( TEMPERATURE ) as temp , avg ( HUMIDITY ) as humid , null as motion , Rum_ID as ID , 'Aarhus' as KOMMUNE , 'Strandskolen' as SKOLE , concat ( kommune , '-' , rum_id ) as kommune_lokale from \"3_CLEANSED_AARHUS_IOT\" s , \"2_STANDARD_AARHUS_LOKALE_STAMDATA_AREAL\" m where trim ( s . LOKALEID :: string ) = m . RUM_ID :: string group by s . date , time , m . rum_id ) , iot_syddjurs as ( select distinct s . date , time_slice ( TIMESTAMP_NTZ_FROM_PARTS ( s . date , TIME ), 15 , 'MINUTE' ):: time as time , time_slice ( TIMESTAMP_NTZ_FROM_PARTS ( s . date , s . TIME ), 15 , 'MINUTE' ) as timestamp , avg ( \"'CO2'\" :: float ) as co2 , null as sound , avg ( \"'Belysningsstyrke'\" :: float ) as light , null as IAQ , avg ( \"'Temperature'\" :: float ) as temp , avg ( \"'Relativ Luftfugtighed'\" :: float ) as Humid , sum ( \"'Quantity'\" :: float ) as motion , RIGHT ( LOKALENUMMER , length ( LOKALENUMMER ) - 4 ) as ID , 'Syddjurs' as KOMMUNE , 'Thorsager Skole' as SKOLE , concat ( kommune , '-' , id ) as kommune_lokale from \"3_CLEANSED_SYDDJURS_IOT\" PIVOT ( AVG ( value ) FOR FORBRUGSTYPE IN ( 'Temperature' , 'CO2' , 'Belysningsstyrke' , 'Quantity' , 'Light' , 'Relativ Luftfugtighed' )) as s , \"2_STANDARD_SYDDJURS_LOKALE_STAMDATA_AREAL\" m where s . LOKALEID = m . LOKALEID group by ID , s . date , s . time , m . lokalenummer ), cts_favrskov as ( select distinct Concat ( Right ( Left ( s . Timestamp , 10 ), 4 ), '-' , substr ( Left ( s . Timestamp , 10 ), 4 , 2 ), '-' , Left ( Left ( s . Timestamp , 10 ), 2 )):: DATE as DATE , time_slice ( TIMESTAMP_NTZ_FROM_PARTS ( date , TIME ), 15 , 'MINUTE' ):: time as time , time_slice ( TIMESTAMP_NTZ_FROM_PARTS ( date , TIME ), 15 , 'MINUTE' ) as timestamp , avg ( s . CO2 :: float ) as co2 , null as sound , null as light , null as IAQ , avg ( REPLACE ( s . TEMPERATUR , ',' , '.' ):: float ) as temp , null as Humid , null as motion , s . LOKALE :: string as ID , 'Favrskov' as KOMMUNE , 'R\u00f8nb\u00e6kskolen' as SKOLE , concat ( kommune , '-' , id ) as kommune_lokale from \"2_STANDARD_FAVRSKOV_CTS_HISTORY\" s , \"2_STANDARD_FAVRSKOV_LOKALE_STAMDATA_AREAL\" m where s . LOKALE = m . Lokalenr group by ID , date , s . Time , m . Lokalenr ), iot_favrskov as ( select distinct s . date , time_slice ( TIMESTAMP_NTZ_FROM_PARTS ( s . date , TIME ), 15 , 'MINUTE' ):: time as time , time_slice ( TIMESTAMP_NTZ_FROM_PARTS ( s . date , TIME ), 15 , 'MINUTE' ) as timestamp , avg ( CO2 ) as CO2 , avg ( SOUND ) as SOUND , avg ( LIGHT ) as LIGHT , avg ( IAQ ) as IAQ , avg ( TEMPERATURE ) as TEMPERATURE , avg ( HUMIDITY ) as HUMIDITY , null as motion , navn as ID , 'Favrskov' as KOMMUNE , 'R\u00f8nb\u00e6kskolen' as SKOLE , concat ( kommune , '-' , navn ) as kommune_lokale from \"3_CLEANSED_FAVRSKOV_IOT\" s group by s . date , time , s . navn ) select * from iot_aarhus union all select * from iot_syddjurs union all select * from iot_favrskov union all select * from cts_favrskov ; ---TRAINING--- create or replace view \"3_CLEANSED_DRIFTOPTIMERING_TRAINING\" as select distinct * from \"2_STANDARD_DRIFTOPTIMERING_DATA\" where date < convert_timezone ( 'Europe/Copenhagen' , CURRENT_TIMESTAMP ):: date ; -- where date > 6 months (future) ; create or replace table \"4_FEATURIZ_DRIFTOPTIMERING_TRAINING\" as select * from \"3_CLEANSED_DRIFTOPTIMERING_TRAINING\" ; ---PREDICT--- create or replace view \"3_CLEANSED_DRIFTOPTIMERING_PREDICT\" as with unpredicted_data as ( select distinct t . * from \"2_STANDARD_DRIFTOPTIMERING_DATA\" t LEFT JOIN \"1_RAW_DRIFTOPTIMERINGSMODEL\" r on r . date = t . date and r . time = t . time and r . id = t . id and r . kommune = t . kommune where r . id is null order by date asc ), unpredicted_rooms_dates as ( select distinct kommune , skole , id , date from unpredicted_data order by date ) select distinct t . * from \"2_STANDARD_DRIFTOPTIMERING_DATA\" t LEFT JOIN unpredicted_rooms_dates d on d . kommune = t . kommune and d . skole = t . skole and d . id = t . id and d . date = t . date where d . id is not null and d . date < convert_timezone ( 'Europe/Copenhagen' , CURRENT_TIMESTAMP ):: date ; create or replace table \"4_FEATURIZ_DRIFTOPTIMERING_PREDICT\" as select * from \"3_CLEANSED_DRIFTOPTIMERING_PREDICT\" ;","title":"Driftoptimeringsmodel"},{"location":"analytics/transformations/general/energy/","text":"create or replace view \"3_CLEANSED_ENERGIDATA\" as select KEY as MAALER_ID , DATO as DATO , MEASURETYPE as measure_type , VALUE :: float , ENHED , TIME as TIME , concat ( dato , ' ' , time ):: datetime as timestamp , concat ( dato :: string , '-Aarhus' ) as KOMMUNE_DATO , concat ( 'Aarhus-' , timestamp ) as KOMMUNE_DATO_TIME , 'Aarhus' as KOMMUNE , 'Strandskolen' as SKOLE from \"3_CLEANSED_AARHUS_ENERGIDATA\" -- Aarhus union all select ENHEDID as MAALER_ID , DATO :: date as DATO , FORBRUGSTYPE as measure_type , MAALING :: string :: float as VALUE , MAALEENHED as ENHED , TIME as TIME , concat ( dato , ' ' , time ):: datetime as timestamp , concat ( dato :: string , '-Syddjurs' ) as KOMMUNE_DATO , concat ( 'Syddjurs-' , timestamp ) as KOMMUNE_DATO_TIME , 'Syddjurs' as KOMMUNE , 'Thorsager Skole' as SKOLE from \"3_CLEANSED_SYDDJURS_ENERGIDATA\" -- Syddjurs union all select m . TAGID as MAALER_ID , v . DATE as DATO , m . ConsumptionType as measure_type , v . VALUE :: float as VALUE , m . UNIT as ENHED , v . TIME as TIME , concat ( dato , ' ' , time ):: datetime as timestamp , concat ( dato :: string , '-Favrskov' ) as KOMMUNE_DATO , concat ( 'Favrskov-' , timestamp ) as KOMMUNE_DATO_TIME , 'Favrskov' as KOMMUNE , 'R\u00f8nb\u00e6kskolen' as SKOLE from \"2_STANDARD_FAVRSKOV_ENERGIDATA_META\" m , \"3_CLEANSED_FAVRSKOV_ENERGIDATA\" v where m . TAGID = v . TAGID ; --Favrskov create or replace table \"4_FEATURIZ_ENERGIDATA\" as -- Persist data in table select distinct * from \"3_CLEANSED_ENERGIDATA\" ; create or replace view \"3_CLEANSED_ENERGIDATA_OPTIMIZED\" as with aarhus as ( select distinct * from \"3_CLEANSED_AARHUS_ENERGIDATA\" ) , syddjurs as ( select distinct dato , time , maaling , maaleenhed , enhedid , forbrugstype from \"3_CLEANSED_SYDDJURS_ENERGIDATA\" ) , favrskov as ( select distinct * from \"3_CLEANSED_FAVRSKOV_ENERGIDATA\" ) , El as ( select 'Aarhus' as Kommune , DATO , TIME , SUM ( VALUE :: float ) as value , ENHED as ENHED from aarhus where measuretype = 'electricity' and key = 3261220102 group by dato , time , ENHED union all select 'Syddjurs' as Kommune , dato , time , sum ( maaling :: float ) as value , MAALEENHED as ENHED from syddjurs where enhedid in ( 216 , 305 ) and forbrugstype = 'El_Samlet' group by dato , time , ENHED union all select 'Favrskov' as Kommune , DATE , TIME , SUM ( VALUE :: float ) as value , 'kWh' as ENHED from favrskov where tagid = 74 group by date , time , ENHED ) , Fjernvarme as ( select 'Aarhus' as Kommune , DATO , TIME , SUM ( VALUE :: float ) as value , ENHED as ENHED from aarhus where measuretype = 'district_heating' and key = 3261220204 group by dato , time , ENHED union all select 'Syddjurs' as Kommune , dato , time , sum ( maaling :: float ) as value , MAALEENHED as ENHED from syddjurs where enhedid in ( 216 ) and forbrugstype = 'Varme_Samlet' group by dato , time , ENHED union all select 'Favrskov' as Kommune , DATE , TIME , SUM ( VALUE :: float ) as value , 'MWh' as ENHED from favrskov where tagid = 75 group by date , time , ENHED ) , joined as ( select 'Fjernvarme' as measure_type , * from Fjernvarme union all select 'El' as measure_type , * from El ) select * , TIMESTAMP_FROM_PARTS ( year ( dato ), month ( dato ), day ( dato ), hour ( time ), 0 , 0 ) as timestamp , concat ( kommune , '-' , timestamp ) as kommune_dato_time from joined ; create or replace table \"4_FEATURIZ_ENERGIDATA_OPTIMIZED\" as -- Persist data in table select distinct * from \"3_CLEANSED_ENERGIDATA_OPTIMIZED\" ;","title":"Energy"},{"location":"analytics/transformations/general/idealcurve/","text":"create or replace table \"4_FEATURIZ_IDEALKURVE\" as WITH RECURSIVE number_range AS ( SELECT - 20 AS value UNION ALL SELECT value + 0 . 01 FROM number_range WHERE value < 40 ) SELECT value as grader , - 106 as haeldning_bygning , 1810 as skaering_bygning , iff (( grader * - 0 . 0225 ) + 0 . 3817 > 0 . 01 , ( grader * - 0 . 0225 ) + 0 . 3817 , 0 . 01 ) as idealkurve , 'Syddjurs' as Kommune FROM number_range ;","title":"Idealcurve"},{"location":"analytics/transformations/general/iot/","text":"create or replace view \"3_CLEANSED_CTS_X_IOT\" as with iot_aarhus as ( select distinct s . date , date_trunc ( 'HOUR' , TIME ) as time , TIMESTAMP_NTZ_FROM_PARTS ( s . date , date_trunc ( 'HOUR' , TIME )) as timestamp , date_trunc ( 'HOUR' , date_trunc ( 'HOUR' , TIME )) as TIME_GROUP , avg ( CO2 ) as CO2 , avg ( SOUND ) as SOUND , avg ( LIGHT ) as LIGHT , avg ( IAQ ) as IAQ , avg ( TEMPERATURE ) as TEMPERATURE , avg ( HUMIDITY ) as HUMIDITY , null as motion , Rum_ID as ID , 'Aarhus' as KOMMUNE , 'Strandskolen' as SKOLE , concat ( kommune , '-' , rum_id ) as kommune_lokale from \"3_CLEANSED_AARHUS_IOT\" s , \"2_STANDARD_AARHUS_LOKALE_STAMDATA_AREAL\" m where trim ( s . LOKALEID :: string ) = m . RUM_ID :: string group by s . date , time_group , time , m . rum_id ) , iot_syddjurs as ( select distinct s . date , s . TIME as TIME , TIMESTAMP_NTZ_FROM_PARTS ( s . date , date_trunc ( 'HOUR' , TIME )) as timestamp , date_trunc ( 'HOUR' , TIME ) as TIME_GROUP , avg ( \"'CO2'\" :: float ) as co2 , null as sound , avg ( \"'Belysningsstyrke'\" :: float ) as light , null as IAQ , avg ( \"'Temperature'\" :: float ) as temp , avg ( \"'Relativ Luftfugtighed'\" :: float ) as Humid , sum ( \"'Quantity'\" :: float ) as motion , RIGHT ( LOKALENUMMER , length ( LOKALENUMMER ) - 4 ) as ID , 'Syddjurs' as KOMMUNE , 'Thorsager Skole' as SKOLE , concat ( kommune , '-' , id ) as kommune_lokale from \"3_CLEANSED_SYDDJURS_IOT\" PIVOT ( AVG ( value ) FOR FORBRUGSTYPE IN ( 'Temperature' , 'CO2' , 'Belysningsstyrke' , 'Quantity' , 'Light' , 'Relativ Luftfugtighed' )) as s , \"2_STANDARD_SYDDJURS_LOKALE_STAMDATA_AREAL\" m where s . LOKALEID = m . LOKALEID group by ID , s . date , s . time , m . lokalenummer ), cts_favrskov as ( select Concat ( Right ( Left ( s . Timestamp , 10 ), 4 ), '-' , substr ( Left ( s . Timestamp , 10 ), 4 , 2 ), '-' , Left ( Left ( s . Timestamp , 10 ), 2 )):: DATE as DATE , s . TIME as TIME , TIMESTAMP_NTZ_FROM_PARTS ( date , date_trunc ( 'HOUR' , TIME )) as timestamp , date_trunc ( 'HOUR' , TIME ) as TIME_GROUP , avg ( s . CO2 :: float ) as co2 , null as sound , null as light , null as IAQ , avg ( REPLACE ( s . TEMPERATUR , ',' , '.' ):: float ) as temp , null as Humid , null as motion , s . LOKALE :: string as ID , 'Favrskov' as KOMMUNE , 'R\u00f8nb\u00e6kskolen' as SKOLE , concat ( kommune , '-' , id ) as kommune_lokale from \"2_STANDARD_FAVRSKOV_CTS_HISTORY\" s , \"2_STANDARD_FAVRSKOV_LOKALE_STAMDATA_AREAL\" m where s . LOKALE = m . Lokalenr group by ID , date , s . Time , m . Lokalenr ), iot_favrskov as ( select distinct s . date , date_trunc ( 'HOUR' , TIME ) as time , TIMESTAMP_NTZ_FROM_PARTS ( s . date , date_trunc ( 'HOUR' , TIME )) as timestamp , date_trunc ( 'HOUR' , date_trunc ( 'HOUR' , TIME )) as TIME_GROUP , avg ( CO2 ) as CO2 , avg ( SOUND ) as SOUND , avg ( LIGHT ) as LIGHT , avg ( IAQ ) as IAQ , avg ( TEMPERATURE ) as TEMPERATURE , avg ( HUMIDITY ) as HUMIDITY , null as motion , navn as ID , 'Favrskov' as KOMMUNE , 'R\u00f8nb\u00e6kskolen' as SKOLE , concat ( kommune , '-' , navn ) as kommune_lokale from \"3_CLEANSED_FAVRSKOV_IOT\" s group by s . date , time , s . navn ) select * from iot_aarhus union all select * from iot_syddjurs union all select * from cts_favrskov union all select * from iot_favrskov ; create or replace table \"4_FEATURIZ_CTS_X_IOT\" as select distinct * from \"3_CLEANSED_CTS_X_IOT\" ;","title":"Iot"},{"location":"analytics/transformations/general/manualbenchmark/","text":"create or replace table \"4_FEATURIZ_BENCHMARK_WEEKEND_MANUEL\" ( Kommune varchar , benchmark_weekend float ); insert into \"4_FEATURIZ_BENCHMARK_WEEKEND_MANUEL\" values ( 'Syddjurs' , 154 );","title":"Manualbenchmark"},{"location":"analytics/transformations/general/municipalities/","text":"create or replace table \"1_RAW_KOMMUNER_SKOLER\" ( Kommune varchar , Skole varchar , Driftsareal float ); insert into \"1_RAW_KOMMUNER_SKOLER\" values ( 'Syddjurs' , 'Thorsager Skole' , 4740 ); insert into \"1_RAW_KOMMUNER_SKOLER\" values ( 'Aarhus' , 'Strandskolen' , 9093 ); insert into \"1_RAW_KOMMUNER_SKOLER\" values ( 'Favrskov' , 'R\u00f8nb\u00e6kskolen' , 9039 );","title":"Municipalities"},{"location":"analytics/transformations/general/passivehours/","text":"create or replace view \"3_CLEANSED_PASSIVTIMER\" as select timestamp , date , time , year , month , monthname , monthno , kommune , iff ( sum ( booket ) > 0 , 0 , 1 ) as passivtime , kommune_dato_time from ( SELECT d . timestamp , d . date , d . time , year ( d . date :: date ):: string as year , month ( d . date :: date ):: string as month , monthname ( d . date :: date ) as monthname , month ( d . date :: date ):: int as monthno , r . kommune , sum ( ifnull ( booket , 0 )) as booket , concat ( r . kommune , '-' , d . timestamp ) as kommune_dato_time FROM \"4_FEATURIZ_DATOER\" d CROSS JOIN \"4_FEATURIZ_LOKALER\" r LEFT JOIN \"4_FEATURIZ_BOOKINGS_TIME\" b ON d . Timestamp = b . Timestamp AND r . id = b . id and r . kommune = b . kommune where d . tidspunkt_type = 'Fritid' and date <= current_date () group by d . date , d . time , d . timestamp , r . kommune , kommune_dato_time order by timestamp asc ) group by timestamp , date , time , year , month , monthname , monthno , kommune , kommune_dato_time order by timestamp asc ; create or replace table \"4_FEATURIZ_PASSIVTIMER\" as select * from \"3_CLEANSED_PASSIVTIMER\" ; create or replace view \"3_CLEANSED_MINIMUM_EL\" as with Aarhus as ( select 'Aarhus' as Kommune , DATO , TIME , SUM ( VALUE :: float ) as elforbrug from \"3_CLEANSED_AARHUS_ENERGIDATA\" where measuretype = 'electricity' and key = 3261220102 group by dato , time ) , Aarhus_minimum as ( select * from Aarhus where elforbrug > 1 and time < '07:00' order by elforbrug asc limit 100 ) , Syddjurs as ( select 'Syddjurs' as Kommune , dato , time , sum ( maaling :: float ) as elforbrug from \"3_CLEANSED_SYDDJURS_ENERGIDATA\" where enhedid in ( 216 , 305 ) and forbrugstype = 'El_Samlet' group by dato , time ) , Syddjurs_minimum as ( select * from Syddjurs where elforbrug > 1 and time < '07:00' order by elforbrug asc limit 100 ) , Favrskov as ( select 'Favrskov' as Kommune , DATE , TIME , SUM ( VALUE :: float ) as elforbrug from \"3_CLEANSED_FAVRSKOV_ENERGIDATA\" where tagid = 74 group by date , time ) , Favrskov_minimum as ( select * from Favrskov where elforbrug > 1 and time < '07:00' order by elforbrug asc limit 100 ) select kommune , avg ( elforbrug ) as minimum_el , null as manuel_minimum from Aarhus_minimum group by kommune union all select kommune , avg ( elforbrug ) as minimum_el , 2 . 5 as manuel_minimum from Syddjurs_minimum group by kommune union all select kommune , avg ( elforbrug ) as minimum_el , null as manuel_minimum from Favrskov_minimum group by kommune ; create or replace table \"4_FEATURIZ_MINIMUM_EL\" as select * from \"3_CLEANSED_MINIMUM_EL\" ;","title":"Passivehours"},{"location":"analytics/transformations/general/procedures/","text":"`` sql create or replace PROCEDURE RAWLOADDATA(\"TGTDB\" VARCHAR(16777216), \"TGTSCHEMA\" VARCHAR(16777216), \"TGTTABLE\" VARCHAR(16777216),\"KOMMUNE\" VARCHAR(16777216), \"STAGE\" VARCHAR(16777216)) RETURNS VARCHAR(16777216) LANGUAGE JAVASCRIPT EXECUTE AS OWNER AS ' var insert_sql_command = COPY INTO \" {TGTDB}\".\" {TGTDB}\".\" {TGTSCHEMA}\".\" {TGTTABLE}\" FROM @\" {TGTTABLE}\" FROM @\" {STAGE}\"/${KOMMUNE}`; var statement = snowflake.createStatement({sqlText: insert_sql_command}); statement.execute(); return statement.getRowCount(); '; create or replace PROCEDURE UPDATEFEATURIZ(\"TGTDB\" VARCHAR(16777216), \"TGTSCHEMA\" VARCHAR(16777216), \"TGTTABLE\" VARCHAR(16777216),\"CLEANSEDTABLE\" VARCHAR(16777216)) RETURNS VARCHAR(16777216) LANGUAGE JAVASCRIPT EXECUTE AS OWNER AS ' var insert_sql_command = CREATE OR REPLACE TABLE \"${TGTDB}\".\"${TGTSCHEMA}\".\"${TGTTABLE}\" AS SELECT * FROM \"${TGTDB}\".\"${TGTSCHEMA}\".\"${CLEANSEDTABLE}\" ; var statement = snowflake.createStatement({sqlText: insert_sql_command}); statement.execute(); return statement.getRowCount(); '; create or replace PROCEDURE UPDATEFEATURIZDISTINCT(\"TGTDB\" VARCHAR(16777216), \"TGTSCHEMA\" VARCHAR(16777216), \"TGTTABLE\" VARCHAR(16777216),\"CLEANSEDTABLE\" VARCHAR(16777216)) RETURNS VARCHAR(16777216) LANGUAGE JAVASCRIPT EXECUTE AS OWNER AS ' var insert_sql_command = CREATE OR REPLACE TABLE \"${TGTDB}\".\"${TGTSCHEMA}\".\"${TGTTABLE}\" AS SELECT DISTINCT * FROM \"${TGTDB}\".\"${TGTSCHEMA}\".\"${CLEANSEDTABLE}\" ; var statement = snowflake.createStatement({sqlText: insert_sql_command}); statement.execute(); return statement.getRowCount(); '; ```","title":"Procedures"},{"location":"analytics/transformations/general/rooms/","text":"create or replace view \"3_CLEANSED_LOKALER\" as -- Unifying datatypes and names from the municipalities to a singular table of rooms and metadata select RUM_ID as ID , AREAL as AREAL , REPLACE ( AREAL , ',' , '.' ) as AREAL_FORMAT , RUMFUNKTION as type_rum , BYGNINGS_ID as FLOJ , 'Aarhus' as KOMMUNE , 'Strandskolen' as SKOLE , concat ( kommune , '-' , id ) as kommune_lokale from \"2_STANDARD_AARHUS_LOKALE_STAMDATA_AREAL\" union all select LOKALENR as ID , AREAL as AREAL , REPLACE ( AREAL , ',' , '.' ) as AREAL_FORMAT , TYPE as type_rum , FLOJ as FLOJ , 'Favrskov' as KOMMUNE , 'R\u00f8nb\u00e6kskolen' as SKOLE , concat ( kommune , '-' , id ) from \"2_STANDARD_FAVRSKOV_LOKALE_STAMDATA_AREAL\" union all select RIGHT ( LOKALENUMMER , length ( LOKALENUMMER ) - 4 ) as ID , NETTOAREAL as AREAL , REPLACE ( AREAL , ',' , '.' ) as AREAL_FORMAT , LOKALEANVENDELSE as type_rum , LEFT ( ID , 2 ) as FLOJ , 'Syddjurs' as KOMMUNE , 'Thorsager Skole' as SKOLE , concat ( kommune , '-' , id ) from \"2_STANDARD_SYDDJURS_LOKALE_STAMDATA_AREAL\" ; create or replace table \"4_FEATURIZ_LOKALER\" as -- update the persisted table with all rooms for BI select distinct * from \"3_CLEANSED_LOKALER\" ;","title":"Rooms"},{"location":"analytics/transformations/general/stages/","text":"CREATE or replace STORAGE INTEGRATION govtech_storage_integration TYPE = EXTERNAL_STAGE STORAGE_PROVIDER = AZURE AZURE_TENANT_ID = '9c0c1cb6-e571-4e1d-a416-5b2b2bc8def6' ENABLED = TRUE STORAGE_ALLOWED_LOCATIONS = ( 'azure://stgovtech.blob.core.windows.net/bookings' , 'azure://stgovtech.blob.core.windows.net/dmi' , 'azure://stgovtech.blob.core.windows.net/zurface-data' , 'azure://stgovtech.blob.core.windows.net/energi-data' , 'azure://stgovtech.blob.core.windows.net/iot-data' ); create or replace stage BOOKINGS_STAGE storage_integration = govtech_storage_integration url = 'azure://stgovtech.blob.core.windows.net/bookings' FILE_FORMAT = ( TYPE = JSON ); create or replace stage ENERGIDATA_STAGE storage_integration = govtech_storage_integration url = 'azure://stgovtech.blob.core.windows.net/energi-data' FILE_FORMAT = ( TYPE = JSON ); create or replace stage DMI_STAGE storage_integration = govtech_storage_integration url = 'azure://stgovtech.blob.core.windows.net/dmi' FILE_FORMAT = ( TYPE = JSON ); create or replace stage IOT_STAGE storage_integration = govtech_storage_integration url = 'azure://stgovtech.blob.core.windows.net/iot-data' FILE_FORMAT = ( TYPE = JSON ); create or replace stage ZURFACE_HISTORY_STAGE storage_integration = govtech_storage_integration url = 'azure://stgovtech.blob.core.windows.net/zurface-data/history' FILE_FORMAT = CSV_FORMAT ; create or replace stage ZURFACE_STAGE storage_integration = govtech_storage_integration url = 'azure://stgovtech.blob.core.windows.net/zurface-data' FILE_FORMAT = ( TYPE = JSON );","title":"Stages"},{"location":"analytics/transformations/general/weather/","text":"create or replace table \"1_RAW_DMI_HISTORY\" ( INDEX INT , STATIONID INT , PARAMETER VARCHAR , VALUE VARCHAR , TIMESTAMP VARCHAR , SKOLE VARCHAR ); create or replace table \"1_RAW_DMI\" ( data variant ); create or replace view \"2_STANDARD_DMI\" as select INDEX , STATIONID , PARAMETER , VALUE , TIMESTAMP , DATE ( TIMESTAMP ) as DATE , concat ( DATE :: string , '-Aarhus' ), CASE WHEN PARAMETER = 'sun_last1h_glob' THEN 'Solskinsminutter' WHEN PARAMETER = 'temp_mean_past1h' THEN 'Temperatur' WHEN PARAMETER = 'humidity_past1h' THEN 'Fugtighed' END as PARAMETER_DANSK as KOMMUNE_DATO , SKOLE as KOMMUNE from \"1_RAW_DMI_HISTORY\" ; create or replace table \"2_STANDARD_DMI_META\" ( Stationid varchar , skole varchar ); insert into \"2_STANDARD_DMI_META\" values ( '06072' , 'Favrskov' ), ( '06073' , 'Syddjurs' ), ( '06074' , 'Aarhus' ); create or replace view \"2_STANDARD_DMI\" as select value : properties : stationId :: varchar as STATIONID , value : properties : parameterId :: varchar as PARAMETER , value : properties : value :: varchar as VALUE , value : properties : observed :: varchar as observed , SKOLE from \"1_RAW_DMI\" , lateral flatten ( input => data : features ), \"2_STANDARD_DMI_META\" m where value : properties : stationId :: varchar = m . STATIONID ; create or replace view \"2_STANDARD_DMI_JOINED\" as select * from \"2_STANDARD_DMI\" union all select STATIONID , PARAMETER , VALUE , TIMESTAMP , SKOLE from \"1_RAW_DMI_HISTORY\" ; create or replace view \"3_CLEANSED_DMI\" as select STATIONID , PARAMETER , CASE WHEN PARAMETER = 'sun_last1h_glob' THEN 'Solskinsminutter' WHEN PARAMETER = 'temp_mean_past1h' THEN 'Temperatur' WHEN PARAMETER = 'humidity_past1h' THEN 'Fugtighed' END as PARAMETER_DANSK , VALUE :: float as VALUE , round ( value ) as value_round , TIMESTAMP :: datetime as timestamp , DATE ( TIMESTAMP ) as DATE , SKOLE as KOMMUNE from \"2_STANDARD_DMI_JOINED\" ; create or replace table \"4_FEATURIZ_DMI\" as select distinct * from \"3_CLEANSED_DMI\" ;","title":"Weather"},{"location":"analytics/transformations/syddjurs/syddjurs_transformations/","text":"Sample of outputs Metadata LOKALEID LOKALENUMMER ENHED BYGNING LOKALE ADRESSE POSTNUMMER POSTDISTRIKT NETTOAREAL LOKALEANVENDELSE ENERGIMAERKE ENERGIMAERKEAAR ENERGIMAERKEUDLOEBAAR BYGNINGBOOKBAR LOKALEBOOKBAR OPVARMET EJERFORHOLD GYLDIGFRA GYLDIGTIL CONVENTUSRESSOURCEID CARETAKERID ENHEDSKODE ENHEDID 3622 305.3.0.001 Sportshal i Thorsager Sportshal med klub og Servicesmedarbejer 305.3.0.001 (Entr\u00e9) Kl\u00f8vervangen 8410 R\u00f8nde 5.6 Entr\u00e9 NULL NULL NULL Bookbar Ej bookbar Ingen data Ejer NULL NULL NULL 2001 305 53 3623 305.3.0.002 Sportshal i Thorsager Sportshal med klub og Servicesmedarbejer 305.3.0.002 (Gang) Kl\u00f8vervangen 8410 R\u00f8nde 35.77 Gang NULL NULL NULL Bookbar Ej bookbar Ingen data Ejer NULL NULL NULL 2000 305 53 Bookings BOOKING_START BOOKING_END BOOKING_TYPE CATEGORY_NAME CATEGORY_ID ORGANIZATION_ID ORGANIZATION_NAME RESOURCE_ID RESOURCE_NAME RESOURCE_ORGANIZATION_ID RESOURCE_ORGANIZATION_NAME 2022-01-01 09:00:00.000 2022-01-01 11:00:00.000 ordinary Skumtennis 26907 5104 Thorsager-R\u00f8nde Idr\u00e6tsforening 10339 Hallen 5125 Thorsager Skole 2022-01-02 08:00:00.000 2022-01-02 14:00:00.000 ordinary Andet 9294 6133 Cykel Klubben Djurs 39100 multirum 5125 Thorsager Skole Energy ID ENHEDID FORBRUGSMAALERID DATO TIME MAALING FORBRUGSTYPE MAALEENHED 152 45 3 2022-04-01 02:00:00 0 Vand_Samlet m3 176 45 3 2022-04-02 02:00:00 0 Vand_Samlet m3 IoT LOKALEID SENSORNAVN DATE TIME FORBRUGSTYPE VALUE ENDING_NAVN 3571 a81758fffe06b5fa_Temperature 2022-05-15 00:00:00 Temperature 20.033333333 Temperature 3574 a81758fffe06b5fe_Temperature 2022-05-15 00:00:00 Temperature 19.366666667 Temperature","title":"Sample of outputs"},{"location":"analytics/transformations/syddjurs/syddjurs_transformations/#sample-of-outputs","text":"","title":"Sample of outputs"},{"location":"analytics/transformations/syddjurs/syddjurs_transformations/#metadata","text":"LOKALEID LOKALENUMMER ENHED BYGNING LOKALE ADRESSE POSTNUMMER POSTDISTRIKT NETTOAREAL LOKALEANVENDELSE ENERGIMAERKE ENERGIMAERKEAAR ENERGIMAERKEUDLOEBAAR BYGNINGBOOKBAR LOKALEBOOKBAR OPVARMET EJERFORHOLD GYLDIGFRA GYLDIGTIL CONVENTUSRESSOURCEID CARETAKERID ENHEDSKODE ENHEDID 3622 305.3.0.001 Sportshal i Thorsager Sportshal med klub og Servicesmedarbejer 305.3.0.001 (Entr\u00e9) Kl\u00f8vervangen 8410 R\u00f8nde 5.6 Entr\u00e9 NULL NULL NULL Bookbar Ej bookbar Ingen data Ejer NULL NULL NULL 2001 305 53 3623 305.3.0.002 Sportshal i Thorsager Sportshal med klub og Servicesmedarbejer 305.3.0.002 (Gang) Kl\u00f8vervangen 8410 R\u00f8nde 35.77 Gang NULL NULL NULL Bookbar Ej bookbar Ingen data Ejer NULL NULL NULL 2000 305 53","title":"Metadata"},{"location":"analytics/transformations/syddjurs/syddjurs_transformations/#bookings","text":"BOOKING_START BOOKING_END BOOKING_TYPE CATEGORY_NAME CATEGORY_ID ORGANIZATION_ID ORGANIZATION_NAME RESOURCE_ID RESOURCE_NAME RESOURCE_ORGANIZATION_ID RESOURCE_ORGANIZATION_NAME 2022-01-01 09:00:00.000 2022-01-01 11:00:00.000 ordinary Skumtennis 26907 5104 Thorsager-R\u00f8nde Idr\u00e6tsforening 10339 Hallen 5125 Thorsager Skole 2022-01-02 08:00:00.000 2022-01-02 14:00:00.000 ordinary Andet 9294 6133 Cykel Klubben Djurs 39100 multirum 5125 Thorsager Skole","title":"Bookings"},{"location":"analytics/transformations/syddjurs/syddjurs_transformations/#energy","text":"ID ENHEDID FORBRUGSMAALERID DATO TIME MAALING FORBRUGSTYPE MAALEENHED 152 45 3 2022-04-01 02:00:00 0 Vand_Samlet m3 176 45 3 2022-04-02 02:00:00 0 Vand_Samlet m3","title":"Energy"},{"location":"analytics/transformations/syddjurs/syddjurs_transformations/#iot","text":"LOKALEID SENSORNAVN DATE TIME FORBRUGSTYPE VALUE ENDING_NAVN 3571 a81758fffe06b5fa_Temperature 2022-05-15 00:00:00 Temperature 20.033333333 Temperature 3574 a81758fffe06b5fe_Temperature 2022-05-15 00:00:00 Temperature 19.366666667 Temperature","title":"IoT"}]}